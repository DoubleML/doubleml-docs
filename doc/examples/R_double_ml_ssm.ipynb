{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03c754a",
   "metadata": {},
   "source": [
    "# R: Sample Selection Models\n",
    "\n",
    "In this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the average treatment effect (ATE) under sample selection or outcome attrition. The estimation is based on a simulated DGP from Appendix E of [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071).  \n",
    "\n",
    "Consider the following DGP:\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y_i &= \\theta_0 D_i + X_i'\\beta_0 + \\varepsilon_i,\\\\\n",
    "S_i &= \\mathbb{1}\\{D_i + \\gamma_0 Z_i + X_i'\\beta_0 + \\upsilon_i > 0\\}, \\\\\n",
    "D_i &= \\mathbb{1}\\{X_i'\\beta_0 + \\xi_i > 0\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $Y_i$ is observed if $S_i=1$\n",
    "with\n",
    "$$X_i \\sim N(0, \\sigma^2_X), \\quad Z_i \\sim N(0, 1), \\quad (\\varepsilon,_i \\nu_i) \\sim N(0, \\sigma^2_{\\varepsilon, \\nu}), \\quad \\xi_i \\sim N(0, 1).$$\n",
    "\n",
    "Let $D_i\\in\\{0,1\\}$ denote the treatment status of unit $i$ and let $Y_{i}$ be the outcome of interest of unit $i$.\n",
    "Using the potential outcome notation, we can write $Y_{i}(d)$ for the potential outcome of unit $i$ and treatment status $d$. Further, let $X_i$ denote a vector of pre-treatment covariates.  \n",
    "\n",
    "## Outcome missing at random (MAR)  \n",
    "Now consider the first setting, in which the outcomes are missing at random (MAR), according to assumptions in [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071). \n",
    "Let the covariance matrix $\\sigma^2_X$ be such that $a_{ij} = 0.5^{|i - j|}$, $\\gamma_0 = 0$, $\\sigma^2_{\\varepsilon, \\upsilon} = \\begin{pmatrix} 1 & 0 \\\\  0 & 1 \\end{pmatrix}$ and finally, let the vector of coefficients $\\beta_0$ resemble a quadratic decay of coefficients importance; $\\beta_{0,j} = 0.4/j^2$ for $j = 1, \\ldots, p$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25181797",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will use the implemented data generating process `make_ssm_data` to generate data according to the simulation in Appendix E of [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071). The true ATE in this DGP is equal to $\\theta_0=1$ (it can be changed by setting the parameter `theta`). \n",
    "\n",
    "The data generating process `make_ssm_data` by default settings already returns a `DoubleMLData` object (however, it can return a pandas DataFrame or a NumPy array if `return_type` is specified accordingly). In this first setting, we are estimating the ATE under missingness at random, so we set `mar=True`.\n",
    "The selection indicator `S` can be set via `s_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1803b2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#remotes::install_github(\"DoubleML/doubleml-for-r\", ref = \"p-ssm_branch\", force = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2f984",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(DoubleML)\n",
    "library(mlr3)\n",
    "library(ggplot2)\n",
    "\n",
    "# suppress messages during fitting\n",
    "lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35090ed",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================= DoubleMLData Object ==================\n",
       "\n",
       "\n",
       "------------------ Data summary      ------------------\n",
       "Outcome variable: y\n",
       "Treatment variable(s): d\n",
       "Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100\n",
       "Instrument(s): \n",
       "Selection variable: s\n",
       "No. Observations: 2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(3141)\n",
    "n_obs = 2000\n",
    "df = make_ssm_data(n_obs=n_obs, mar=TRUE, return_type=\"data.table\")\n",
    "\n",
    "dml_data = DoubleMLData$new(df, y_col=\"y\", d_cols=\"d\", s_col=\"s\")\n",
    "dml_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee9309",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "To estimate the ATE under sample selection, we will use the `DoubleMLSSM` class. \n",
    "\n",
    "As for all `DoubleML` classes, we have to specify learners, which have to be initialized first.\n",
    "Given the simulated quadratic decay of coefficients importance, Lasso regression should be a suitable option (as for propensity scores, this will be a $\\mathcal{l}_1$-penalized Logistic Regression). \n",
    "\n",
    "The learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[Y_i|D_i, S_i, X_i]$, whereas the learners `ml_m` and `ml_pi` will be used to estimate the treatment and selection propensity scores $P(D_i=1|X_i)$ and $P(S_i=1|D_i, X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a905df9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ml_g = lrn(\"regr.cv_glmnet\", nfolds = 5, s = \"lambda.min\")\n",
    "ml_m = lrn(\"classif.cv_glmnet\", nfolds = 5, s = \"lambda.min\")\n",
    "ml_pi = lrn(\"classif.cv_glmnet\", nfolds = 5, s = \"lambda.min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a87ea",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "The `DoubleMLSSM` class can be used as any other `DoubleML` class. \n",
    "\n",
    "The score is set to `score='missing-at-random'`, since the parameters of the DGP were set to satisfy the assumptions of outcomes missing at random. Further, since the simulation in [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071) uses normalization of inverse probability weights, we will apply the same setting by `normalize_ipw=True`.\n",
    "\n",
    "After initialization, we have to call the `fit()` method to estimate the nuisance elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761ab3d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= DoubleMLSSM Object ==================\n",
      "\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: y\n",
      "Treatment variable(s): d\n",
      "Covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100\n",
      "Instrument(s): \n",
      "Selection variable: s\n",
      "No. Observations: 2000\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: missing-at-random\n",
      "DML algorithm: dml2\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "ml_g: regr.cv_glmnet\n",
      "ml_pi: classif.cv_glmnet\n",
      "ml_m: classif.cv_glmnet\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 5\n",
      "No. repeated sample splits: 1\n",
      "Apply cross-fitting: TRUE\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      " Estimates and significance testing of the effect of target variables\n",
      "  Estimate. Std. Error t value Pr(>|t|)    \n",
      "d   0.94473    0.03045   31.03   <2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dml_ssm = DoubleMLSSM$new(dml_data, ml_g, ml_m, ml_pi, score=\"missing-at-random\",\n",
    "                            normalize_ipw = TRUE)\n",
    "dml_ssm$fit()\n",
    "\n",
    "print(dml_ssm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e5376",
   "metadata": {},
   "source": [
    "Confidence intervals at different levels can be obtained via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1297e329",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        5 %      95 %\n",
      "d 0.8946549 0.9948104\n"
     ]
    }
   ],
   "source": [
    "print(dml_ssm$confint(level = 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe05e01",
   "metadata": {},
   "source": [
    "### ATE estimates distribution\n",
    "\n",
    "Here, we add a small simulation where we generate multiple datasets, estimate the ATE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d5fcd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a10b47",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n_rep = 100\n",
    "ATE_estimates = rep(NA, n_rep)\n",
    "ATE_estimates[1] = dml_ssm$coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956cb51",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n_rep = 200\n",
    "ATE = 1.0\n",
    "\n",
    "ATE_estimates = rep(NA, n_rep)\n",
    "\n",
    "set.seed(42)\n",
    "for (i_rep in seq_len(n_rep)) {\n",
    "    if (i_rep %% (n_rep %/% 10) == 0) {\n",
    "        print(paste0(\"Iteration: \", i_rep, \"/\", n_rep))\n",
    "    }\n",
    "    dml_data = make_ssm_data(n_obs=n_obs, mar=TRUE)\n",
    "    dml_ssm = DoubleMLSSM$new(dml_data, ml_g, ml_m, ml_pi, score='missing-at-random', normalize_ipw=TRUE)\n",
    "    dml_ssm$fit()\n",
    "    ATE_estimates[i_rep] = dml_ssm$coef\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a6094",
   "metadata": {},
   "source": [
    "The distribution of the estimates takes the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd8060",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histogram of ATE_estimates with ggplot\n",
    "hist_data = data.frame(ATE_estimates = ATE_estimates)\n",
    "\n",
    "ggplot(hist_data, aes(x = ATE_estimates)) +\n",
    "  geom_histogram(binwidth = 0.1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n",
    "  geom_vline(aes(xintercept = ATE), color = \"red\", linetype = \"dashed\", size = 1) +\n",
    "  labs(title = \"Histogram of ATE Estimates\",\n",
    "       x = \"ATE Estimates\",\n",
    "       y = \"Frequency\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645df4f8",
   "metadata": {},
   "source": [
    "## Outcome missing under nonignorable nonresponse\n",
    "Now consider a different setting, in which the outcomes are missing under nonignorable nonresponse assumptions in [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071). \n",
    "Let the covariance matrix $\\sigma^2_X$ again be such that $a_{ij} = 0.5^{|i - j|}$, but now $\\gamma_0 = 1$ and $\\sigma^2_{\\varepsilon, \\upsilon} = \\begin{pmatrix} 1 & 0.8 \\\\  0.8 & 1 \\end{pmatrix}$ to show a strong correlation between $\\varepsilon$ and $\\upsilon$. Let the vector of coefficients $\\beta$ again resemble a quadratic decay of coefficients importance; $\\beta_{0,j} = 0.4/j^2$ for $j = 1, \\ldots, p$.\n",
    "\n",
    "The directed acyclic graph (DAG) shows the the structure of the causal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7121a72",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will again use the implemented data generating process `make_ssm_data` to generate data according to the simulation in Appendix E of [Bia, Huber and Lafférs (2023)](https://doi.org/10.1080/07350015.2023.2271071). We will again leave the default ATE equal to $\\theta_0=1$.\n",
    "\n",
    "In this setting, we are estimating the ATE under nonignorable nonresponse, so we set `mar=False`. Again, the selection indicator `S` can be set via `s_col`. Further, we need to specify an intrument via `z_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4f61f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8539",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "We will again use the `DoubleMLSSM` class. \n",
    "\n",
    "Further,  will leave he learners for all nuisance functions to be the same as in the first setting, as the simulated quadratic decay of coefficients importance still holds.\n",
    "\n",
    "Now the learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[Y_i|D_i, S_i, X_i, \\Pi_i]$, whereas the learners `ml_m` and `ml_pi` will be used to estimate the treatment and selection propensity scores $P(D_i=1|X_i, \\Pi_i)$ and $P(S_i=1|D_i, X_i, Z_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccb8f7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bea41",
   "metadata": {},
   "source": [
    "The score is now set to `'nonignorable'`, since the parameters of the DGP were set to satisfy the assumptions of outcomes missing under nonignorable nonresponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a55e3f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ad184",
   "metadata": {},
   "source": [
    "### ATE estimates distribution\n",
    "\n",
    "Here we again add a small simulation where we generate multiple datasets, estimate the ATE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9eae4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f5d2e",
   "metadata": {},
   "source": [
    "And plot the estimates distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaebd62",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
