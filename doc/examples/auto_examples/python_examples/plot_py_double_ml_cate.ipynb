{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CATE\n\nThis is an example of how to use the double machine learning approach for conditional value-at-risk estimation.\nThe code demonstrates the usage of the `DoubleMLCateEstimator` class and provides a step-by-step guide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python: Conditional Average Treatment Effects (CATEs)\n\nIn this simple example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate conditional average treatment effects with B-splines for one or two-dimensional effects.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n\nWe define a data generating process to create synthetic data to compare the estimates to the true effect. The data generating process is based on the Monte Carlo simulation from [Oprescu et al. (2019)](http://proceedings.mlr.press/v97/oprescu19a.html) and this [notebook](https://github.com/py-why/EconML/blob/main/notebooks/Causal%20Forest%20and%20Orthogonal%20Random%20Forest%20Examples.ipynb) from [EconML](https://github.com/py-why/EconML).\n%%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import plotly.io as pio\npio.renderers.default = 'sphinx_gallery'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport doubleml as dml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is generated as\n\n$\n\\begin{align}\nY_i & = g(X_i)T_i + \\langle W_i,\\gamma_0\\rangle + \\epsilon_i \\\\\nT_i & = \\langle W_i,\\beta_0\\rangle +\\eta_i,\n\\end{align}\n$\n\nwhere $W_i\\sim\\mathcal{N}(0,I_{d_w})$, $X_i\\sim\\mathcal{U}[0,1]^{d_x}$ and $\\epsilon_i,\\eta_i\\sim\\mathcal{U}[0,1]$.\nThe coefficient vectors $\\gamma_0$ and $\\beta_0$ both have small random support which values are drawn independently from $\\mathcal{U}[0,1]$.\nFurther, $g(x)$ defines the conditional treatment effect, which is defined differently depending on the dimension of $x$.\n\nIf $x$ is univariate the conditional treatment effect takes the following form\n\n$$ g(x) = \\exp(2x) + 3\\sin(4x),$$\n\nwhereas for a two-dimensional variable $x=(x_1,x_2)$ the conditional treatment effect is defined as\n\n$$ g(x) = \\exp(2x_1) + 3\\sin(4x_2).$$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def treatment_effect_1d(x):\n    te = np.exp(2 * x) + 3 * np.sin(4 * x)\n    return te\n\ndef treatment_effect_2d(x):\n    te = np.exp(2 * x[0]) + 3 * np.sin(4 * x[1])\n    return te\n\ndef create_synthetic_data(n_samples=200, n_w=30, support_size=5, n_x=1):\n    # Outcome support\n    # With the next two lines we are effectively choosing the matrix gamma in the example\n    support_y = np.random.choice(np.arange(n_w), size=support_size, replace=False)\n    coefs_y = np.random.uniform(0, 1, size=support_size)\n    # Define the function to generate the noise\n    epsilon_sample = lambda n: np.random.uniform(-1, 1, size=n_samples)\n    # Treatment support\n    # Assuming the matrices gamma and beta have the same non-zero components\n    support_t = support_y\n    coefs_t = np.random.uniform(0, 1, size=support_size)\n    # Define the function to generate the noise\n    eta_sample = lambda n: np.random.uniform(-1, 1, size=n_samples)\n\n    # Generate controls, covariates, treatments and outcomes\n    w = np.random.normal(0, 1, size=(n_samples, n_w))\n    x = np.random.uniform(0, 1, size=(n_samples, n_x))\n    # Heterogeneous treatment effects\n    if n_x == 1:\n        te = np.array([treatment_effect_1d(x_i) for x_i in x]).reshape(-1)\n    elif n_x == 2:\n        te = np.array([treatment_effect_2d(x_i) for x_i in x]).reshape(-1)\n    # Define treatment\n    log_odds = np.dot(w[:, support_t], coefs_t) + eta_sample(n_samples)\n    t_sigmoid = 1 / (1 + np.exp(-log_odds))\n    t = np.array([np.random.binomial(1, p) for p in t_sigmoid])\n    # Define the outcome\n    y = te * t + np.dot(w[:, support_y], coefs_y) + epsilon_sample(n_samples)\n\n    # Now we build the dataset\n    y_df = pd.DataFrame({'y': y})\n    if n_x == 1:\n        x_df = pd.DataFrame({'x': x.reshape(-1)})\n    elif n_x == 2:\n        x_df = pd.DataFrame({'x_0': x[:,0],\n                             'x_1': x[:,1]})\n    t_df = pd.DataFrame({'t': t})\n    w_df = pd.DataFrame(data=w, index=np.arange(w.shape[0]), columns=[f'w_{i}' for i in range(w.shape[1])])\n\n    data = pd.concat([y_df, x_df, t_df, w_df], axis=1)\n\n    covariates = list(w_df.columns.values) + list(x_df.columns.values)\n    return data, covariates, te"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-dimensional Example\n\nWe start with $X$ being one-dimensional and create our training data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DGP constants\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\nn_samples = 2000\nn_w = 10\nsupport_size = 5\nn_x = 1\n\n# Create data\ndata, covariates, true_effect = create_synthetic_data(n_samples=n_samples, n_w=n_w, support_size=support_size, n_x=n_x)\ndata_dml_base = dml.DoubleMLData(data,\n                                 y_col='y',\n                                 d_cols='t',\n                                 x_cols=covariates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, define the learners for the nuisance functions and fit the [IRM Model](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLIRM.html). Remark that the learners are not optimal for the linear form of this example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First stage estimation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nrandomForest_reg = RandomForestRegressor(n_estimators=500)\nrandomForest_class = RandomForestClassifier(n_estimators=500)\n\nnp.random.seed(42)\n\ndml_irm = dml.DoubleMLIRM(data_dml_base,\n                          ml_g=randomForest_reg,\n                          ml_m=randomForest_class,\n                          trimming_threshold=0.01,\n                          n_folds=5)\nprint(\"Training IRM Model\")\ndml_irm.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the CATE, we rely on the best-linear-predictor of the linear score as in [Semenova et al. (2021)](https://doi.org/10.1093/ectj/utaa027) To approximate the target function $g(x)$ with a linear form, we have to define a data frame of basis functions. Here, we rely on [patsy](https://patsy.readthedocs.io/en/latest/) to construct a suitable basis of [B-splines](https://en.wikipedia.org/wiki/B-spline).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import patsy\ndesign_matrix = patsy.dmatrix(\"bs(x, df=5, degree=2)\", {\"x\":data[\"x\"]})\nspline_basis = pd.DataFrame(design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the parameters to calculate the CATE estimate call the ``cate()`` method and supply the dataframe of basis elements.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cate = dml_irm.cate(spline_basis)\nprint(cate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To obtain the confidence intervals for the CATE, we have to call the ``confint()`` method and a supply a dataframe of basis elements.\nThis could be the same basis as for fitting the CATE model or a new basis to e.g. evaluate the CATE model on a grid.\nHere, we will evaluate the CATE on a grid from 0.1 to 0.9 to plot the final results.\nFurther, we construct uniform confidence intervals by setting the option ``joint`` and providing a number of bootstrap repetitions ``n_rep_boot``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_data = {\"x\": np.linspace(0.1, 0.9, 100)}\nspline_grid = pd.DataFrame(patsy.build_design_matrices([design_matrix.design_info], new_data)[0])\ndf_cate = cate.confint(spline_grid, level=0.95, joint=True, n_rep_boot=2000)\nprint(df_cate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot our results and compare them with the true effect.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = 10., 7.5\n\ndf_cate['x'] = new_data['x']\ndf_cate['true_effect'] = treatment_effect_1d(new_data['x'])\nfig, ax = plt.subplots()\nax.plot(df_cate['x'],df_cate['effect'], label='Estimated Effect')\nax.plot(df_cate['x'],df_cate['true_effect'], color=\"green\", label='True Effect')\nax.fill_between(df_cate['x'], df_cate['2.5 %'], df_cate['97.5 %'], color='b', alpha=.3, label='Confidence Interval')\n\nplt.legend()\nplt.title('CATE')\nplt.xlabel('x')\n_ =  plt.ylabel('Effect and 95%-CI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two-Dimensional Example\n\nIt is also possible to estimate multi-dimensional conditional effects. We will use the same data-generating process as above, but let $X$ be two-dimensional.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DGP constants\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\nn_samples = 5000\nn_w = 10\nsupport_size = 5\nn_x = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data, covariates, true_effect = create_synthetic_data(n_samples=n_samples, n_w=n_w, support_size=support_size, n_x=n_x)\ndata_dml_base = dml.DoubleMLData(data,\n                                 y_col='y',\n                                 d_cols='t',\n                                 x_cols=covariates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As univariate example estimate the [IRM Model](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLIRM.html).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First stage estimation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nrandomForest_reg = RandomForestRegressor(n_estimators=500)\nrandomForest_class = RandomForestClassifier(n_estimators=500)\n\nnp.random.seed(123)\n\ndml_irm = dml.DoubleMLIRM(data_dml_base,\n                          ml_g=randomForest_reg,\n                          ml_m=randomForest_class,\n                          trimming_threshold=0.01,\n                          n_folds=5)\nprint(\"Training IRM Model\")\ndml_irm.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As above, we will rely on the [patsy](https://patsy.readthedocs.io/en/latest/) package to construct the basis elements.\nIn the two-dimensional case, we will construct a tensor product of B-splines (for more information see [here](https://patsy.readthedocs.io/en/latest/spline-regression.html#tensor-product-smooths)).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = patsy.dmatrix(\"te(bs(x_0, df=7, degree=3), bs(x_1, df=7, degree=3))\", {\"x_0\": data[\"x_0\"], \"x_1\": data[\"x_1\"]})\nspline_basis = pd.DataFrame(design_matrix)\n\ncate = dml_irm.cate(spline_basis)\nprint(cate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create a new grid to evaluate and plot the effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_size = 100\nx_0 = np.linspace(0.1, 0.9, grid_size)\nx_1 = np.linspace(0.1, 0.9, grid_size)\nx_0, x_1 = np.meshgrid(x_0, x_1)\n\nnew_data = {\"x_0\": x_0.ravel(), \"x_1\": x_1.ravel()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline_grid = pd.DataFrame(patsy.build_design_matrices([design_matrix.design_info], new_data)[0])\ndf_cate = cate.confint(spline_grid, joint=True, n_rep_boot=2000)\nprint(df_cate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n\ntrue_effect = np.array([treatment_effect_2d(x_i) for x_i in zip(x_0.ravel(), x_1.ravel())]).reshape(x_0.shape)\neffect = np.asarray(df_cate['effect']).reshape(x_0.shape)\nlower_bound = np.asarray(df_cate['2.5 %']).reshape(x_0.shape)\nupper_bound = np.asarray(df_cate['97.5 %']).reshape(x_0.shape)\n\nfig = go.Figure(data=[\n    go.Surface(x=x_0,\n               y=x_1,\n               z=true_effect),\n    go.Surface(x=x_0,\n               y=x_1,\n               z=upper_bound, showscale=False, opacity=0.4,colorscale='purp'),\n    go.Surface(x=x_0,\n               y=x_1,\n               z=lower_bound, showscale=False, opacity=0.4,colorscale='purp'),\n])\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\", project_z=True))\n\nfig.update_layout(scene = dict(\n                    xaxis_title='X_0',\n                    yaxis_title='X_1',\n                    zaxis_title='Effect'),\n                    width=700,\n                    margin=dict(r=20, b=10, l=10, t=10))\n\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}