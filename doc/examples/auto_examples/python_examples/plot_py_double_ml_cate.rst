
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples\auto_examples\python_examples\plot_py_double_ml_cate.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_auto_examples_python_examples_plot_py_double_ml_cate.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_auto_examples_python_examples_plot_py_double_ml_cate.py:


CATE
===================================

This is an example of how to use the double machine learning approach for conditional value-at-risk estimation.
The code demonstrates the usage of the `DoubleMLCateEstimator` class and provides a step-by-step guide.

.. GENERATED FROM PYTHON SOURCE LINES 10-13

# Python: Conditional Average Treatment Effects (CATEs)

In this simple example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate conditional average treatment effects with B-splines for one or two-dimensional effects.

.. GENERATED FROM PYTHON SOURCE LINES 15-18

## Data

We define a data generating process to create synthetic data to compare the estimates to the true effect. The data generating process is based on the Monte Carlo simulation from [Oprescu et al. (2019)](http://proceedings.mlr.press/v97/oprescu19a.html) and this [notebook](https://github.com/py-why/EconML/blob/main/notebooks/Causal%20Forest%20and%20Orthogonal%20Random%20Forest%20Examples.ipynb) from [EconML](https://github.com/py-why/EconML).

.. GENERATED FROM PYTHON SOURCE LINES 20-24

.. code-block:: default

    import numpy as np
    import pandas as pd
    import doubleml as dml








.. GENERATED FROM PYTHON SOURCE LINES 25-45

The data is generated as

$
\begin{align}
Y_i & = g(X_i)T_i + \langle W_i,\gamma_0\rangle + \epsilon_i \\
T_i & = \langle W_i,\beta_0\rangle +\eta_i,
\end{align}
$

where $W_i\sim\mathcal{N}(0,I_{d_w})$, $X_i\sim\mathcal{U}[0,1]^{d_x}$ and $\epsilon_i,\eta_i\sim\mathcal{U}[0,1]$.
The coefficient vectors $\gamma_0$ and $\beta_0$ both have small random support which values are drawn independently from $\mathcal{U}[0,1]$.
Further, $g(x)$ defines the conditional treatment effect, which is defined differently depending on the dimension of $x$.

If $x$ is univariate the conditional treatment effect takes the following form

$$ g(x) = \exp(2x) + 3\sin(4x),$$

whereas for a two-dimensional variable $x=(x_1,x_2)$ the conditional treatment effect is defined as

$$ g(x) = \exp(2x_1) + 3\sin(4x_2).$$

.. GENERATED FROM PYTHON SOURCE LINES 47-99

.. code-block:: default

    def treatment_effect_1d(x):
        te = np.exp(2 * x) + 3 * np.sin(4 * x)
        return te

    def treatment_effect_2d(x):
        te = np.exp(2 * x[0]) + 3 * np.sin(4 * x[1])
        return te

    def create_synthetic_data(n_samples=200, n_w=30, support_size=5, n_x=1):
        # Outcome support
        # With the next two lines we are effectively choosing the matrix gamma in the example
        support_y = np.random.choice(np.arange(n_w), size=support_size, replace=False)
        coefs_y = np.random.uniform(0, 1, size=support_size)
        # Define the function to generate the noise
        epsilon_sample = lambda n: np.random.uniform(-1, 1, size=n_samples)
        # Treatment support
        # Assuming the matrices gamma and beta have the same non-zero components
        support_t = support_y
        coefs_t = np.random.uniform(0, 1, size=support_size)
        # Define the function to generate the noise
        eta_sample = lambda n: np.random.uniform(-1, 1, size=n_samples)

        # Generate controls, covariates, treatments and outcomes
        w = np.random.normal(0, 1, size=(n_samples, n_w))
        x = np.random.uniform(0, 1, size=(n_samples, n_x))
        # Heterogeneous treatment effects
        if n_x == 1:
            te = np.array([treatment_effect_1d(x_i) for x_i in x]).reshape(-1)
        elif n_x == 2:
            te = np.array([treatment_effect_2d(x_i) for x_i in x]).reshape(-1)
        # Define treatment
        log_odds = np.dot(w[:, support_t], coefs_t) + eta_sample(n_samples)
        t_sigmoid = 1 / (1 + np.exp(-log_odds))
        t = np.array([np.random.binomial(1, p) for p in t_sigmoid])
        # Define the outcome
        y = te * t + np.dot(w[:, support_y], coefs_y) + epsilon_sample(n_samples)

        # Now we build the dataset
        y_df = pd.DataFrame({'y': y})
        if n_x == 1:
            x_df = pd.DataFrame({'x': x.reshape(-1)})
        elif n_x == 2:
            x_df = pd.DataFrame({'x_0': x[:,0],
                                 'x_1': x[:,1]})
        t_df = pd.DataFrame({'t': t})
        w_df = pd.DataFrame(data=w, index=np.arange(w.shape[0]), columns=[f'w_{i}' for i in range(w.shape[1])])

        data = pd.concat([y_df, x_df, t_df, w_df], axis=1)

        covariates = list(w_df.columns.values) + list(x_df.columns.values)
        return data, covariates, te








.. GENERATED FROM PYTHON SOURCE LINES 100-103

## One-dimensional Example

We start with $X$ being one-dimensional and create our training data.

.. GENERATED FROM PYTHON SOURCE LINES 105-106

DGP constants

.. GENERATED FROM PYTHON SOURCE LINES 106-119

.. code-block:: default

    np.random.seed(42)
    n_samples = 2000
    n_w = 10
    support_size = 5
    n_x = 1

    # Create data
    data, covariates, true_effect = create_synthetic_data(n_samples=n_samples, n_w=n_w, support_size=support_size, n_x=n_x)
    data_dml_base = dml.DoubleMLData(data,
                                     y_col='y',
                                     d_cols='t',
                                     x_cols=covariates)








.. GENERATED FROM PYTHON SOURCE LINES 120-121

Next, define the learners for the nuisance functions and fit the [IRM Model](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLIRM.html). Remark that the learners are not optimal for the linear form of this example.

.. GENERATED FROM PYTHON SOURCE LINES 123-124

First stage estimation

.. GENERATED FROM PYTHON SOURCE LINES 124-138

.. code-block:: default

    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    randomForest_reg = RandomForestRegressor(n_estimators=500)
    randomForest_class = RandomForestClassifier(n_estimators=500)

    np.random.seed(42)

    dml_irm = dml.DoubleMLIRM(data_dml_base,
                              ml_g=randomForest_reg,
                              ml_m=randomForest_class,
                              trimming_threshold=0.01,
                              n_folds=5)
    print("Training IRM Model")
    dml_irm.fit()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training IRM Model

    <doubleml.double_ml_irm.DoubleMLIRM object at 0x000001E6DDD27220>



.. GENERATED FROM PYTHON SOURCE LINES 139-140

To estimate the CATE, we rely on the best-linear-predictor of the linear score as in [Semenova et al. (2021)](https://doi.org/10.1093/ectj/utaa027) To approximate the target function $g(x)$ with a linear form, we have to define a data frame of basis functions. Here, we rely on [patsy](https://patsy.readthedocs.io/en/latest/) to construct a suitable basis of [B-splines](https://en.wikipedia.org/wiki/B-spline).

.. GENERATED FROM PYTHON SOURCE LINES 142-146

.. code-block:: default

    import patsy
    design_matrix = patsy.dmatrix("bs(x, df=5, degree=2)", {"x":data["x"]})
    spline_basis = pd.DataFrame(design_matrix)








.. GENERATED FROM PYTHON SOURCE LINES 147-148

To estimate the parameters to calculate the CATE estimate call the ``cate()`` method and supply the dataframe of basis elements.

.. GENERATED FROM PYTHON SOURCE LINES 150-153

.. code-block:: default

    cate = dml_irm.cate(spline_basis)
    print(cate)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ================== DoubleMLBLP Object ==================

    ------------------ Fit summary ------------------
           coef   std err          t          P>|t|    [0.025    0.975]
    0  0.803791  0.187235   4.292962   1.847588e-05  0.436595  1.170987
    1  2.314264  0.312868   7.396925   2.042198e-13  1.700681  2.927847
    2  4.728872  0.200007  23.643503  3.756644e-109  4.336627  5.121117
    3  4.498218  0.239422  18.787794   1.230483e-72  4.028674  4.967763
    4  3.863826  0.245961  15.709100   1.614717e-52  3.381458  4.346193
    5  4.109739  0.266586  15.416163   9.566667e-51  3.586922  4.632556




.. GENERATED FROM PYTHON SOURCE LINES 154-158

To obtain the confidence intervals for the CATE, we have to call the ``confint()`` method and a supply a dataframe of basis elements.
This could be the same basis as for fitting the CATE model or a new basis to e.g. evaluate the CATE model on a grid.
Here, we will evaluate the CATE on a grid from 0.1 to 0.9 to plot the final results.
Further, we construct uniform confidence intervals by setting the option ``joint`` and providing a number of bootstrap repetitions ``n_rep_boot``.

.. GENERATED FROM PYTHON SOURCE LINES 160-165

.. code-block:: default

    new_data = {"x": np.linspace(0.1, 0.9, 100)}
    spline_grid = pd.DataFrame(patsy.build_design_matrices([design_matrix.design_info], new_data)[0])
    df_cate = cate.confint(spline_grid, level=0.95, joint=True, n_rep_boot=2000)
    print(df_cate)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

           2.5 %    effect    97.5 %
    0   2.162122  2.487363  2.812604
    1   2.280823  2.607708  2.934594
    2   2.394917  2.725681  3.056445
    3   2.505394  2.841281  3.177167
    4   2.613100  2.954508  3.295916
    ..       ...       ...       ...
    95  4.480330  4.812276  5.144223
    96  4.483393  4.809948  5.136503
    97  4.487370  4.808733  5.130095
    98  4.491619  4.808631  5.125642
    99  4.495382  4.809642  5.123901

    [100 rows x 3 columns]




.. GENERATED FROM PYTHON SOURCE LINES 166-167

Finally, we can plot our results and compare them with the true effect.

.. GENERATED FROM PYTHON SOURCE LINES 169-184

.. code-block:: default

    from matplotlib import pyplot as plt
    plt.rcParams['figure.figsize'] = 10., 7.5

    df_cate['x'] = new_data['x']
    df_cate['true_effect'] = treatment_effect_1d(new_data['x'])
    fig, ax = plt.subplots()
    ax.plot(df_cate['x'],df_cate['effect'], label='Estimated Effect')
    ax.plot(df_cate['x'],df_cate['true_effect'], color="green", label='True Effect')
    ax.fill_between(df_cate['x'], df_cate['2.5 %'], df_cate['97.5 %'], color='b', alpha=.3, label='Confidence Interval')

    plt.legend()
    plt.title('CATE')
    plt.xlabel('x')
    _ =  plt.ylabel('Effect and 95%-CI')




.. image-sg:: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cate_001.png
   :alt: CATE
   :srcset: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cate_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 185-188

## Two-Dimensional Example

It is also possible to estimate multi-dimensional conditional effects. We will use the same data-generating process as above, but let $X$ be two-dimensional.

.. GENERATED FROM PYTHON SOURCE LINES 190-191

DGP constants

.. GENERATED FROM PYTHON SOURCE LINES 191-197

.. code-block:: default

    np.random.seed(42)
    n_samples = 5000
    n_w = 10
    support_size = 5
    n_x = 2








.. GENERATED FROM PYTHON SOURCE LINES 198-199

Create data

.. GENERATED FROM PYTHON SOURCE LINES 199-205

.. code-block:: default

    data, covariates, true_effect = create_synthetic_data(n_samples=n_samples, n_w=n_w, support_size=support_size, n_x=n_x)
    data_dml_base = dml.DoubleMLData(data,
                                     y_col='y',
                                     d_cols='t',
                                     x_cols=covariates)








.. GENERATED FROM PYTHON SOURCE LINES 206-207

As univariate example estimate the [IRM Model](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLIRM.html).

.. GENERATED FROM PYTHON SOURCE LINES 209-210

First stage estimation

.. GENERATED FROM PYTHON SOURCE LINES 210-224

.. code-block:: default

    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    randomForest_reg = RandomForestRegressor(n_estimators=500)
    randomForest_class = RandomForestClassifier(n_estimators=500)

    np.random.seed(123)

    dml_irm = dml.DoubleMLIRM(data_dml_base,
                              ml_g=randomForest_reg,
                              ml_m=randomForest_class,
                              trimming_threshold=0.01,
                              n_folds=5)
    print("Training IRM Model")
    dml_irm.fit()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training IRM Model

    <doubleml.double_ml_irm.DoubleMLIRM object at 0x000001E6DFF2B3D0>



.. GENERATED FROM PYTHON SOURCE LINES 225-227

As above, we will rely on the [patsy](https://patsy.readthedocs.io/en/latest/) package to construct the basis elements.
In the two-dimensional case, we will construct a tensor product of B-splines (for more information see [here](https://patsy.readthedocs.io/en/latest/spline-regression.html#tensor-product-smooths)).

.. GENERATED FROM PYTHON SOURCE LINES 229-235

.. code-block:: default

    design_matrix = patsy.dmatrix("te(bs(x_0, df=7, degree=3), bs(x_1, df=7, degree=3))", {"x_0": data["x_0"], "x_1": data["x_1"]})
    spline_basis = pd.DataFrame(design_matrix)

    cate = dml_irm.cate(spline_basis)
    print(cate)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ================== DoubleMLBLP Object ==================

    ------------------ Fit summary ------------------
             coef   std err          t          P>|t|     [0.025     0.975]
    0    2.919648  0.131732  22.163500  7.505209e-104   2.661394   3.177901
    1   -3.130747  1.054532  -2.968848   3.003512e-03  -5.198098  -1.063396
    2    1.316476  1.082389   1.216269   2.239404e-01  -0.805486   3.438437
    3    3.767636  0.932460   4.040533   5.413836e-05   1.939601   5.595672
    4    1.130924  0.960049   1.177985   2.388591e-01  -0.751198   3.013046
    5   -4.096709  1.163356  -3.521459   4.330488e-04  -6.377402  -1.816016
    6   -4.555625  1.255806  -3.627650   2.888950e-04  -7.017561  -2.093688
    7   -8.178163  1.354629  -6.037198   1.682499e-09 -10.833836  -5.522489
    8   -0.682519  1.128264  -0.604929   5.452542e-01  -2.894416   1.529378
    9    0.594137  1.175277   0.505529   6.132097e-01  -1.709928   2.898202
    10   1.110112  0.966559   1.148520   2.508094e-01  -0.784771   3.004995
    11  -1.482823  1.004768  -1.475786   1.400648e-01  -3.452613   0.486968
    12   1.020411  1.247564   0.817922   4.134409e-01  -1.425368   3.466189
    13  -3.180077  1.300553  -2.445173   1.451287e-02  -5.729736  -0.630417
    14   0.089071  1.139368   0.078176   9.376916e-01  -2.144596   2.322738
    15   0.390760  0.984341   0.396977   6.914017e-01  -1.538984   2.320504
    16   1.266491  1.008145   1.256260   2.090812e-01  -0.709919   3.242902
    17   2.721208  0.825326   3.297129   9.836757e-04   1.103202   4.339214
    18   2.038821  0.818614   2.490576   1.278616e-02   0.433974   3.643669
    19  -1.585979  1.023183  -1.550044   1.211949e-01  -3.591871   0.419914
    20  -3.513600  1.112878  -3.157220   1.602380e-03  -5.695334  -1.331866
    21  -4.983581  0.955229  -5.217161   1.890983e-07  -6.856252  -3.110909
    22   0.847614  0.981734   0.863384   3.879680e-01  -1.077020   2.772247
    23   3.496231  1.047460   3.337817   8.506253e-04   1.442745   5.549718
    24   4.307474  0.855433   5.035430   4.936977e-07   2.630446   5.984502
    25   2.324569  0.839842   2.767865   5.663466e-03   0.678106   3.971031
    26   0.300033  1.018906   0.294466   7.684145e-01  -1.697474   2.297539
    27  -1.867418  1.128209  -1.655207   9.794598e-02  -4.079207   0.344371
    28   0.391576  1.040519   0.376327   7.066896e-01  -1.648302   2.431454
    29   5.356217  1.207016   4.437568   9.295674e-06   2.989930   7.722505
    30   1.176939  1.297555   0.907043   3.644280e-01  -1.366844   3.720722
    31   5.537472  1.055507   5.246268   1.616827e-07   3.468210   7.606733
    32   2.889968  1.087127   2.658353   7.877617e-03   0.758717   5.021219
    33   2.076608  1.314694   1.579537   1.142769e-01  -0.500776   4.653992
    34   0.409390  1.401057   0.292201   7.701453e-01  -2.337302   3.156082
    35  -3.041573  1.421668  -2.139439   3.244887e-02  -5.828674  -0.254473
    36   5.040965  1.308198   3.853364   1.179805e-04   2.476316   7.605614
    37   7.435404  1.401874   5.303903   1.182869e-07   4.687109  10.183698
    38   7.697526  1.149373   6.697154   2.361537e-11   5.444246   9.950807
    39   6.226618  1.192212   5.222744   1.835130e-07   3.889354   8.563882
    40   2.114851  1.415960   1.493581   1.353489e-01  -0.661058   4.890759
    41   3.663107  1.513740   2.419905   1.556035e-02   0.695506   6.630709
    42   2.833281  1.532167   1.849199   6.448867e-02  -0.170445   5.837007
    43  10.009858  1.278242   7.830957   5.881139e-15   7.503937  12.515779
    44   4.348488  1.351513   3.217495   1.301484e-03   1.698922   6.998053
    45   7.467394  1.129961   6.608539   4.290017e-11   5.252169   9.682619
    46   6.725389  1.125483   5.975556   2.453538e-09   4.518943   8.931836
    47   5.269670  1.425440   3.696872   2.206345e-04   2.475175   8.064164
    48   0.107168  1.519445   0.070531   9.437741e-01  -2.871619   3.085954
    49   3.113977  1.414928   2.200802   2.779607e-02   0.340090   5.887863




.. GENERATED FROM PYTHON SOURCE LINES 236-237

Finally, we create a new grid to evaluate and plot the effects.

.. GENERATED FROM PYTHON SOURCE LINES 239-246

.. code-block:: default

    grid_size = 100
    x_0 = np.linspace(0.1, 0.9, grid_size)
    x_1 = np.linspace(0.1, 0.9, grid_size)
    x_0, x_1 = np.meshgrid(x_0, x_1)

    new_data = {"x_0": x_0.ravel(), "x_1": x_1.ravel()}








.. GENERATED FROM PYTHON SOURCE LINES 247-251

.. code-block:: default

    spline_grid = pd.DataFrame(patsy.build_design_matrices([design_matrix.design_info], new_data)[0])
    df_cate = cate.confint(spline_grid, joint=True, n_rep_boot=2000)
    print(df_cate)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

             2.5 %    effect    97.5 %
    0     1.167304  1.995776  2.824247
    1     1.208964  2.007761  2.806558
    2     1.262869  2.028352  2.793834
    3     1.325047  2.056630  2.788214
    4     1.391454  2.091680  2.791905
    ...        ...       ...       ...
    9995  4.099745  4.866276  5.632807
    9996  4.163580  4.963192  5.762804
    9997  4.221636  5.053162  5.884688
    9998  4.275130  5.135095  5.995060
    9999  4.324927  5.207899  6.090872

    [10000 rows x 3 columns]




.. GENERATED FROM PYTHON SOURCE LINES 252-283

.. code-block:: default

    import plotly.graph_objects as go

    true_effect = np.array([treatment_effect_2d(x_i) for x_i in zip(x_0.ravel(), x_1.ravel())]).reshape(x_0.shape)
    effect = np.asarray(df_cate['effect']).reshape(x_0.shape)
    lower_bound = np.asarray(df_cate['2.5 %']).reshape(x_0.shape)
    upper_bound = np.asarray(df_cate['97.5 %']).reshape(x_0.shape)

    fig = go.Figure(data=[
        go.Surface(x=x_0,
                   y=x_1,
                   z=true_effect),
        go.Surface(x=x_0,
                   y=x_1,
                   z=upper_bound, showscale=False, opacity=0.4,colorscale='purp'),
        go.Surface(x=x_0,
                   y=x_1,
                   z=lower_bound, showscale=False, opacity=0.4,colorscale='purp'),
    ])
    fig.update_traces(contours_z=dict(show=True, usecolormap=True,
                                      highlightcolor="limegreen", project_z=True))

    fig.update_layout(scene = dict(
                        xaxis_title='X_0',
                        yaxis_title='X_1',
                        zaxis_title='Effect'),
                        width=700,
                        margin=dict(r=20, b=10, l=10, t=10))

    fig.show()










.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  18.971 seconds)


.. _sphx_glr_download_examples_auto_examples_python_examples_plot_py_double_ml_cate.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_py_double_ml_cate.py <plot_py_double_ml_cate.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_py_double_ml_cate.ipynb <plot_py_double_ml_cate.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
