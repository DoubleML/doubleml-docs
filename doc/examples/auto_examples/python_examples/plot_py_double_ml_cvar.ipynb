{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CVAR\n\nThis is an example of how to use the double machine learning approach for conditional value-at-risk estimation.\nThe code demonstrates the usage of the `DoubleMLCateEstimator` class and provides a step-by-step guide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python: Conditional Value at Risk of potential outcomes\nIn this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the conditional Value at Risk of potential outcomes. The estimation is based on [Kallus  et al. (2019)](https://arxiv.org/abs/1912.12945).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\nWe define a data generating process to create synthetic data to compare the estimates to the true effect.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport doubleml as dml\nimport multiprocessing\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is generated as a location-scale model with\n\n$$Y_i = \\text{loc}(D_i,X_i) + \\text{scale}(D_i,X_i)\\cdot\\varepsilon_i,$$\n\nwhere $X_i\\sim\\mathcal{U}[-1,1]^{p}$ and $\\varepsilon_i \\sim \\mathcal{N}(0,1)$.\nFurther, the location and scale are determined according to the following functions\n\n$$\\begin{aligned}\n\\text{loc}(d,x) &:= 0.5d + 2dx_5 + 2\\cdot 1\\{x_2 > 0.1\\} - 1.7\\cdot 1\\{x_1x_3 > 0\\} - 3x_4 \\\\\n\\text{scale}(d,x) &:= \\sqrt{0.5d + 0.3dx_1 + 2},\n\\end{aligned}$$\n\nand the treatment takes the following form\n\n$$D_i = 1_{\\{(X_2 - X_4 + 1.5\\cdot 1\\{x_1 > 0\\} + \\epsilon_i > 0)\\}}$$\n\nwith $\\epsilon_i \\sim \\mathcal{N}(0,1)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f_loc(D, X):\n  loc = 0.5*D + 2*D*X[:,4] + 2.0*(X[:,1] > 0.1) - 1.7*(X[:,0] * X[:,2] > 0) - 3*X[:,3]\n  return loc\n\ndef f_scale(D, X):\n  scale = np.sqrt(0.5*D + 0.3*D*X[:,1] + 2)\n  return scale\n\ndef dgp(n=200, p=5):\n    X = np.random.uniform(-1,1,size=[n,p])\n    D = ((X[:,1 ] - X[:,3] + 1.5*(X[:,0] > 0) + np.random.normal(size=n)) > 0)*1.0\n    epsilon = np.random.normal(size=n)\n\n    Y = f_loc(D, X) + f_scale(D, X)*epsilon\n\n    return Y, X, D, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can calculate the true conditional value at risk through simulations. Here, we will just approximate the true conditional value at risk for the potential outcomes for a range of quantiles.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tau_vec = np.arange(0.1,0.95,0.05)\np = 5\nn_true = int(10e+6)\n\n_, X_true, _, epsilon_true = dgp(n=n_true, p = p)\nD1 = np.ones(n_true)\nD0 = np.zeros(n_true)\n\nY1 = f_loc(D1, X_true) + f_scale(D1, X_true)*epsilon_true\nY0 = f_loc(D0, X_true) + f_scale(D0, X_true)*epsilon_true\n\nY1_quant = np.quantile(Y1, q=tau_vec)\nY0_quant = np.quantile(Y0, q=tau_vec)\n\nY1_cvar = [Y1[Y1 >= quant].mean() for quant in Y1_quant]\nY0_cvar = [Y0[Y0 >= quant].mean() for quant in Y0_quant]\n\nprint(f'Conditional Value at Risk Y(0): {Y0_cvar}')\nprint(f'Conditional Value at Risk Y(1): {Y1_cvar}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us generate $n=5000$ observations and convert them to a [DoubleMLData](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLData.html) object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 5000\nnp.random.seed(42)\nY, X, D, _ = dgp(n=n,p=p)\nobj_dml_data = dml.DoubleMLData.from_arrays(X, Y, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional Value at Risk (CVaR)\nNext, we can initialize our two machine learning algorithms to train the different nuisance elements (remark that in contrast to potential quantile estimation `ml_g` is a regressor). Then we can initialize the `DoubleMLCVAR` objects and call `fit()` to estimate the relevant parameters. To obtain confidence intervals, we can use the `confint()` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ml_g = LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=10)\nml_m = LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=10)\n\nCVAR_0 = np.full((len(tau_vec)), np.nan)\nCVAR_1 = np.full((len(tau_vec)), np.nan)\n\nci_CVAR_0 = np.full((len(tau_vec),2), np.nan)\nci_CVAR_1 = np.full((len(tau_vec),2), np.nan)\n\nfor idx_tau, tau in enumerate(tau_vec):\n    print(f'Quantile: {tau}')\n    dml_CVAR_0 = dml.DoubleMLCVAR(obj_dml_data,\n                                ml_g, ml_m,\n                                quantile=tau,\n                                treatment=0,\n                                n_folds=5)\n    dml_CVAR_1 = dml.DoubleMLCVAR(obj_dml_data,\n                                ml_g, ml_m,\n                                quantile=tau,\n                                treatment=1,\n                                n_folds=5)\n\n    dml_CVAR_0.fit()\n    dml_CVAR_1.fit()\n\n    ci_CVAR_0[idx_tau, :] = dml_CVAR_0.confint(level=0.95).to_numpy()\n    ci_CVAR_1[idx_tau, :] = dml_CVAR_1.confint(level=0.95).to_numpy()\n\n    CVAR_0[idx_tau] = dml_CVAR_0.coef\n    CVAR_1[idx_tau] = dml_CVAR_1.coef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let us take a look at the estimated values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = {\"Quantile\": tau_vec, \"CVaR Y(0)\": Y0_cvar, \"CVaR Y(1)\": Y1_cvar,\n        \"DML CVaR Y(0)\": CVAR_0, \"DML CVaR Y(1)\": CVAR_1,\n        \"DML CVaR Y(0) lower\": ci_CVAR_0[:, 0], \"DML CVaR Y(0) upper\": ci_CVAR_0[:, 1],\n        \"DML CVaR Y(1) lower\": ci_CVAR_1[:, 0], \"DML CVaR Y(1) upper\": ci_CVAR_1[:, 1]}\ndf = pd.DataFrame(data)\nprint(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = 10., 7.5\nfig, (ax1, ax2) = plt.subplots(1 ,2)\nax1.grid(); ax2.grid()\n\nax1.plot(df['Quantile'],df['DML CVaR Y(0)'], color='violet', label='Estimated CVaR Y(0)')\nax1.plot(df['Quantile'],df['CVaR Y(0)'], color='green', label='True CVaR Y(0)')\nax1.fill_between(df['Quantile'], df['DML CVaR Y(0) lower'], df['DML CVaR Y(0) upper'], color='violet', alpha=.3, label='Confidence Interval')\nax1.legend()\nax1.set_ylim(-2, 6)\n\nax2.plot(df['Quantile'],df['DML CVaR Y(1)'], color='violet', label='Estimated CVaR Y(1)')\nax2.plot(df['Quantile'],df['CVaR Y(1)'], color='green', label='True CVaR Y(1)')\nax2.fill_between(df['Quantile'], df['DML CVaR Y(1) lower'], df['DML CVaR Y(1) upper'], color='violet', alpha=.3, label='Confidence Interval')\nax2.legend()\nax2.set_ylim(-2, 6)\n\nfig.suptitle('Conditional Value at Risk', fontsize=16)\nfig.supxlabel('Quantile')\n_ = fig.supylabel('CVaR and 95%-CI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CVaR Treatment Effects\nIn most cases, we want to evaluate the treatment effect on the CVaR as the difference between potential CVaRs.\nTo estimate the treatment effect, we can use the `DoubleMLQTE` object and specify `CVaR` as the score. \n\nAs for quantile treatment effects, different quantiles can be estimated in parallel with `n_jobs_models`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_cores = multiprocessing.cpu_count()\ncores_used = np.min([5, n_cores - 1])\nprint(f\"Number of Cores used: {cores_used}\")\n\ndml_CVAR = dml.DoubleMLQTE(obj_dml_data,\n                           ml_g,\n                           ml_m,\n                           score='CVaR',\n                           quantiles=tau_vec,\n                           n_folds=5)\ndml_CVAR.fit(n_jobs_models=cores_used)\nprint(dml_CVAR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for standard `DoubleMLCVAR` objects, we can construct valid confidencebands with the `confint()` method. Additionally, it might be helpful to construct uniformly valid confidence regions via boostrap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ci_CVAR = dml_CVAR.confint(level=0.95, joint=False)\n\ndml_CVAR.bootstrap(n_rep_boot=2000)\nci_joint_CVAR = dml_CVAR.confint(level=0.95, joint=True)\nci_joint_CVAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can compare the predicted treatment effect with the true treatment effect on the CVaR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CVAR = np.array(Y1_cvar) - np.array(Y0_cvar)\ndata = {\"Quantile\": tau_vec, \"CVaR\": CVAR, \"DML CVaR\": dml_CVAR.coef,\n        \"DML CVaR pointwise lower\": ci_CVAR['2.5 %'], \"DML CVaR pointwise upper\": ci_CVAR['97.5 %'],\n        \"DML CVaR joint lower\": ci_joint_CVAR['2.5 %'], \"DML CVaR joint upper\": ci_joint_CVAR['97.5 %']}\ndf = pd.DataFrame(data)\nprint(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = 10., 7.5\nfig, ax = plt.subplots()\nax.grid()\n\nax.plot(df['Quantile'],df['DML CVaR'], color='violet', label='Estimated CVaR')\nax.plot(df['Quantile'],df['CVaR'], color='green', label='True CVaR')\nax.fill_between(df['Quantile'], df['DML CVaR pointwise lower'], df['DML CVaR pointwise upper'], color='violet', alpha=.3, label='Pointwise Confidence Interval')\nax.fill_between(df['Quantile'], df['DML CVaR joint lower'], df['DML CVaR joint upper'], color='violet', alpha=.2, label='Joint Confidence Interval')\n\nplt.legend()\nplt.title('Conditional Value at Risk', fontsize=16)\nplt.xlabel('Quantile')\n_ = plt.ylabel('QTE and 95%-CI')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}