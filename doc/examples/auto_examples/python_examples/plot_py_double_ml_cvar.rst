
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples\auto_examples\python_examples\plot_py_double_ml_cvar.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_auto_examples_python_examples_plot_py_double_ml_cvar.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_auto_examples_python_examples_plot_py_double_ml_cvar.py:


CVAR
===================================

This is an example of how to use the double machine learning approach for conditional value-at-risk estimation.
The code demonstrates the usage of the `DoubleMLCateEstimator` class and provides a step-by-step guide.

.. GENERATED FROM PYTHON SOURCE LINES 11-13

# Python: Conditional Value at Risk of potential outcomes
In this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the conditional Value at Risk of potential outcomes. The estimation is based on [Kallus  et al. (2019)](https://arxiv.org/abs/1912.12945).

.. GENERATED FROM PYTHON SOURCE LINES 15-17

## Data
We define a data generating process to create synthetic data to compare the estimates to the true effect.

.. GENERATED FROM PYTHON SOURCE LINES 19-26

.. code-block:: default

    import numpy as np
    import pandas as pd
    import doubleml as dml
    import multiprocessing

    from lightgbm import LGBMClassifier, LGBMRegressor








.. GENERATED FROM PYTHON SOURCE LINES 27-44

The data is generated as a location-scale model with

$$Y_i = \text{loc}(D_i,X_i) + \text{scale}(D_i,X_i)\cdot\varepsilon_i,$$

where $X_i\sim\mathcal{U}[-1,1]^{p}$ and $\varepsilon_i \sim \mathcal{N}(0,1)$.
Further, the location and scale are determined according to the following functions

$$\begin{aligned}
\text{loc}(d,x) &:= 0.5d + 2dx_5 + 2\cdot 1\{x_2 > 0.1\} - 1.7\cdot 1\{x_1x_3 > 0\} - 3x_4 \\
\text{scale}(d,x) &:= \sqrt{0.5d + 0.3dx_1 + 2},
\end{aligned}$$

and the treatment takes the following form

$$D_i = 1_{\{(X_2 - X_4 + 1.5\cdot 1\{x_1 > 0\} + \epsilon_i > 0)\}}$$

with $\epsilon_i \sim \mathcal{N}(0,1)$.

.. GENERATED FROM PYTHON SOURCE LINES 46-63

.. code-block:: default

    def f_loc(D, X):
      loc = 0.5*D + 2*D*X[:,4] + 2.0*(X[:,1] > 0.1) - 1.7*(X[:,0] * X[:,2] > 0) - 3*X[:,3]
      return loc

    def f_scale(D, X):
      scale = np.sqrt(0.5*D + 0.3*D*X[:,1] + 2)
      return scale

    def dgp(n=200, p=5):
        X = np.random.uniform(-1,1,size=[n,p])
        D = ((X[:,1 ] - X[:,3] + 1.5*(X[:,0] > 0) + np.random.normal(size=n)) > 0)*1.0
        epsilon = np.random.normal(size=n)

        Y = f_loc(D, X) + f_scale(D, X)*epsilon

        return Y, X, D, epsilon








.. GENERATED FROM PYTHON SOURCE LINES 64-65

We can calculate the true conditional value at risk through simulations. Here, we will just approximate the true conditional value at risk for the potential outcomes for a range of quantiles.

.. GENERATED FROM PYTHON SOURCE LINES 67-87

.. code-block:: default

    tau_vec = np.arange(0.1,0.95,0.05)
    p = 5
    n_true = int(10e+6)

    _, X_true, _, epsilon_true = dgp(n=n_true, p = p)
    D1 = np.ones(n_true)
    D0 = np.zeros(n_true)

    Y1 = f_loc(D1, X_true) + f_scale(D1, X_true)*epsilon_true
    Y0 = f_loc(D0, X_true) + f_scale(D0, X_true)*epsilon_true

    Y1_quant = np.quantile(Y1, q=tau_vec)
    Y0_quant = np.quantile(Y0, q=tau_vec)

    Y1_cvar = [Y1[Y1 >= quant].mean() for quant in Y1_quant]
    Y0_cvar = [Y0[Y0 >= quant].mean() for quant in Y0_quant]

    print(f'Conditional Value at Risk Y(0): {Y0_cvar}')
    print(f'Conditional Value at Risk Y(1): {Y1_cvar}')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Conditional Value at Risk Y(0): [0.5480562107249161, 0.7572832061619581, 0.9581149429592241, 1.154495507610463, 1.3491331696319826, 1.544021414924525, 1.7409780950052371, 1.9418382751086667, 2.148484910248208, 2.363137852162645, 2.5884458556272567, 2.827938410060375, 3.086392654888284, 3.3709617002658234, 3.693042566281823, 4.0730905513697495, 4.554960713829936]
    Conditional Value at Risk Y(1): [1.1135474746771024, 1.3479971633386079, 1.5727061442597958, 1.7923362669984078, 2.0099935345474678, 2.228145982369703, 2.4488830292756383, 2.6743634995022316, 2.906853304705428, 3.148811065698498, 3.4034825848105807, 3.6752588415046854, 3.9699527038739864, 4.295980360130773, 4.667282127962201, 5.108691284395785, 5.672944773495295]




.. GENERATED FROM PYTHON SOURCE LINES 88-89

Let us generate $n=5000$ observations and convert them to a [DoubleMLData](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLData.html) object.

.. GENERATED FROM PYTHON SOURCE LINES 91-96

.. code-block:: default

    n = 5000
    np.random.seed(42)
    Y, X, D, _ = dgp(n=n,p=p)
    obj_dml_data = dml.DoubleMLData.from_arrays(X, Y, D)








.. GENERATED FROM PYTHON SOURCE LINES 97-99

## Conditional Value at Risk (CVaR)
Next, we can initialize our two machine learning algorithms to train the different nuisance elements (remark that in contrast to potential quantile estimation `ml_g` is a regressor). Then we can initialize the `DoubleMLCVAR` objects and call `fit()` to estimate the relevant parameters. To obtain confidence intervals, we can use the `confint()` method.

.. GENERATED FROM PYTHON SOURCE LINES 101-132

.. code-block:: default

    ml_g = LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=10)
    ml_m = LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=10)

    CVAR_0 = np.full((len(tau_vec)), np.nan)
    CVAR_1 = np.full((len(tau_vec)), np.nan)

    ci_CVAR_0 = np.full((len(tau_vec),2), np.nan)
    ci_CVAR_1 = np.full((len(tau_vec),2), np.nan)

    for idx_tau, tau in enumerate(tau_vec):
        print(f'Quantile: {tau}')
        dml_CVAR_0 = dml.DoubleMLCVAR(obj_dml_data,
                                    ml_g, ml_m,
                                    quantile=tau,
                                    treatment=0,
                                    n_folds=5)
        dml_CVAR_1 = dml.DoubleMLCVAR(obj_dml_data,
                                    ml_g, ml_m,
                                    quantile=tau,
                                    treatment=1,
                                    n_folds=5)

        dml_CVAR_0.fit()
        dml_CVAR_1.fit()

        ci_CVAR_0[idx_tau, :] = dml_CVAR_0.confint(level=0.95).to_numpy()
        ci_CVAR_1[idx_tau, :] = dml_CVAR_1.confint(level=0.95).to_numpy()

        CVAR_0[idx_tau] = dml_CVAR_0.coef
        CVAR_1[idx_tau] = dml_CVAR_1.coef





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Quantile: 0.1
    Quantile: 0.15000000000000002
    Quantile: 0.20000000000000004
    Quantile: 0.25000000000000006
    Quantile: 0.30000000000000004
    Quantile: 0.3500000000000001
    Quantile: 0.40000000000000013
    Quantile: 0.45000000000000007
    Quantile: 0.5000000000000001
    Quantile: 0.5500000000000002
    Quantile: 0.6000000000000002
    Quantile: 0.6500000000000001
    Quantile: 0.7000000000000002
    Quantile: 0.7500000000000002
    Quantile: 0.8000000000000002
    Quantile: 0.8500000000000002
    Quantile: 0.9000000000000002




.. GENERATED FROM PYTHON SOURCE LINES 133-134

Finally, let us take a look at the estimated values.

.. GENERATED FROM PYTHON SOURCE LINES 136-143

.. code-block:: default

    data = {"Quantile": tau_vec, "CVaR Y(0)": Y0_cvar, "CVaR Y(1)": Y1_cvar,
            "DML CVaR Y(0)": CVAR_0, "DML CVaR Y(1)": CVAR_1,
            "DML CVaR Y(0) lower": ci_CVAR_0[:, 0], "DML CVaR Y(0) upper": ci_CVAR_0[:, 1],
            "DML CVaR Y(1) lower": ci_CVAR_1[:, 0], "DML CVaR Y(1) upper": ci_CVAR_1[:, 1]}
    df = pd.DataFrame(data)
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

        Quantile  CVaR Y(0)  CVaR Y(1)  DML CVaR Y(0)  DML CVaR Y(1)  DML CVaR Y(0) lower  DML CVaR Y(0) upper  DML CVaR Y(1) lower  DML CVaR Y(1) upper
    0       0.10   0.548056   1.113547       0.360683       1.057962             0.162710             0.558655             0.957745             1.158178
    1       0.15   0.757283   1.347997       0.590911       1.273356             0.360801             0.821021             1.175284             1.371429
    2       0.20   0.958115   1.572706       0.829543       1.489699             0.606342             1.052745             1.393604             1.585793
    3       0.25   1.154496   1.792336       1.015038       1.697000             0.824889             1.205187             1.601061             1.792939
    4       0.30   1.349133   2.009994       1.203284       1.925736             1.009428             1.397140             1.824750             2.026723
    5       0.35   1.544021   2.228146       1.502494       2.144084             1.292028             1.712960             2.041147             2.247020
    6       0.40   1.740978   2.448883       1.678826       2.338775             1.455078             1.902573             2.234534             2.443016
    7       0.45   1.941838   2.674363       1.822482       2.559144             1.579238             2.065725             2.455107             2.663182
    8       0.50   2.148485   2.906853       2.153119       2.824701             1.883914             2.422325             2.715407             2.933996
    9       0.55   2.363138   3.148811       2.156969       3.041831             1.907491             2.406446             2.932027             3.151636
    10      0.60   2.588446   3.403483       2.495657       3.298120             2.250210             2.741104             3.183526             3.412714
    11      0.65   2.827938   3.675259       2.653846       3.582761             2.382872             2.924821             3.466440             3.699082
    12      0.70   3.086393   3.969953       2.847948       3.842405             2.554076             3.141820             3.722848             3.961962
    13      0.75   3.370962   4.295980       3.076347       4.163895             2.727976             3.424717             4.041284             4.286507
    14      0.80   3.693043   4.667282       3.523163       4.543075             3.140833             3.905494             4.409746             4.676405
    15      0.85   4.073091   5.108691       3.869020       4.913774             3.483717             4.254324             4.773177             5.054370
    16      0.90   4.554961   5.672945       4.372097       5.482038             3.921372             4.822822             5.313209             5.650867




.. GENERATED FROM PYTHON SOURCE LINES 144-165

.. code-block:: default

    from matplotlib import pyplot as plt
    plt.rcParams['figure.figsize'] = 10., 7.5
    fig, (ax1, ax2) = plt.subplots(1 ,2)
    ax1.grid(); ax2.grid()

    ax1.plot(df['Quantile'],df['DML CVaR Y(0)'], color='violet', label='Estimated CVaR Y(0)')
    ax1.plot(df['Quantile'],df['CVaR Y(0)'], color='green', label='True CVaR Y(0)')
    ax1.fill_between(df['Quantile'], df['DML CVaR Y(0) lower'], df['DML CVaR Y(0) upper'], color='violet', alpha=.3, label='Confidence Interval')
    ax1.legend()
    ax1.set_ylim(-2, 6)

    ax2.plot(df['Quantile'],df['DML CVaR Y(1)'], color='violet', label='Estimated CVaR Y(1)')
    ax2.plot(df['Quantile'],df['CVaR Y(1)'], color='green', label='True CVaR Y(1)')
    ax2.fill_between(df['Quantile'], df['DML CVaR Y(1) lower'], df['DML CVaR Y(1) upper'], color='violet', alpha=.3, label='Confidence Interval')
    ax2.legend()
    ax2.set_ylim(-2, 6)

    fig.suptitle('Conditional Value at Risk', fontsize=16)
    fig.supxlabel('Quantile')
    _ = fig.supylabel('CVaR and 95%-CI')




.. image-sg:: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cvar_001.png
   :alt: Conditional Value at Risk
   :srcset: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cvar_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 166-171

## CVaR Treatment Effects
In most cases, we want to evaluate the treatment effect on the CVaR as the difference between potential CVaRs.
To estimate the treatment effect, we can use the `DoubleMLQTE` object and specify `CVaR` as the score. 

As for quantile treatment effects, different quantiles can be estimated in parallel with `n_jobs_models`.

.. GENERATED FROM PYTHON SOURCE LINES 173-186

.. code-block:: default

    n_cores = multiprocessing.cpu_count()
    cores_used = np.min([5, n_cores - 1])
    print(f"Number of Cores used: {cores_used}")

    dml_CVAR = dml.DoubleMLQTE(obj_dml_data,
                               ml_g,
                               ml_m,
                               score='CVaR',
                               quantiles=tau_vec,
                               n_folds=5)
    dml_CVAR.fit(n_jobs_models=cores_used)
    print(dml_CVAR)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of Cores used: 5
    ================== DoubleMLQTE Object ==================

    ------------------ Fit summary       ------------------
              coef   std err         t         P>|t|     2.5 %    97.5 %
    0.10  0.627564  0.103806  6.045553  1.488982e-09  0.424108  0.831019
    0.15  0.677980  0.102616  6.606954  3.923074e-11  0.476856  0.879103
    0.20  0.706645  0.100356  7.041387  1.903351e-12  0.509951  0.903339
    0.25  0.716793  0.102775  6.974414  3.071488e-12  0.515358  0.918227
    0.30  0.716762  0.107073  6.694154  2.169230e-11  0.506903  0.926621
    0.35  0.740869  0.112216  6.602168  4.051867e-11  0.520930  0.960808
    0.40  0.756969  0.114647  6.602628  4.039310e-11  0.532266  0.981672
    0.45  0.751710  0.117710  6.386102  1.701672e-10  0.521002  0.982417
    0.50  0.779682  0.122408  6.369556  1.895768e-10  0.539767  1.019596
    0.55  0.786744  0.130370  6.034690  1.592681e-09  0.531223  1.042265
    0.60  0.814351  0.138378  5.884996  3.980643e-09  0.543136  1.085566
    0.65  0.848868  0.144800  5.862359  4.563374e-09  0.565066  1.132671
    0.70  0.946968  0.154828  6.116274  9.578846e-10  0.643512  1.250425
    0.75  0.997621  0.164805  6.053331  1.418806e-09  0.674609  1.320633
    0.80  1.073520  0.190915  5.623024  1.876431e-08  0.699333  1.447706
    0.85  1.053558  0.236008  4.464076  8.041491e-06  0.590991  1.516125
    0.90  1.097468  0.338908  3.238251  1.202650e-03  0.433221  1.761714




.. GENERATED FROM PYTHON SOURCE LINES 187-188

As for standard `DoubleMLCVAR` objects, we can construct valid confidencebands with the `confint()` method. Additionally, it might be helpful to construct uniformly valid confidence regions via boostrap.

.. GENERATED FROM PYTHON SOURCE LINES 190-196

.. code-block:: default

    ci_CVAR = dml_CVAR.confint(level=0.95, joint=False)

    dml_CVAR.bootstrap(n_rep_boot=2000)
    ci_joint_CVAR = dml_CVAR.confint(level=0.95, joint=True)
    ci_joint_CVAR






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>2.5 %</th>
          <th>97.5 %</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0.10</th>
          <td>0.367571</td>
          <td>0.887556</td>
        </tr>
        <tr>
          <th>0.15</th>
          <td>0.420967</td>
          <td>0.934992</td>
        </tr>
        <tr>
          <th>0.20</th>
          <td>0.455293</td>
          <td>0.957996</td>
        </tr>
        <tr>
          <th>0.25</th>
          <td>0.459383</td>
          <td>0.974202</td>
        </tr>
        <tr>
          <th>0.30</th>
          <td>0.448587</td>
          <td>0.984937</td>
        </tr>
        <tr>
          <th>0.35</th>
          <td>0.459812</td>
          <td>1.021926</td>
        </tr>
        <tr>
          <th>0.40</th>
          <td>0.469825</td>
          <td>1.044113</td>
        </tr>
        <tr>
          <th>0.45</th>
          <td>0.456892</td>
          <td>1.046527</td>
        </tr>
        <tr>
          <th>0.50</th>
          <td>0.473099</td>
          <td>1.086264</td>
        </tr>
        <tr>
          <th>0.55</th>
          <td>0.460218</td>
          <td>1.113270</td>
        </tr>
        <tr>
          <th>0.60</th>
          <td>0.467770</td>
          <td>1.160932</td>
        </tr>
        <tr>
          <th>0.65</th>
          <td>0.486202</td>
          <td>1.211534</td>
        </tr>
        <tr>
          <th>0.70</th>
          <td>0.559186</td>
          <td>1.334750</td>
        </tr>
        <tr>
          <th>0.75</th>
          <td>0.584849</td>
          <td>1.410393</td>
        </tr>
        <tr>
          <th>0.80</th>
          <td>0.595353</td>
          <td>1.551686</td>
        </tr>
        <tr>
          <th>0.85</th>
          <td>0.462451</td>
          <td>1.644665</td>
        </tr>
        <tr>
          <th>0.90</th>
          <td>0.248638</td>
          <td>1.946297</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 197-198

Finally, we can compare the predicted treatment effect with the true treatment effect on the CVaR.

.. GENERATED FROM PYTHON SOURCE LINES 200-207

.. code-block:: default

    CVAR = np.array(Y1_cvar) - np.array(Y0_cvar)
    data = {"Quantile": tau_vec, "CVaR": CVAR, "DML CVaR": dml_CVAR.coef,
            "DML CVaR pointwise lower": ci_CVAR['2.5 %'], "DML CVaR pointwise upper": ci_CVAR['97.5 %'],
            "DML CVaR joint lower": ci_joint_CVAR['2.5 %'], "DML CVaR joint upper": ci_joint_CVAR['97.5 %']}
    df = pd.DataFrame(data)
    print(df)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

          Quantile      CVaR  DML CVaR  DML CVaR pointwise lower  DML CVaR pointwise upper  DML CVaR joint lower  DML CVaR joint upper
    0.10      0.10  0.565491  0.627564                  0.424108                  0.831019              0.367571              0.887556
    0.15      0.15  0.590714  0.677980                  0.476856                  0.879103              0.420967              0.934992
    0.20      0.20  0.614591  0.706645                  0.509951                  0.903339              0.455293              0.957996
    0.25      0.25  0.637841  0.716793                  0.515358                  0.918227              0.459383              0.974202
    0.30      0.30  0.660860  0.716762                  0.506903                  0.926621              0.448587              0.984937
    0.35      0.35  0.684125  0.740869                  0.520930                  0.960808              0.459812              1.021926
    0.40      0.40  0.707905  0.756969                  0.532266                  0.981672              0.469825              1.044113
    0.45      0.45  0.732525  0.751710                  0.521002                  0.982417              0.456892              1.046527
    0.50      0.50  0.758368  0.779682                  0.539767                  1.019596              0.473099              1.086264
    0.55      0.55  0.785673  0.786744                  0.531223                  1.042265              0.460218              1.113270
    0.60      0.60  0.815037  0.814351                  0.543136                  1.085566              0.467770              1.160932
    0.65      0.65  0.847320  0.848868                  0.565066                  1.132671              0.486202              1.211534
    0.70      0.70  0.883560  0.946968                  0.643512                  1.250425              0.559186              1.334750
    0.75      0.75  0.925019  0.997621                  0.674609                  1.320633              0.584849              1.410393
    0.80      0.80  0.974240  1.073520                  0.699333                  1.447706              0.595353              1.551686
    0.85      0.85  1.035601  1.053558                  0.590991                  1.516125              0.462451              1.644665
    0.90      0.90  1.117984  1.097468                  0.433221                  1.761714              0.248638              1.946297




.. GENERATED FROM PYTHON SOURCE LINES 208-223

.. code-block:: default

    plt.rcParams['figure.figsize'] = 10., 7.5
    fig, ax = plt.subplots()
    ax.grid()

    ax.plot(df['Quantile'],df['DML CVaR'], color='violet', label='Estimated CVaR')
    ax.plot(df['Quantile'],df['CVaR'], color='green', label='True CVaR')
    ax.fill_between(df['Quantile'], df['DML CVaR pointwise lower'], df['DML CVaR pointwise upper'], color='violet', alpha=.3, label='Pointwise Confidence Interval')
    ax.fill_between(df['Quantile'], df['DML CVaR joint lower'], df['DML CVaR joint upper'], color='violet', alpha=.2, label='Joint Confidence Interval')

    plt.legend()
    plt.title('Conditional Value at Risk', fontsize=16)
    plt.xlabel('Quantile')
    _ = plt.ylabel('QTE and 95%-CI')





.. image-sg:: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cvar_002.png
   :alt: Conditional Value at Risk
   :srcset: /examples/auto_examples/python_examples/images/sphx_glr_plot_py_double_ml_cvar_002.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  43.889 seconds)


.. _sphx_glr_download_examples_auto_examples_python_examples_plot_py_double_ml_cvar.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_py_double_ml_cvar.py <plot_py_double_ml_cvar.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_py_double_ml_cvar.ipynb <plot_py_double_ml_cvar.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
