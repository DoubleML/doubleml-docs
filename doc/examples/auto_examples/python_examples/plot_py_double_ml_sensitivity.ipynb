{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sensitivity\n\nThis is an example of how to use the double machine learning approach for conditional value-at-risk estimation.\nThe code demonstrates the usage of the `DoubleMLCateEstimator` class and provides a step-by-step guide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python: Sensitivity Analysis\n\nThis notebook illustrates the sensitivity analysis tools with the partiallly linear regression model (PLR). <br>\nThe DoubleML package implements sensitivity analysis based on [Chernozhukov et al. (2022)](https://www.nber.org/papers/w30302).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation Example\n\nFor illustration purposes, we will work with generated data. This enables us to set the counfounding strength, such that we can correctly access quality of e.g. the robustness values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nimport doubleml as dml\nfrom doubleml.datasets import make_confounded_plr_data, fetch_401K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n\nThe data will be generated via `make_confounded_plr_data` and set the confounding values to `cf_y=0.1` and `cf_d=0.1`.\n\nBoth parameters determine the strength of the confounding\n\n- `cf_y` measures the proportion of residual variance in the outcome explained by unobserved confounders\n- `cf_d` measires the porportion of residual variance of the treatment representer generated by unobserved confounders.\n\nFor further details, see [Chernozhukov et al. (2022)](https://www.nber.org/papers/w30302) or [User guide](https://docs.doubleml.org/stable/guide/guide.html).\nFurther, the true treatment effect is set to `theta=5.0`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cf_y = 0.1\ncf_d = 0.1\ntheta = 5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\ndpg_dict = make_confounded_plr_data(n_obs=1000, cf_y=cf_y, cf_d=cf_d, theta=theta)\nx_cols = [f'X{i + 1}' for i in np.arange(dpg_dict['x'].shape[1])]\ndf = pd.DataFrame(np.column_stack((dpg_dict['x'], dpg_dict['y'], dpg_dict['d'])), columns=x_cols + ['y', 'd'])\ndml_data = dml.DoubleMLData(df, 'y', 'd')\nprint(dml_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DoubleML Object\n\nWe fit a `DoubleMLPLR` object and use basic random forest to flexibly estimate the nuisance elements.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\ndml_obj = dml.DoubleMLPLR(dml_data,\n                          ml_l=RandomForestRegressor(),\n                          ml_m=RandomForestRegressor(),\n                          n_folds=5,\n                          score='partialling out')\ndml_obj.fit()\nprint(dml_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The effect estimate is biased due to the unobserved confounding and the corresponding confidence interval does not cover the true parameter `theta=5.0`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.confint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Analysis\n\nTo perform a sensitivity analysis with the [DoubleML](https://docs.doubleml.org/stable/index.html) package you can use the `sensitivity_analysis()` method. <br>\nThe sensitivity analysis is based on the strength of the confounding `cf_y` and `cf_d` (default values $0.03$) and the parameter `rho`, which measures the correlation between the difference of the long and short form of the outcome regression and the Riesz representer (the default value $1.0$ is conservative and considers adversarial counfounding). To additionally incorporate statistical uncertainty, a significance level (default $0.95$) is used.\n\nThese input parameters are used to calculate upper and lower bounds (including the corresponding confidence level) on the treatment effect estimate. \n\nFurther, the sensitivity analysis includes a null hypothesis (default $0.0$), which is used to compute robustness values. The robustness value $RV$ is defined as the required confounding strength (`cf_y=rv` and `cf_d=rv`), such that the lower or upper bound of the causal parameter includes the null hypothesis. The robustness value $RVa$ defined analogous but additionally incorporates statistical uncertainty (as it is based on the confidence intervals of the bounds). For a more detailed explanation\nsee the [User Guide](https://docs.doubleml.org/stable/guide/guide.html) or [Chernozhukov et al. (2022)](https://www.nber.org/papers/w30302).\n\nThe results of the analysis can be printed via the `sensitivity_summary` property or directly accessed via the `sensitivity_params` property.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.sensitivity_analysis()\nprint(dml_obj.sensitivity_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.sensitivity_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The robustness value $RV$ means that unobserved counfounders would have to account for $34\\%$ of the residual variation (in treatment and outcome) to explain away the treatment effect.\n\nWe can also consider a contour plot to consider different values of confounding `cf_y` and `cf_d`. The contour plot and robustness values are based on the upper or lower bounds based on the null hypothesis (in this case this results in the lower bound).\n\nRemark that both, the robustness values and the contour plot use the prespecified value of `rho` and the ``null_hypothesis``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.sensitivity_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To consider a different null hypothesis and add a different scenario to the the contour plot, we can adjust the parameter `null_hypothesis`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.sensitivity_analysis(cf_y=cf_y, cf_d=cf_d, null_hypothesis=theta)\nprint(dml_obj.sensitivity_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_obj.sensitivity_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Application: 401(k)\n\nIn this real-data example, we illustrate how to use the sensitivity analysis from the [DoubleML](https://docs.doubleml.org/stable/index.html) package to evaluate effect estimates from 401(k) eligibility on accumulated assets. The 401(k) data set has been analyzed in several studies, among others [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060), see [Kallus et al. (2019)](https://arxiv.org/abs/1912.12945) for quantile effects.\n\n**Remark:**\nThis notebook focuses on sensitivity analysis. For a basic introduction to the [DoubleML](https://docs.doubleml.org/stable/index.html) package and a detailed example of the average treatment effect estimation for the 401(k) data set, we refer to the notebook [Python: Impact of 401(k) on Financial Wealth](https://docs.doubleml.org/stable/examples/py_double_ml_pension.html).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data and Effect Estimation\n\nDefine eligiblity as treatment and the net financial assets as outcome to construct a DoubleML object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = fetch_401K(return_type='DataFrame')\n\n# Set up basic model: Specify variables for data-backend\nfeatures_base = ['age', 'inc', 'educ', 'fsize', 'marr',\n                 'twoearn', 'db', 'pira', 'hown']\n\n# Initialize DoubleMLData (data-backend of DoubleML)\ndata_dml = dml.DoubleMLData(data,\n                            y_col='net_tfa',\n                            d_cols='e401',\n                            x_cols=features_base)\nprint(data_dml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use basic random forests to estimate the nuisance elements and fit a [DoubleMLPLR](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLPLR.html) model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner_l = RandomForestRegressor(n_estimators=500, max_depth=7,\n                                  max_features=3, min_samples_leaf=3)\nlearner_m = RandomForestClassifier(n_estimators=500, max_depth=5,\n                                   max_features=4, min_samples_leaf=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\ndml_plr_obj = dml.DoubleMLPLR(data_dml,\n                              ml_l = learner_l,\n                              ml_m = learner_m,\n                              n_folds = 5)\ndml_plr_obj.fit()\nprint(dml_plr_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Analysis\n\nIn their paper [Chernozhukov et al. (2022)](https://www.nber.org/papers/w30302) argue that confounding should not account for more than $4\\%$ of the residual variation of the outcome and $3\\%$ of the residual variation of the treatment. Consequently, we set `cf_y=0.04` and `cf_d=0.03`. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_plr_obj.sensitivity_analysis(cf_y=0.04, cf_d=0.03)\nprint(dml_plr_obj.sensitivity_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our robustness values are similar to the ones in the paper. The effect estimate is robust in the sense that unobserved confounders (e.g. latent firm characteristics) would have to account for at least $7,1\\%$ of the residual variation (in the outcome and treatment) to reduce the lower bound on the effect to zero. Including estimation uncertainty unobserved confounders still have to explain $5.3\\%$ of the residual variation to render the effect estimate insignificant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_plr_obj.sensitivity_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity Analysis with IRM\n\nThe sensitivity analysis with the IRM model is basically analogous.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\ndml_irm_obj = dml.DoubleMLIRM(data_dml,\n                              ml_g = learner_l,\n                              ml_m = learner_m,\n                              n_folds = 5)\ndml_irm_obj.fit()\n\nprint(dml_irm_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_irm_obj.sensitivity_analysis(cf_y=0.04, cf_d=0.03)\nprint(dml_irm_obj.sensitivity_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dml_irm_obj.sensitivity_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = dml_irm_obj.sensitivity_plot()\n\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500)\n\nfig"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}