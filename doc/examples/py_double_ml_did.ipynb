{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Difference-in-Differences\n",
    "\n",
    "In this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the average treatment effect on the treated (ATT) under the conditional parallel trend assumption. The estimation is based on [Chang (2020)](https://doi.org/10.1093/ectj/utaa001), [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) and [Zimmert et al. (2018)](https://arxiv.org/abs/1809.01643).\n",
    "\n",
    "In this example, we will adopt the notation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003).\n",
    "\n",
    "In the whole example our treatment and time variable $t\\in\\{0,1\\}$ will be binary. \n",
    "Let $D_i\\in\\{0,1\\}$ denote the treatment status of unit $i$ at time $t=1$ (at time $t=0$ all units are not treated) and let $Y_{it}$ be the outcome of interest of unit $i$ at time $t$.\n",
    "Using the potential outcome notation, we can write $Y_{it}(d)$ for the potential outcome of unit $i$ at time $t$ and treatment status $d$. Further, let $X_i$ denote a vector of pre-treatment covariates.\n",
    "In these difference-in-differences settings [Abadie (2005)](https://doi.org/10.1111/0034-6527.00321) showed that the ATTE\n",
    "\n",
    "$$\\theta = \\mathbb{E}[Y_{i1}(1)- Y_{i1}(0)|D_i=1]$$\n",
    "\n",
    "is identified when panel data are available or under stationarity assumptions for repeated cross-sections. Further, the basic assumptions are \n",
    "\n",
    " - **Parallel Trends:** We have $\\mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=1] = \\mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=0]\\quad a.s.$\n",
    "\n",
    "- **Overlap:** For some $\\epsilon > 0$, $P(D_i=1) > \\epsilon$ and $P(D_i=1|X_i) \\le 1-\\epsilon$ a.s.\n",
    "\n",
    "For a detailed explanation of the assumptions see e.g. [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) or [Zimmert et al. (2018)](https://arxiv.org/abs/1809.01643).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panel Data (Repeated Outcomes)\n",
    "\n",
    "At first, we will consider two-period panel data, where we observe i.i.d. data $W_i = (Y_{i0}, Y_{i1}, D_i, X_i)$.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will use the implemented data generating process `make_did_SZ2020` to generate data according to the simulation in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (Section 4.1). \n",
    "\n",
    "In this example, we will use `dgp_tpye=4`, which corresponds to the misspecified settings in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (other data generating processes are also available via the `dgp_type` parameter). In all settings the true ATTE is zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a corresponding `DoubleMLData` object, we have to specify a single outcome `y`. For panel data, the outcome consists of the difference of \n",
    "\n",
    "$$\\Delta Y_i = Y_{i1}- Y_{i0}.$$\n",
    "\n",
    "This difference will then be defined as outcome in our `DoubleMLData` object. The data generating process `make_did_SZ2020` already specifies the outcome `y` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from doubleml import DoubleMLData\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "x, y, d = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=False, return_type='array')\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "print(dml_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTE Estimation\n",
    "\n",
    "To estimate the ATTE with panel data, we will use the `DoubleMLDID` class. \n",
    "\n",
    "As for all `DoubleML` classes, we have to specify learners, which have to be initialized first.\n",
    "Here, we will just rely on a tree based method. \n",
    "\n",
    "The learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[\\Delta Y_i|D_i=0, X_i]$, whereas the learner `ml_m` will be used to estimate the propensity score $P(D_i=1|X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "n_estimators = 30\n",
    "ml_g = LGBMRegressor(n_estimators=n_estimators)\n",
    "ml_m = LGBMClassifier(n_estimators=n_estimators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DoubleMLDID` class can be used as any other `DoubleML` class. \n",
    "\n",
    "The score is set to `score='observational'`, since the we generated data where the treatment probability depends on the pretreatment covariates. Further, we will use `in_sample_normalization=True`, since normalization generally improved the results in our simulations (both `score='observational'` and `in_sample_normalization=True` are default values).\n",
    "\n",
    "After initialization, we have to call the `fit()` method to estimate the nuisance elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLDID\n",
    "dml_did = DoubleMLDID(dml_data, \n",
    "                      ml_g=ml_g, \n",
    "                      ml_m=ml_m,\n",
    "                      score='observational',\n",
    "                      in_sample_normalization=True,\n",
    "                      n_folds=5)\n",
    "\n",
    "dml_did.fit()\n",
    "print(dml_did)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, confidence intervals at different levels can be obtained via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_did.confint(level=0.90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Simulation\n",
    "\n",
    "Here, we add a small coverage simulation to highlight the difference to the linear implementation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003). We generate multiple datasets, estimate the ATTE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 200\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep/10)) == 0:\n",
    "        print(f'Iteration: {i_rep}/{n_rep}')\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=False)\n",
    "\n",
    "    dml_did = DoubleMLDID(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage [i_rep] = (confint['2.5 %'].iloc[0] <= ATTE) & (ATTE <= confint['97.5 %'].iloc[0])\n",
    "    ci_length[i_rep] = confint['97.5 %'].iloc[0] - confint['2.5 %'].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the corresponding coverage and the length of the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coverage: {coverage.mean()}')\n",
    "print(f'Average CI length: {ci_length.mean()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can observe that the coverage is still valid, since we did not rely on linear learners, so the setting is not misspecified in this example. \n",
    "\n",
    "If we know the conditional expectation is correctly specified (linear form), we can use this to obtain smaller confidence intervals but in many applications, we may want to safeguard against misspecification and use flexible models such as random forest or boosting.\n",
    "\n",
    "The distribution of the estimates takes the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=['Estimate'])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Cross-Sectional Data\n",
    "\n",
    "For repeated cross-sectional data, we assume that we observe i.i.d. data $W_i = (Y_{i}, D_i, X_i, T_i)$. \n",
    "\n",
    "Here $Y_i = T_i Y_{i1} + (1-T_i)Y_{i0}$ corresponds to the outcome of unit $i$ which is observed at time $T_i$.\n",
    "\n",
    "### Data\n",
    "\n",
    "As for panel data, we will use the implemented data generating process `make_did_SZ2020` to generate data according to the simulation in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (Section 4.2). \n",
    "\n",
    "In this example, we will use `dgp_tpye=4`, which corresponds to the misspecified settings in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (other data generating processes are also available via the `dgp_type` parameter). In all settings the true ATTE is zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to other `DoubleMLData` objects, we have to specify which column corresponds to our time variable $T$.\n",
    "\n",
    "The time variable can be simply set via the argument `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from doubleml import DoubleMLData\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "x, y, d, t = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=True, return_type='array')\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d, t=t)\n",
    "print(dml_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTE Estimation\n",
    "\n",
    "To estimate the ATTE with panel data, we will use the `DoubleMLDIDCS` class. \n",
    "\n",
    "As for all `DoubleML` classes, we have to specify learners, which have to be initialized first.\n",
    "Here, we will just rely on a tree based method. \n",
    "\n",
    "The learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[\\Delta Y_i| D_i=d, T_i =t, X_i]$ for all combinations of $d,t\\in\\{0,1\\}$, whereas the learner `ml_m` will be used to estimate the propensity score $P(D_i=1|X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "n_estimators = 30\n",
    "ml_g = LGBMRegressor(n_estimators=n_estimators)\n",
    "ml_m = LGBMClassifier(n_estimators=n_estimators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DoubleMLDIDCS` class can be used as any other `DoubleML` class. \n",
    "\n",
    "The score is set to `score='observational'`, since the we generated data where the treatment probability depends on the pretreatment covariates. Further, we will use `in_sample_normalization=True`, since normalization generally improved the results in our simulations (both `score='observational'` and `in_sample_normalization=True` are default values).\n",
    "\n",
    "After initialization, we have to call the `fit()` method to estimate the nuisance elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLDIDCS\n",
    "dml_did = DoubleMLDIDCS(dml_data,\n",
    "                        ml_g=ml_g,\n",
    "                        ml_m=ml_m,\n",
    "                        score='observational',\n",
    "                        in_sample_normalization=True,\n",
    "                        n_folds=5)\n",
    "\n",
    "dml_did.fit()\n",
    "print(dml_did)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, confidence intervals at different levels can be obtained via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_did.confint(level=0.90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Simulation\n",
    "\n",
    "Again, we add a small coverage simulation to highlight the difference to the linear implementation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003). We generate multiple datasets, estimate the ATTE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 200\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep/10)) == 0:\n",
    "        print(f'Iteration: {i_rep}/{n_rep}')\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=True)\n",
    "\n",
    "    dml_did = DoubleMLDIDCS(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage [i_rep] = (confint['2.5 %'].iloc[0] <= ATTE) & (ATTE <= confint['97.5 %'].iloc[0])\n",
    "    ci_length[i_rep] = confint['97.5 %'].iloc[0] - confint['2.5 %'].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the corresponding coverage and the length of the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coverage: {coverage.mean()}')\n",
    "print(f'Average CI length: {ci_length.mean()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for panel data the coverage is still valid, since we did not rely on linear learners, so the setting is not misspecified in this example. \n",
    "\n",
    "If we know the conditional expectation is correctly specified (linear form), we can use this to obtain smaller confidence intervals but in many applications, we may want to safeguard against misspecification and use flexible models such as random forest or boosting.\n",
    "\n",
    "The distribution of the estimates takes the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=['Estimate'])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doubleml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c17b3f7bd1a966b23df6bc421cb393eedb89185fca3be80230a2e6676a5e11a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
