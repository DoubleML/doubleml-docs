{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Choice of learners\n",
    "\n",
    "This notebooks contains some practical recommendations to choose the right learner and evaluate different learners for the corresponding nuisance components.\n",
    "\n",
    "For the example, we will work with a IRM, but all of the important components are directly usable for all other models too.\n",
    "\n",
    "To be able to compare the properties of different learners, we will start by setting the true treatment parameter to zero, fix some other parameters of the data generating process and generate several datasets \n",
    "to obtain some information about the distribution of the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import doubleml as dml\n",
    "\n",
    "from doubleml.datasets import make_irm_data\n",
    "\n",
    "theta = 0\n",
    "n_obs = 500\n",
    "dim_x = 5\n",
    "n_rep = 200\n",
    "\n",
    "np.random.seed(42)\n",
    "datasets = []\n",
    "for i in range(n_rep):\n",
    "    data = make_irm_data(theta=theta, n_obs=n_obs, dim_x=dim_x, \n",
    "                         R2_d=0.8, R2_y=0.8, return_type='DataFrame')\n",
    "    datasets.append(dml.DoubleMLData(data, 'y', 'd'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different learners\n",
    "For simplicity, we will restrict ourselves to the comparison of two different types and evaluate a learner of linear type and a tree based estimator for each nuisance component (with default hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "reg_learner_1 = LinearRegression()\n",
    "reg_learner_2 = GradientBoostingRegressor()\n",
    "class_learner_1 = LogisticRegressionCV()\n",
    "class_learner_2 = GradientBoostingClassifier()\n",
    "\n",
    "learner_list = [{'ml_g': reg_learner_1, 'ml_m': class_learner_1},\n",
    "                {'ml_g': reg_learner_2, 'ml_m': class_learner_1},\n",
    "                {'ml_g': reg_learner_1, 'ml_m': class_learner_2},\n",
    "                {'ml_g': reg_learner_2, 'ml_m': class_learner_2}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all combinations, we now can try to evaluate four different IRM models. To make the comparison fair, we will apply all different models to the same cross-fitting samples (usually this should not matter, we only consider this here to get slightly cleaner comparison)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard approach\n",
    "\n",
    "At first, we will look at the most straightforward approach using the inbuild RMSE. The `rmses` attribute contains the out-of-sample RMSE for the nuisance functions. We will save all RMSEs and the corresponding treatment estimates for all combinations of learners over all repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml._utils_resampling import DoubleMLResampling\n",
    "\n",
    "coefs = np.full(shape=(n_rep, len(learner_list)), fill_value=np.nan)\n",
    "rmses_ml_m = np.full(shape=(n_rep, len(learner_list)), fill_value=np.nan)\n",
    "rmses_ml_g0 = np.full(shape=(n_rep, len(learner_list)), fill_value=np.nan)\n",
    "rmses_ml_g1 = np.full(shape=(n_rep, len(learner_list)), fill_value=np.nan)\n",
    "\n",
    "coverage = np.full(shape=(n_rep, len(learner_list)), fill_value=np.nan)\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    print(f\"\\rProcessing: {round((i_rep+1)/n_rep*100, 3)} %\", end=\"\")\n",
    "    dml_data = datasets[i_rep]\n",
    "    # define the sample splitting\n",
    "    smpls = DoubleMLResampling(n_folds=5, n_rep=1, n_obs=n_obs,\n",
    "                               apply_cross_fitting=True, stratify=dml_data.d).split_samples()\n",
    "    \n",
    "    for i_learners, learners in enumerate(learner_list):\n",
    "        np.random.seed(42)\n",
    "        dml_irm = dml.DoubleMLIRM(dml_data,\n",
    "                                  ml_g=clone(learners['ml_g']),\n",
    "                                  ml_m=clone(learners['ml_m']),\n",
    "                                  draw_sample_splitting=False)\n",
    "        dml_irm.set_sample_splitting(smpls)\n",
    "        dml_irm.fit(n_jobs_cv=5)\n",
    "\n",
    "        coefs[i_rep, i_learners] = dml_irm.coef[0]\n",
    "        rmses_ml_m[i_rep, i_learners] = dml_irm.rmses['ml_m']\n",
    "        rmses_ml_g0[i_rep, i_learners] = dml_irm.rmses['ml_g0']\n",
    "        rmses_ml_g1[i_rep, i_learners] = dml_irm.rmses['ml_g1']\n",
    "\n",
    "        confint = dml_irm.confint()\n",
    "        coverage[i_rep, i_learners] = (confint['2.5 %'] <= theta) & (confint['97.5 %'] >= theta)\n",
    "\n",
    "print(f'\\nCoverage: {coverage.mean(0)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us take a look at the corresponding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colnames = ['Linear + Logit','Boost + Logit', 'Linear + Boost', 'Boost + Boost']\n",
    "\n",
    "df_coefs = pd.DataFrame(coefs, columns=colnames)\n",
    "df_ml_m = pd.DataFrame(rmses_ml_m, columns=colnames)\n",
    "df_ml_g0 = pd.DataFrame(rmses_ml_g0, columns=colnames)\n",
    "df_ml_g1 = pd.DataFrame(rmses_ml_g1, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 5))\n",
    "fig.suptitle('Learner Comparison')\n",
    "\n",
    "\n",
    "sns.kdeplot(data=df_coefs,ax=axes[0][0], fill=True, alpha=0.3)\n",
    "sns.kdeplot(data=df_ml_m, ax=axes[0][1], fill=True, alpha=0.3, legend=False)\n",
    "sns.kdeplot(data=df_ml_g0, ax=axes[1][0], fill=True, alpha=0.3, legend=False)\n",
    "sns.kdeplot(data=df_ml_g1, ax=axes[1][1], fill=True, alpha=0.3)\n",
    "\n",
    "axes[0][0].title.set_text('Estimated Parameter')\n",
    "axes[0][1].title.set_text('RMSE ml_m')\n",
    "axes[1][0].title.set_text('RMSE ml_g0')\n",
    "axes[1][1].title.set_text('RMSE ml_g1')\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily observe that in this setting, the linear learners are able to approximate the corresponding nuisance functions better than the boosting algorithm (as should be expected since the data is generated accordingly).\n",
    "\n",
    "Let us take a look at what would have happend if a each repetition for each nuisance element, we would have selected the learner with smallest out-of-sample rmse (in our example this corresponds to minimizing the product of rmses). \n",
    "Remark that we cannot select different learners for `ml_g0` and `ml_g1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_learners = (rmses_ml_m * (rmses_ml_g0 + rmses_ml_g1)).argmin(axis=1)\n",
    "np.unique(selected_learners, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we will use linear learners for both nuisance elements. Sometimes the tree-based estimator is chosen for the propensity score `ml_m`. \n",
    "Let us compare which learners, how the estimated coefficients would have performed with the selected learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Coverage of selected learners: {np.mean(np.array([coverage[i_rep, selected_learners[i_rep]] for i_rep in range(n_rep)]))}')\n",
    "\n",
    "selected_coefs = np.array([coefs[i_rep, selected_learners[i_rep]] for i_rep in range(n_rep)])\n",
    "df_coefs['Selected'] = selected_coefs\n",
    "sns.kdeplot(data=df_coefs, fill=True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure will be generally valid as long as we do not compare a excessively large number of different learners."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom evaluation metrics\n",
    "\n",
    "If one wants to evaluate a learner based on some other metric/loss it is possible to use the inbuilt `evaluate_learners()` method.\n",
    "Without further arguments this will default to the RMSE for all nuisance components and result in the same output as the `rmses` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_irm.evaluate_learners())\n",
    "print(dml_irm.rmses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a self-defined metric, the user has to hand over a callable. In this example, we define the mean absolute deviation as an error metric.\n",
    "\n",
    "Remark that the metric should be able to handle `nan` values, since e.g. in the IRM model the learner `ml_g` is used to onto two different subsamples. As a result, we have two different nuisance components for\n",
    "\n",
    "\\begin{align*}\n",
    "g_0(x) &= \\mathbb{E}[Y|X=x, D=0] \\\\\n",
    "g_1(x) &= \\mathbb{E}[Y|X=x, D=1]\n",
    "\\end{align*}\n",
    "\n",
    "which are both fitted with the learner `ml_g`. Of course, we can only observe the target value for $g_0(x)$ if $D=0$ and vice versa, resulting in `nan` values for all other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    subset = np.logical_not(np.isnan(y_true))\n",
    "    return mean_absolute_error(y_true[subset], y_pred[subset])\n",
    "\n",
    "dml_irm.evaluate_learners(metric=mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to access the out-of-sample predictions and target values for the nuisance elements via the `nuisance_targets` and `predictions` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_irm.nuisance_targets['ml_g1'].shape)\n",
    "print(dml_irm.predictions['ml_g1'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most models minimizing the RMSE for each learner should result in improved performance as the theoretical backbone of the DML Framework is build on $\\ell_2$-convergence rates for the nuisance estimates ([Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097)). But for some models (e.g. classification learners) it might be helpful to further check other error metrics (e.g. as in [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#)) to gain a overview whether the nuisance function can be approximated sufficiently well. \n",
    "\n",
    "Of course, if one has some prior knowledge on functional form assumptions (e.g. linearity as in the IRM example above) using these learners will usually improve the performance of the estimator and might speed up computation time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time\n",
    "\n",
    "The choice of the learner has a huge impact on the computation time of the DoubleML models. As the largest part of the computation time is usually used to train the learners for the nuisance components, some clever choices of learners and hyperparameters can speed up the computation time. \n",
    "\n",
    "Resourcewise, most implementations support the `n_jobs_cv` argument, which can parallelize the k-fold estimation and might speed up the calculation nearly up to $k$-times if the resources are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from time import perf_counter\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "dml_data = dml.DoubleMLData(make_irm_data(theta=0, n_obs=n_obs, dim_x=20, return_type='DataFrame'), 'y', 'd')\n",
    "\n",
    "# define the sample splitting\n",
    "smpls = DoubleMLResampling(n_folds=5, n_rep=1, n_obs=n_obs, apply_cross_fitting=True, stratify=dml_data.d).split_samples()\n",
    "\n",
    "dml_irm = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=RandomForestRegressor(),\n",
    "                          ml_m=RandomForestClassifier(),\n",
    "                          draw_sample_splitting=False)\n",
    "dml_irm.set_sample_splitting(smpls)\n",
    "\n",
    "np.random.seed(42)\n",
    "t_1_start = perf_counter()\n",
    "dml_irm.fit()\n",
    "t_1_stop = perf_counter()\n",
    "print(f'Time without parallelization of crossfitting: {round(t_1_stop - t_1_start, 4)} seconds')\n",
    "\n",
    "np.random.seed(42)\n",
    "t_2_start = perf_counter()\n",
    "dml_irm.fit(n_jobs_cv=5)\n",
    "t_2_stop = perf_counter()\n",
    "print(f'Time with parallelization of crossfitting: {round(t_2_stop - t_2_start, 4)} seconds')\n",
    "print(f'Speedup of factor {round((t_1_stop - t_1_start) / (t_2_stop - t_2_start), 2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other more helpful ways to improve computation time will largly depend on the implemented learner. Of course linear learners are quite fast, but if no functional form restrictions are known Boosting or Random Forest might be better default options to saveguard against wrong model assumptions. Especially Boosting performs very well as a default option for tabular data. As a general recommendation all popular Boosting frameworks (XGBoost, Lightgbm, Catboost, etc.) should improve computation time.\n",
    "But this might vary heavily with the number of features in your dataset.\n",
    "Let us compare the computation time with Boosting and Random Forest (we increase the sample size and the number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "dml_data = dml.DoubleMLData(make_irm_data(theta=0, n_obs=n_obs, dim_x=50, return_type='DataFrame'), 'y', 'd')\n",
    "\n",
    "# define the sample splitting\n",
    "smpls = DoubleMLResampling(n_folds=5, n_rep=1, n_obs=n_obs, apply_cross_fitting=True, stratify=dml_data.d).split_samples()\n",
    "\n",
    "np.random.seed(42)\n",
    "t_1_start = perf_counter()\n",
    "dml_irm = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=RandomForestRegressor(),\n",
    "                          ml_m=RandomForestClassifier(),\n",
    "                          draw_sample_splitting=False)\n",
    "dml_irm.set_sample_splitting(smpls)\n",
    "dml_irm.fit()\n",
    "t_1_stop = perf_counter()\n",
    "print(f'Time without RandomForest (Scikit-Learn): {round(t_1_stop - t_1_start, 4)} seconds')\n",
    "\n",
    "np.random.seed(42)\n",
    "t_2_start = perf_counter()\n",
    "dml_irm = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=XGBRegressor(),\n",
    "                          ml_m=XGBClassifier(),\n",
    "                          draw_sample_splitting=False)\n",
    "dml_irm.set_sample_splitting(smpls)\n",
    "dml_irm.fit()\n",
    "t_2_stop = perf_counter()\n",
    "print(f'Time with XGBoost: {round(t_2_stop - t_2_start, 4)} seconds')\n",
    "print(f'Speedup of factor {round((t_1_stop - t_1_start) / (t_2_stop - t_2_start), 2)}')\n",
    "\n",
    "np.random.seed(42)\n",
    "t_3_start = perf_counter()\n",
    "dml_irm = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=LGBMRegressor(),\n",
    "                          ml_m=LGBMClassifier(),\n",
    "                          draw_sample_splitting=False)\n",
    "dml_irm.set_sample_splitting(smpls)\n",
    "dml_irm.fit()\n",
    "t_3_stop = perf_counter()\n",
    "print(f'Time with LightGBM: {round(t_3_stop - t_3_start, 4)} seconds')\n",
    "print(f'Speedup of factor {round((t_1_stop - t_1_start) / (t_3_stop - t_3_start), 2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doubleml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
