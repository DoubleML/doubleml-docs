{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNcwBqn11_ZX"
   },
   "source": [
    "# Python: Cluster Robust Double Machine Learning\n",
    "\n",
    "\n",
    "## Motivation \n",
    "\n",
    "In many empirical applications, errors exhibit a clustered structure such that the usual i.i.d. assumption does not hold anymore. In order to perform valid statistical inference, researchers have to account for clustering. In this notebook, we will shortly emphasize the consequences of clustered data on inference based on the double machine learning (DML) approach as has been considered in [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815). We will demonstrate how users of the [DoubleML](https://docs.doubleml.org/stable/index.html) package can account for one- and two-way clustering in their analysis.\n",
    "\n",
    "Clustered errors in terms of one or multiple dimensions might arise in many empirical applications. For example, in a cross-sectional study, errors might be correlated (i) within regions (one-way clustering) or (ii) within regions and industries at the same time (two-way clustering). Another example for two-way clustering, discussed in [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815), refers to market share data with market shares being subject to shocks on the market and product level at the same time. We refer to [Cameron et al. (2011)](https://doi.org/10.1198/jbes.2010.07136) for an introduction to multiway clustering and a illustrative list of empirical examples.\n",
    "\n",
    "## Clustering and double machine learning\n",
    "\n",
    "Clustering creates a challenge to the double machine learning (DML) approach in terms of \n",
    "\n",
    "1. a necessary adjustment of the formulae used for estimation of the variance covariance matrix, standard errors, p-values etc., and,\n",
    "2. an adjusted resampling scheme for the cross-fitting algorithm.\n",
    "\n",
    "The first point equally applies to classical statistical models, for example a linear regression model (see, for example [Cameron et al. 2011](https://doi.org/10.1198/jbes.2010.07136)). The second point arises because the clustering implies a correlation of errors from train and test samples if the standard cross-fitting procedure suggested in [Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097) was employed. The DML approach builds on independent sample splits into partitions that are used for training of the machine learning (ML) model learners and generation of predictions that are eventually used for solving the score function. For a motivation of the necessity of sample splitting, we refer to the illustration example in the [user guide]( \n",
    "https://docs.doubleml.org/stable/guide/basics.html#sample-splitting-to-remove-bias-induced-by-overfitting) as well as to the explanation in [Chernozhukov et al. (2018)](https://doi.org/10.1111/ectj.12097) .\n",
    "\n",
    "In order to achieve independent data splits in a setting with one-way or multi-way clustering, [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815) develop an updated  $K$-fold sample splitting procedure that ensures independent sample splits: The data set is split into disjoint partitions in terms of all clustering dimensions. For example, in a situation with two-way clustering, the data is split into $K^2$ folds. The machine learning models are then trained on a specific fold and used for generation of predictions in hold-out samples. Thereby, the sample splitting procedure ensures that the hold-out samples do not contain observations of the same clusters as used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1nf2-YD1_Z2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from doubleml import DoubleMLClusterData, DoubleMLData, DoubleMLPLIV\n",
    "\n",
    "from doubleml.datasets import make_pliv_multiway_cluster_CKMS2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lptyTy61_Z_"
   },
   "source": [
    "## A Motivating Example: Two-Way Cluster Robust DML\n",
    "\n",
    "In a first part, we show how the two-way cluster robust double machine learning (DML) ([Chiang et al. 2021](https://doi.org/10.1080/07350015.2021.1895815)) can be implemented with the [DoubleML](https://docs.doubleml.org/stable/index.html) package.\n",
    "[Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815) consider double-indexed data\n",
    "\n",
    "\\begin{equation}\n",
    "\\lbrace W_{ij}: i \\in \\lbrace 1, \\ldots, N \\rbrace, j \\in \\lbrace 1, \\ldots, M \\rbrace \\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "and the partially linear IV regression model (PLIV)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Y_{ij} = D_{ij} \\theta_0 +  g_0(X_{ij}) + \\epsilon_{ij}, & &\\mathbb{E}(\\epsilon_{ij} | X_{ij}, Z_{ij}) = 0, \\\\\n",
    "Z_{ij} = m_0(X_{ij}) + v_{ij}, & &\\mathbb{E}(v_{ij} | X_{ij}) = 0.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9L-nrVMH1_aD"
   },
   "source": [
    "### Simulate two-way cluster data\n",
    "\n",
    "We use the PLIV data generating process described in Section 4.1 of [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815).\n",
    "The DGP is defined as\n",
    "$$\\begin{aligned}\n",
    "Z_{ij} &= X_{ij}' \\xi_0 + V_{ij}, \\\\\n",
    "D_{ij} &= Z_{ij}' \\pi_{10} + X_{ij}' \\pi_{20} + v_{ij}, \\\\\n",
    "Y_{ij} &= D_{ij} \\theta + X_{ij}' \\zeta_0 + \\varepsilon_{ij},\n",
    "\\end{aligned}$$\n",
    "with\n",
    "$$\\begin{aligned}\n",
    "X_{ij} &= (1 - \\omega_1^X - \\omega_2^X) \\alpha_{ij}^X\n",
    "+ \\omega_1^X \\alpha_{i}^X + \\omega_2^X \\alpha_{j}^X, \\\\\n",
    "\\varepsilon_{ij} &= (1 - \\omega_1^\\varepsilon - \\omega_2^\\varepsilon) \\alpha_{ij}^\\varepsilon\n",
    "+ \\omega_1^\\varepsilon \\alpha_{i}^\\varepsilon + \\omega_2^\\varepsilon \\alpha_{j}^\\varepsilon, \\\\\n",
    "v_{ij} &= (1 - \\omega_1^v - \\omega_2^v) \\alpha_{ij}^v\n",
    "+ \\omega_1^v \\alpha_{i}^v + \\omega_2^v \\alpha_{j}^v, \\\\\n",
    "V_{ij} &= (1 - \\omega_1^V - \\omega_2^V) \\alpha_{ij}^V\n",
    "+ \\omega_1^V \\alpha_{i}^V + \\omega_2^V \\alpha_{j}^V,\n",
    "\\end{aligned}$$\n",
    "and $\\alpha_{ij}^X, \\alpha_{i}^X, \\alpha_{j}^X \\sim \\mathcal{N}(0, \\Sigma)$\n",
    "where  $\\Sigma$ is a $p_x \\times p_x$ matrix with entries\n",
    "$\\Sigma_{kj} = s_X^{|j-k|}$.\n",
    "Further\n",
    "$$\\begin{aligned}\n",
    "\\left(\\begin{matrix} \\alpha_{ij}^\\varepsilon \\\\ \\alpha_{ij}^v \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} \\alpha_{i}^\\varepsilon \\\\ \\alpha_{i}^v \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} \\alpha_{j}^\\varepsilon \\\\ \\alpha_{j}^v \\end{matrix}\\right)\n",
    "\\sim \\mathcal{N}\\left(0, \\left(\\begin{matrix} 1 & s_{\\varepsilon v} \\\\\n",
    "s_{\\varepsilon v} & 1 \\end{matrix} \\right) \\right)\n",
    "\\end{aligned}$$\n",
    "and $\\alpha_{ij}^V, \\alpha_{i}^V, \\alpha_{j}^V \\sim \\mathcal{N}(0, 1)$.\n",
    "\n",
    "Data from this DGP can be generated with the [make_pliv_multiway_cluster_CKMS2021()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.html#doubleml.datasets.make_pliv_multiway_cluster_CKMS2021) function from [DoubleML](https://docs.doubleml.org/stable/index.html).\n",
    "Analogously to [Chiang et al. (2021, Section 5)](https://doi.org/10.1080/07350015.2021.1895815)\n",
    "we use the following parameter setting:\n",
    "$\\theta=1.0$, $N=M=25$, $p_x=100$, $\\pi_{10}=1.0$, $\\omega_X = \\omega_{\\varepsilon} = \\omega_V = \\omega_v = (0.25, 0.25)$, $s_X = s_{\\varepsilon v} = 0.25$ and the $j$-th entries of the $p_x$-vectors $\\zeta_0 = \\pi_{20} = \\xi_0$ are $(\\zeta_{0})_j = 0.5^j$.\n",
    "This are also the default values of [make_pliv_multiway_cluster_CKMS2021()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.html#doubleml.datasets.make_pliv_multiway_cluster_CKMS2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hepXKz91_aM"
   },
   "outputs": [],
   "source": [
    "# Set the simulation parameters\n",
    "N = 25  # number of observations (first dimension)\n",
    "M = 25  # number of observations (second dimension)\n",
    "dim_X = 100  # dimension of X\n",
    "np.random.seed(3141) # set seed\n",
    "\n",
    "obj_dml_data = make_pliv_multiway_cluster_CKMS2021(N, M, dim_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlePOlu21_aP"
   },
   "source": [
    "### Data-Backend for Cluster Data\n",
    "The implementation of cluster robust double machine learning is based on a special data-backend called [DoubleMLClusterData](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLClusterData.html#doubleml.DoubleMLClusterData). As compared to the standard data-backend [DoubleMLData](https://docs.doubleml.org/dev/api/generated/doubleml.DoubleMLData.html), users can specify the clustering variables during instantiation of a  [DoubleMLClusterData](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLClusterData.html#doubleml.DoubleMLClusterData) object. The estimation framework will subsequently account for the provided clustering options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9DBwEFT1_aS"
   },
   "outputs": [],
   "source": [
    "# The simulated data is of type DoubleMLClusterData\n",
    "print(obj_dml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMU5Y0n81_aW"
   },
   "outputs": [],
   "source": [
    "# The cluster variables are part of the DataFrame\n",
    "obj_dml_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPXEpA3J1_aa"
   },
   "source": [
    "### Initialize the objects of class `DoubleMLPLIV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvjBpafF1_ae"
   },
   "outputs": [],
   "source": [
    "# Set machine learning methods for l, m & r\n",
    "learner = LassoCV()\n",
    "ml_l = clone(learner)\n",
    "ml_m = clone(learner)\n",
    "ml_r = clone(learner)\n",
    "\n",
    "# initialize the DoubleMLPLIV object\n",
    "dml_pliv_obj = DoubleMLPLIV(obj_dml_data,\n",
    "                            ml_l, ml_m, ml_r,\n",
    "                            n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rh3-MK2Q1_ah"
   },
   "outputs": [],
   "source": [
    "print(dml_pliv_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "#discrete color scheme\n",
    "x = sns.color_palette(\"RdBu_r\", 7)\n",
    "cMap = ListedColormap([x[0], x[3], x[6]])\n",
    "plt.rcParams['figure.figsize'] = 15, 12\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "def plt_smpls(smpls, n_folds_per_cluster):\n",
    "    df = pd.DataFrame(np.zeros([N*M, n_folds_per_cluster*n_folds_per_cluster]))\n",
    "    for i_split, this_split_ind in enumerate(smpls):\n",
    "        df.loc[this_split_ind[0], i_split] = -1.\n",
    "        df.loc[this_split_ind[1], i_split] = 1.\n",
    "\n",
    "    ax = sns.heatmap(df, cmap=cMap);\n",
    "    ax.invert_yaxis();\n",
    "    ax.set_ylim([0, N*M]);\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('Observation')\n",
    "    colorbar = ax.collections[0].colorbar\n",
    "    colorbar.set_ticks([-0.667, 0, 0.667])\n",
    "    colorbar.set_ticklabels(['Nuisance', '', 'Score'])\n",
    "\n",
    "def plt_smpls_cluster(smpls_cluster, n_folds_per_cluster):\n",
    "    for i_split in range(len(smpls_cluster)):\n",
    "        plt.subplot(n_folds_per_cluster, n_folds_per_cluster, i_split + 1)\n",
    "        df = pd.DataFrame(np.zeros([N*M, 1]),\n",
    "                      index = pd.MultiIndex.from_product([range(N), range(M)]),\n",
    "                      columns=['value'])\n",
    "\n",
    "        df.loc[pd.MultiIndex.from_product(smpls_cluster[i_split][0]), :] = -1.\n",
    "        df.loc[pd.MultiIndex.from_product(smpls_cluster[i_split][1]), :] = 1.\n",
    "\n",
    "        df_wide = df.reset_index().pivot(index=\"level_0\", columns=\"level_1\", values=\"value\")\n",
    "        df_wide.index.name=''\n",
    "        df_wide.columns.name=''\n",
    "\n",
    "        ax = sns.heatmap(df_wide, cmap=cMap);\n",
    "        ax.invert_yaxis();\n",
    "        ax.set_ylim([0, M]);\n",
    "        colorbar = ax.collections[0].colorbar\n",
    "        colorbar.set_ticks([-0.667, 0, 0.667])\n",
    "\n",
    "        l = i_split % n_folds_per_cluster + 1\n",
    "        k = np.floor_divide(i_split, n_folds_per_cluster) + 1\n",
    "        title = f'Nuisance: $I_{{{k}}}^C \\\\times J_{{{l}}}^C$; Score: $I_{{{k}}} \\\\times J_{{{l}}}$'\n",
    "        ax.set_title(title)\n",
    "        if l == n_folds_per_cluster:\n",
    "            colorbar.set_ticklabels(['Nuisance', '', 'Score'])\n",
    "        else:\n",
    "            colorbar.set_ticklabels(['', '', ''])\n",
    "        if l == 1:\n",
    "            ax.set_ylabel('First Cluster Variable $k$')\n",
    "        if k == 3:\n",
    "            ax.set_xlabel('Second Cluster Variable $\\ell$')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbxf265L1_ak"
   },
   "source": [
    "### Cluster Robust Cross Fitting\n",
    "A key element of cluster robust DML ([Chiang et al. 2021](https://doi.org/10.1080/07350015.2021.1895815)) is a special sample splitting used for the cross-fitting.\n",
    "In case of two-way clustering, we assume $N$ clusters in the first dimension and $M$ clusters in the second dimension.\n",
    "\n",
    "For $K$-fold cross-fitting, [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815) proposed to randomly partition $[N]:=\\{1,\\ldots,N\\}$ into $K$ subsets $\\{I_1, \\ldots, I_K\\}$ and $[M]:=\\{1,\\ldots,N\\}$ into $K$ subsets $\\{J_1, \\ldots, J_K\\}$.\n",
    "Effectively, one then considers $K^2$ folds.\n",
    "Basically for each $(k, \\ell) \\in \\{1, \\ldots, K\\} \\times \\{1, \\ldots, K\\}$, the nuisance functions are estimated for all double-indexed observations in $([N]\\setminus I_K) \\times ([M]\\setminus J_\\ell)$, i.e.,\n",
    "$$\n",
    "\\hat{\\eta}_{k\\ell} = \\hat{\\eta}\\left((W_{ij})_{(i,j)\\in ([N]\\setminus I_K) \\times ([M]\\setminus J_\\ell)}\\right)\n",
    "$$\n",
    "The causal parameter is then estimated as usual by solving a moment condition with a Neyman orthogonal score function.\n",
    "For two-way cluster robust double machine learning with algorithm [DML2](https://docs.doubleml.org/stable/guide/algorithms.html#algorithm-dml2) this results in solving\n",
    "$$\n",
    "\\frac{1}{K^2} \\sum_{k=1}^{K} \\sum_{\\ell=1}^{K} \\frac{1}{|I_k| |J_\\ell|} \\sum_{(i,j) \\in I_K \\times J_\\ell}\n",
    "\\psi(W_{ij}, \\tilde{\\theta}_0, \\hat{\\eta}_{k\\ell}) = 0\n",
    "$$\n",
    "for $\\tilde{\\theta}_0$.\n",
    "Here $|I_k|$ denotes the cardinality, i.e., the number of clusters in the $k$-th fold for the first cluster variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwgzBXvQ1_am"
   },
   "source": [
    "We can visualize the sample splitting of the $N \\cdot M = 625$ observations into $K \\cdot K = 9$ folds. The following heat map illustrates the partitioned data set that is split into $K=9$ folds. The horizontal axis corresponds to the fold indices and the vertical axis to the indices of the observations. A blue field indicates that the observation $i$ is used for fitting the nuisance part, red indicates that the fold is used for prediction generation and white means that an observation is left out from the sample splitting. \n",
    "\n",
    "For example, the first observation as displayed on the very bottom of the figure (using Python indexing starting at `0`) is used for training of the nuisance parts in the first (`0`), the third (`2`), fourth (`3`) and sixth (`5`) fold and used for generation of the predictions in fold eight (`7`). At the same time the observation is left out from the sample splitting procedure in folds two (`1`), five (`4`), seven (`6`) and nine (`8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eU-ukl81_ar"
   },
   "outputs": [],
   "source": [
    "# The function plt_smpls is defined at the end of the Notebook\n",
    "plt_smpls(dml_pliv_obj.smpls[0], dml_pliv_obj._n_folds_per_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbylaFQx1_ay"
   },
   "source": [
    "If we visualize the sample splitting in terms of the cluster variables, the partitioning of the data into $9$ folds $I_k \\times J_\\ell$ becomes clear.\n",
    "The identifiers for the first cluster variable $[N]:=\\{1,\\ldots,N\\}$ have been randomly partioned into $K=3$ folds denoted by $\\{I_1, I_2, I_3\\}$ and the identifiers for the second cluster variable $[M]:=\\{1,\\ldots,M\\}$ have also been randomly partioned into $K=3$ folds denoted by $\\{J_1, J_2, J_3\\}$.\n",
    "By considering every combination $I_k \\times J_\\ell$ for $1 \\leq k, \\ell \\leq K = 3$ we effectively base the cross-fitting on $9$ folds.\n",
    "\n",
    "We now want to focus on the top-left sub-plot showing the partitioning of the cluster data for the first fold.\n",
    "The $x$-axis corresponds to the first cluster variable and the $y$-axis to the second cluster variable.\n",
    "Observations with cluster variables $(i,j) \\in I_K \\times J_\\ell$ are used for estimation of the target parameter $\\tilde{\\theta}_0$ by solving a Neyman orthogonal score function.\n",
    "For estimation of the nuisance function, we only use observation where neither the first cluster variable is in $I_K$ nor the second cluster variable is in $J_\\ell$, i.e., we use observations indexed by $(i,j)\\in ([N]\\setminus I_K) \\times ([M]\\setminus J_\\ell)$ to estimate the nuisance functions\n",
    "$$\n",
    "\\hat{\\eta}_{k\\ell} = \\hat{\\eta}\\left((W_{ij})_{(i,j)\\in ([N]\\setminus I_K) \\times ([M]\\setminus J_\\ell)}\\right).\n",
    "$$\n",
    "This way we guarantee that there are never observations from the same cluster (first and/or second cluster dimension) in the sample for the nuisance function estimation (blue) and at the same time in the sample for solving the score function (red). As a result of this special sample splitting proposed by [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815), the observations in the score (red) and nuisance (blue) sample can be considered independent and the standard cross-fitting approach for double machine learning can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfI-DvHV1_a1",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "# The function plt_smpls_cluster is defined at the end of the Notebook\n",
    "plt_smpls_cluster(dml_pliv_obj.smpls_cluster[0], dml_pliv_obj._n_folds_per_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS8OuXQG1_a5"
   },
   "source": [
    "### Cluster Robust Standard Errors\n",
    "In the abstract base class `DoubleML` the estimation of cluster robust standard errors is implemented for all supported double machine learning models.\n",
    "It is based on the assumption of a linear Neyman orthogonal score function.\n",
    "We use the notation $n \\wedge m := \\min\\{n,m\\}$.\n",
    "For the the asymptotic variance of\n",
    "$\\sqrt{\\underline{C}}(\\tilde{\\theta_0} - \\theta_0)$ with\n",
    "$\\underline{C} := N \\wedge M$\n",
    "[Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815) then propose the following estimator\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\hat{J}^{-1} \\hat{\\Gamma} \\hat{J}^{-1}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\Gamma} = \\frac{1}{K^2} \\sum_{(k, \\ell) \\in[K]^2}\n",
    "\\Bigg[ \\frac{|I_k| \\wedge |J_\\ell|}{(|I_k||J_\\ell|)^2}\n",
    "\\bigg(&\\sum_{i \\in I_k} \\sum_{j \\in J_\\ell} \\sum_{j' \\in J_\\ell}\n",
    "\\psi(W_{ij}; \\tilde{\\theta}, \\hat{\\eta}_{k \\ell}) \\psi(W_{ij'}; \\tilde{\\theta}_0, \\hat{\\eta}_{k \\ell}) \\\\\n",
    "&+ \\sum_{i \\in I_k} \\sum_{i' \\in I_k} \\sum_{j \\in J_\\ell}\n",
    "\\psi(W_{ij}; \\tilde{\\theta}, \\hat{\\eta}_{k \\ell}) \\psi(W_{i'j}; \\tilde{\\theta}_0, \\hat{\\eta}_{k \\ell})\n",
    "\\bigg)\n",
    "\\Bigg]\n",
    "\\end{aligned}$$\n",
    "and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{J} = \\frac{1}{K^2} \\sum_{(k, \\ell) \\in[K]^2} \\frac{1}{|I_k||J_\\ell|}\n",
    "\\sum_{i \\in I_k} \\sum_{j \\in J_\\ell}\n",
    "\\psi_a(W_{ij}; \\tilde{\\theta}_0, \\hat{\\eta}_{k \\ell}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "A $(1-\\alpha)$ confidence interval is then given by ([Chiang et al. 2021](https://doi.org/10.1080/07350015.2021.1895815))\n",
    "$$\\begin{aligned}\n",
    "\\left[\n",
    "\\tilde{\\theta} \\pm \\Phi^{-1}(1-\\alpha/2) \\sqrt{\\hat{\\sigma}^2 / \\underline{C}}\n",
    "\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "with $\\underline{C} = N \\wedge M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcNPzPnC1_a9"
   },
   "outputs": [],
   "source": [
    "# Estimate the PLIV model with cluster robust double machine learning\n",
    "dml_pliv_obj.fit()\n",
    "dml_pliv_obj.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNRVgYrF1_bA"
   },
   "source": [
    "## (One-Way) Cluster Robust Double Machine Learing\n",
    "\n",
    "We again use the PLIV data generating process described in Section 4.1 of [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815).\n",
    "To obtain one-way clustered data, we set the following weights to zero\n",
    "$$\n",
    "\\omega_2^X = \\omega_2^\\varepsilon = \\omega_2^v = \\omega_2^V = 0.\n",
    "$$\n",
    "Again we can simulate this data with [make_pliv_multiway_cluster_CKMS2021()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.make_pliv_multiway_cluster_CKMS2021.html#doubleml.datasets.make_pliv_multiway_cluster_CKMS2021). To prepare the data-backend for one-way clustering, we only have to alter the `cluster_cols` to be `'cluster_var_i'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvJ-wHSV1_bD"
   },
   "outputs": [],
   "source": [
    "obj_dml_data = make_pliv_multiway_cluster_CKMS2021(N, M, dim_X,\n",
    "                                                   omega_X=np.array([0.25, 0]),\n",
    "                                                   omega_epsilon=np.array([0.25, 0]),\n",
    "                                                   omega_v=np.array([0.25, 0]),\n",
    "                                                   omega_V=np.array([0.25, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIMLXmD51_bF"
   },
   "outputs": [],
   "source": [
    "obj_dml_data.cluster_cols = 'cluster_var_i'\n",
    "print(obj_dml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jb0zciH41_bJ"
   },
   "outputs": [],
   "source": [
    "# Set machine learning methods for l, m & r\n",
    "learner = LassoCV()\n",
    "ml_l = clone(learner)\n",
    "ml_m = clone(learner)\n",
    "ml_r = clone(learner)\n",
    "\n",
    "# initialize the DoubleMLPLIV object\n",
    "dml_pliv_obj = DoubleMLPLIV(obj_dml_data,\n",
    "                            ml_l, ml_m, ml_r,\n",
    "                            n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcQHiCFG1_bN"
   },
   "outputs": [],
   "source": [
    "dml_pliv_obj.fit()\n",
    "dml_pliv_obj.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BViUo9UW1_bZ"
   },
   "source": [
    "## Real-Data Application\n",
    "As a real-data application we revist the consumer demand example from [Chiang et al. (2021)](https://doi.org/10.1080/07350015.2021.1895815).\n",
    "The U.S. automobile data of [Berry, Levinsohn, and Pakes (1995)](https://doi.org/10.2307/2171802) is obtained from the `R` package [hdm](https://cran.r-project.org/web/packages/hdm/index.html). In this example, we consider different specifications for the cluster dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7p7eaeO1_bb"
   },
   "source": [
    "### Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM30YzZo1_bh"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from rpy2.robjects.packages import PackageData\n",
    "from rpy2.robjects import pandas2ri, default_converter, conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWO-X1aZ1_bj"
   },
   "outputs": [],
   "source": [
    "r_df = PackageData('hdm').fetch('BLP')['BLP'][0]\n",
    "with conversion.localconverter(default_converter + pandas2ri.converter):\n",
    "    blp_data = conversion.rpy2py(r_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kzu6GG2i1_bo"
   },
   "outputs": [],
   "source": [
    "x_cols = ['hpwt', 'air', 'mpd', 'space']\n",
    "blp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WIE4GPS1_bq"
   },
   "outputs": [],
   "source": [
    "def construct_iv(blp_data, x_cols=['hpwt', 'air', 'mpd', 'space']):\n",
    "    n = blp_data.shape[0]\n",
    "    p = len(x_cols)\n",
    "    \n",
    "    firmid = blp_data['firm.id'].values\n",
    "    cdid = blp_data['cdid'].values\n",
    "    id_var = blp_data['id'].values\n",
    "    X = blp_data[x_cols]\n",
    "    \n",
    "    sum_other = pd.DataFrame(columns=['sum.other.' + var for var in x_cols],\n",
    "                             index=blp_data.index)\n",
    "    sum_rival = pd.DataFrame(columns=['sum.rival.' + var for var in x_cols],\n",
    "                             index=blp_data.index)\n",
    "    \n",
    "    for i in range(n):\n",
    "        other_ind = (firmid == firmid[i]) & (cdid == cdid[i]) & (id_var != id_var[i])\n",
    "        rival_ind = (firmid != firmid[i]) & (cdid == cdid[i])\n",
    "        for j in range(p):\n",
    "            sum_other.iloc[i, j] = X.iloc[:,j][other_ind].sum()\n",
    "            sum_rival.iloc[i, j] = X.iloc[:,j][rival_ind].sum()\n",
    "    \n",
    "    return pd.concat((sum_other, sum_rival), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iTzEmsT1_br"
   },
   "outputs": [],
   "source": [
    "iv_vars = construct_iv(blp_data, x_cols=['hpwt', 'air', 'mpd', 'space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjJEPgd61_bs"
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "data_transf = poly.fit_transform(blp_data[x_cols])\n",
    "x_cols_poly = poly.get_feature_names(x_cols)\n",
    "data_transf = pd.DataFrame(data_transf, columns=x_cols_poly)\n",
    "data_transf.index = blp_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzG84J7K1_b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_cols_chiang = list(np.setdiff1d(data_transf.columns,\n",
    "                                    ['hpwt air mpd', 'hpwt air space',\n",
    "                                     'hpwt mpd space', 'air mpd space']))\n",
    "sel_cols_chiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8IeK-0A1_b6"
   },
   "outputs": [],
   "source": [
    "blp_data['log_p'] = np.log(blp_data['price'] + 11.761)\n",
    "\n",
    "y_col = 'y'\n",
    "d_col = 'log_p'\n",
    "cluster_cols = ['model.id', 'cdid']\n",
    "all_z_cols = ['sum.other.hpwt', 'sum.other.mpd', 'sum.other.space']\n",
    "z_col = all_z_cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eVrppKH1_b7"
   },
   "outputs": [],
   "source": [
    "dml_df = pd.concat((blp_data[[y_col] + [d_col] + cluster_cols],\n",
    "                    data_transf[sel_cols_chiang],\n",
    "                    iv_vars[all_z_cols]),\n",
    "                   axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezGVW5Gl1_b8"
   },
   "outputs": [],
   "source": [
    "dml_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05nt4NKc1_b9"
   },
   "source": [
    "### Initialize `DoubleMLClusterData` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik3fUxCn1_cH"
   },
   "outputs": [],
   "source": [
    "dml_data = DoubleMLClusterData(dml_df,\n",
    "                               y_col=y_col,\n",
    "                               d_cols=d_col,\n",
    "                               z_cols=z_col,\n",
    "                               cluster_cols=cluster_cols,\n",
    "                               x_cols=sel_cols_chiang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Y4wQOsC1_cJ"
   },
   "outputs": [],
   "source": [
    "print(dml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGgk1Yfy1_cK"
   },
   "outputs": [],
   "source": [
    "learner = LassoCV(max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GssTYt2i1_cM"
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame()\n",
    "n_rep = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsagdd4V1_cN"
   },
   "source": [
    "### Two-Way Clustering with Respect to Product and Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dd2InB0w1_cO"
   },
   "outputs": [],
   "source": [
    "dml_data.z_cols = z_col\n",
    "dml_data.cluster_cols = ['model.id', 'cdid']\n",
    "dml_pliv = DoubleMLPLIV(dml_data,\n",
    "                        clone(learner), clone(learner), clone(learner),\n",
    "                        n_folds=2, n_rep=n_rep)\n",
    "dml_pliv.fit()\n",
    "res = dml_pliv.summary.reset_index(drop=True)\n",
    "res['z_col'] = dml_data.z_cols[0]\n",
    "res['clustering'] = 'two-way'\n",
    "res_df = pd.concat([res_df, res]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3RhZ4fH1_cS"
   },
   "source": [
    "### One-Way Clustering with Respect to the Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqJxk4S61_cV"
   },
   "outputs": [],
   "source": [
    "dml_data.z_cols = z_col\n",
    "dml_data.cluster_cols = 'model.id'\n",
    "dml_pliv = DoubleMLPLIV(dml_data,\n",
    "                        clone(learner), clone(learner), clone(learner),\n",
    "                        n_folds=4, n_rep=n_rep)\n",
    "dml_pliv.fit()\n",
    "res = dml_pliv.summary.reset_index(drop=True)\n",
    "res['z_col'] = dml_data.z_cols[0]\n",
    "res['clustering'] = 'one-way-product'\n",
    "res_df = pd.concat([res_df, res]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODt4TiKL1_cX"
   },
   "source": [
    "### One-Way Clustering with Respect to the Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl2CTDBu1_cY"
   },
   "outputs": [],
   "source": [
    "dml_data.z_cols = z_col\n",
    "dml_data.cluster_cols = 'cdid'\n",
    "dml_pliv = DoubleMLPLIV(dml_data,\n",
    "                        clone(learner), clone(learner), clone(learner),\n",
    "                        n_folds=4, n_rep=n_rep)\n",
    "dml_pliv.fit()\n",
    "res = dml_pliv.summary.reset_index(drop=True)\n",
    "res['z_col'] = dml_data.z_cols[0]\n",
    "res['clustering'] = 'one-way-market'\n",
    "res_df = pd.concat([res_df, res]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96RUevUV1_cZ"
   },
   "source": [
    "### No Clustering / Zero-Way Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkKU9Fxx1_cc"
   },
   "outputs": [],
   "source": [
    "dml_data = DoubleMLData(dml_df,\n",
    "                        y_col=y_col,\n",
    "                        d_cols=d_col,\n",
    "                        z_cols=z_col,\n",
    "                        x_cols=sel_cols_chiang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxCoX1CP1_cd"
   },
   "outputs": [],
   "source": [
    "print(dml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WY3iQ5aO1_cq"
   },
   "outputs": [],
   "source": [
    "dml_data.z_cols = z_col\n",
    "dml_pliv = DoubleMLPLIV(dml_data,\n",
    "                        clone(learner), clone(learner), clone(learner),\n",
    "                        n_folds=4, n_rep=n_rep)\n",
    "dml_pliv.fit()\n",
    "res = dml_pliv.summary.reset_index(drop=True)\n",
    "res['z_col'] = dml_data.z_cols[0]\n",
    "res['clustering'] = 'zero-way'\n",
    "res_df = pd.concat([res_df, res]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmc3x1n-1_cr"
   },
   "source": [
    "### Application Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PO-RMWTe1_cs"
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhIE_t-A1_ct"
   },
   "source": [
    "## References\n",
    "Berry, S., Levinsohn, J., and Pakes, A. (1995), Automobile Prices in Market\n",
    "Equilibrium, Econometrica: Journal of the Econometric Society, 63, 841-890, doi: [10.2307/2171802](https://doi.org/10.2307/2171802).\n",
    "\n",
    "Cameron, A. C., Gelbach, J. B. and Miller, D. L. (2011), Robust Inference with Multiway Clustering, Journal of Business & Economic Statistics, 29:2, 238-249, doi: [10.1198/jbes.2010.07136](https://doi.org/10.1198/jbes.2010.07136).\n",
    "\n",
    "Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68, doi: [10.1111/ectj.12097](https://doi.org/10.1111/ectj.12097).\n",
    "\n",
    "Chiang, H. D., Kato K., Ma, Y. and Sasaki, Y. (2021), Multiway Cluster Robust Double/Debiased Machine Learning, Journal of Business & Economic Statistics, doi: [10.1080/07350015.2021.1895815](https://doi.org/10.1080/07350015.2021.1895815), arXiv: [1909.03489](https://arxiv.org/abs/1909.03489)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrete color scheme\n",
    "x = sns.color_palette(\"RdBu_r\", 7)\n",
    "cMap = ListedColormap([x[0], x[3], x[6]])\n",
    "plt.rcParams['figure.figsize'] = 15, 12\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "def plt_smpls(smpls, n_folds_per_cluster):\n",
    "    df = pd.DataFrame(np.zeros([N*M, n_folds_per_cluster*n_folds_per_cluster]))\n",
    "    for i_split, this_split_ind in enumerate(smpls):\n",
    "        df.loc[this_split_ind[0], i_split] = -1.\n",
    "        df.loc[this_split_ind[1], i_split] = 1.\n",
    "\n",
    "    ax = sns.heatmap(df, cmap=cMap);\n",
    "    ax.invert_yaxis();\n",
    "    ax.set_ylim([0, N*M]);\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('Observation')\n",
    "    colorbar = ax.collections[0].colorbar\n",
    "    colorbar.set_ticks([-0.667, 0, 0.667])\n",
    "    colorbar.set_ticklabels(['Nuisance', '', 'Score'])\n",
    "\n",
    "def plt_smpls_cluster(smpls_cluster, n_folds_per_cluster):\n",
    "    for i_split in range(len(smpls_cluster)):\n",
    "        plt.subplot(n_folds_per_cluster, n_folds_per_cluster, i_split + 1)\n",
    "        df = pd.DataFrame(np.zeros([N*M, 1]),\n",
    "                      index = pd.MultiIndex.from_product([range(N), range(M)]),\n",
    "                      columns=['value'])\n",
    "\n",
    "        df.loc[pd.MultiIndex.from_product(smpls_cluster[i_split][0]), :] = -1.\n",
    "        df.loc[pd.MultiIndex.from_product(smpls_cluster[i_split][1]), :] = 1.\n",
    "\n",
    "        df_wide = df.reset_index().pivot(index=\"level_0\", columns=\"level_1\", values=\"value\")\n",
    "        df_wide.index.name=''\n",
    "        df_wide.columns.name=''\n",
    "\n",
    "        ax = sns.heatmap(df_wide, cmap=cMap);\n",
    "        ax.invert_yaxis();\n",
    "        ax.set_ylim([0, M]);\n",
    "        colorbar = ax.collections[0].colorbar\n",
    "        colorbar.set_ticks([-0.667, 0, 0.667])\n",
    "\n",
    "        l = i_split % n_folds_per_cluster + 1\n",
    "        k = np.floor_divide(i_split, n_folds_per_cluster) + 1\n",
    "        title = f'Nuisance: $I_{{{k}}}^C \\\\times J_{{{l}}}^C$; Score: $I_{{{k}}} \\\\times J_{{{l}}}$'\n",
    "        ax.set_title(title)\n",
    "        if l == n_folds_per_cluster:\n",
    "            colorbar.set_ticklabels(['Nuisance', '', 'Score'])\n",
    "        else:\n",
    "            colorbar.set_ticklabels(['', '', ''])\n",
    "        if l == 1:\n",
    "            ax.set_ylabel('First Cluster Variable $k$')\n",
    "        if k == 3:\n",
    "            ax.set_xlabel('Second Cluster Variable $\\ell$')\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [
    "A7p7eaeO1_bb",
    "ODt4TiKL1_cX",
    "Hmc3x1n-1_cr"
   ],
   "name": "py_double_ml_multiway_cluster.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
