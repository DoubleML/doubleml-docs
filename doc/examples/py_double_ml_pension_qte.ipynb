{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2a0fd8",
   "metadata": {},
   "source": [
    "# Python: Impact of 401(k) on Financial Wealth (Quantile Effects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a967ed5f",
   "metadata": {},
   "source": [
    "In this real-data example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the effect of 401(k) eligibility and participation on accumulated assets. The 401(k) data set has been analyzed in several studies, among others [Chernozhukov et al. (2018)](https://arxiv.org/abs/1608.00060), see [Kallus et al. (2019)](https://arxiv.org/abs/1912.12945) for quantile effects.\n",
    "\n",
    "**Remark:**\n",
    "This notebook focuses on the evaluation of the treatment effect at different quantiles. For a basic introduction to the [DoubleML](https://docs.doubleml.org/stable/index.html) package and a detailed example of the average treatment effect estimation for the 401(k) data set, we refer to the notebook [Python: Impact of 401(k) on Financial Wealth](https://docs.doubleml.org/stable/examples/py_double_ml_pension.html). The Data sections of both notebooks coincide.\n",
    "\n",
    "401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n",
    "\n",
    "One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)â€™s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b41785",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The preprocessed data can be fetched by calling [fetch_401K()](https://docs.doubleml.org/stable/api/generated/doubleml.datasets.fetch_401K.html#doubleml.datasets.fetch_401K). Note that an internet connection is required for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import doubleml as dml\n",
    "import multiprocessing\n",
    "from doubleml.datasets import fetch_401K\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28347df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "colors = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10., 7.5\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('whitegrid', {'axes.spines.top': False,\n",
    "                            'axes.spines.bottom': False,\n",
    "                            'axes.spines.left': False,\n",
    "                            'axes.spines.right': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11763be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_401K(return_type='DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8c00f",
   "metadata": {},
   "source": [
    "The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a4f9dc4",
   "metadata": {},
   "source": [
    "Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively.\n",
    "\n",
    "At first consider eligibility as the treatment and define the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up basic model: Specify variables for data-backend\n",
    "features_base = ['age', 'inc', 'educ', 'fsize', 'marr',\n",
    "                 'twoearn', 'db', 'pira', 'hown']\n",
    "\n",
    "\n",
    "# Initialize DoubleMLData (data-backend of DoubleML)\n",
    "data_dml_base = dml.DoubleMLData(data,\n",
    "                                 y_col='net_tfa',\n",
    "                                 d_cols='e401',\n",
    "                                 x_cols=features_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a22a2ed",
   "metadata": {},
   "source": [
    "## Estimating Potential Quantiles and Quantile Treatment Effects\n",
    "\n",
    "We will use the [DoubleML](https://docs.doubleml.org/stable/index.html) package to estimate quantile treatment effects of 401(k) eligibility, i.e. `e401`.\n",
    "As it is more interesting to take a look at a range of quantiles instead of a single one, we will first define a discretisized grid of quanitles `tau_vec`, which will range from the 10%-quantile to the 90%-quantile.\n",
    "Further, we need a machine learning algorithm to estimate the nuisance elements of our model. In this example, we will use a basic `LGBMClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau_vec = np.arange(0.1,0.95,0.05)\n",
    "n_folds = 5\n",
    "\n",
    "# Learners\n",
    "class_learner = LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=10)\n",
    "reg_learner = LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56cd2b7",
   "metadata": {},
   "source": [
    "Next, we will apply create an `DoubleMLPQ` object for each quantile to fit a quantile model. Here, we have to specifiy, whether we would like to estimate a potential quantile for the treatment group `treatment=1` or control `treatment=0`. Further basic options are trimming and normalization of the propensity scores (`trimming_rule=\"truncate\"`, `trimming_threshold=0.01` and `normalize_ipw=True`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PQ_0 = np.full((len(tau_vec)), np.nan)\n",
    "PQ_1 = np.full((len(tau_vec)), np.nan)\n",
    "\n",
    "ci_PQ_0 = np.full((len(tau_vec),2), np.nan)\n",
    "ci_PQ_1 = np.full((len(tau_vec),2), np.nan)\n",
    "\n",
    "for idx_tau, tau in enumerate(tau_vec):\n",
    "    print(f'Quantile: {tau}')\n",
    "    dml_PQ_0 = dml.DoubleMLPQ(data_dml_base, \n",
    "                              ml_g=clone(class_learner),\n",
    "                              ml_m=clone(class_learner),\n",
    "                              score=\"PQ\",\n",
    "                              treatment=0,\n",
    "                              quantile=tau,\n",
    "                              n_folds=n_folds,\n",
    "                              normalize_ipw=True,\n",
    "                              trimming_rule=\"truncate\",\n",
    "                              trimming_threshold=1e-2)\n",
    "    dml_PQ_1 = dml.DoubleMLPQ(data_dml_base,\n",
    "                              ml_g=clone(class_learner),\n",
    "                              ml_m=clone(class_learner),\n",
    "                              score=\"PQ\",\n",
    "                              treatment=1,\n",
    "                              quantile=tau,\n",
    "                              n_folds=n_folds,\n",
    "                              normalize_ipw=True,\n",
    "                              trimming_rule=\"truncate\",\n",
    "                              trimming_threshold=1e-2)\n",
    "\n",
    "    dml_PQ_0.fit()\n",
    "    dml_PQ_1.fit()\n",
    "\n",
    "    PQ_0[idx_tau] = dml_PQ_0.coef\n",
    "    PQ_1[idx_tau] = dml_PQ_1.coef\n",
    "\n",
    "    ci_PQ_0[idx_tau, :] = dml_PQ_0.confint(level=0.95).to_numpy()\n",
    "    ci_PQ_1[idx_tau, :] = dml_PQ_1.confint(level=0.95).to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ba94f1d",
   "metadata": {},
   "source": [
    "Additionally, each `DoubleMLPQ` object has a (hopefully) helpful summary, which indicates also the evaluation of the nuisance elements with cross-validated estimation. See e.g. `dml_PQ_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611263dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_PQ_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3011ec6c",
   "metadata": {},
   "source": [
    "Finally, let us take a look at the estimated potential quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pq = {\"Quantile\": tau_vec,\n",
    "           \"DML Y(0)\": PQ_0, \"DML Y(1)\": PQ_1,\n",
    "           \"DML Y(0) lower\": ci_PQ_0[:, 0], \"DML Y(0) upper\": ci_PQ_0[:, 1],\n",
    "           \"DML Y(1) lower\": ci_PQ_1[:, 0], \"DML Y(1) upper\": ci_PQ_1[:, 1]}\n",
    "df_pq = pd.DataFrame(data_pq)\n",
    "print(df_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 10., 7.5\n",
    "fig, (ax1, ax2) = plt.subplots(1 ,2)\n",
    "ax1.grid(visible=True); ax2.grid(visible=True)\n",
    "\n",
    "ax1.plot(df_pq['Quantile'],df_pq['DML Y(0)'], color='violet', label='Estimated Quantile Y(0)')\n",
    "ax1.fill_between(df_pq['Quantile'], df_pq['DML Y(0) lower'], df_pq['DML Y(0) upper'], color='violet', alpha=.3, label='Confidence Interval')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(df_pq['Quantile'],df_pq['DML Y(1)'], color='violet', label='Estimated Quantile Y(1)')\n",
    "ax2.fill_between(df_pq['Quantile'], df_pq['DML Y(1) lower'], df_pq['DML Y(1) upper'], color='violet', alpha=.3, label='Confidence Interval')\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "fig.suptitle('Potential Quantiles', fontsize=16)\n",
    "fig.supxlabel('Quantile')\n",
    "_ = fig.supylabel('Potential Quantile and 95%-CI')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a78b537",
   "metadata": {},
   "source": [
    "As we are interested in the QTE, we can use the `DoubleMLQTE` object, which internally fits two `DoubleMLPQ` objects for the treatment and control group. The main advantage is to apply this to a list of quantiles and construct uniformly valid confidence intervals for the range of treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_cores = multiprocessing.cpu_count()\n",
    "cores_used = np.min([5, n_cores - 1])\n",
    "print(f\"Number of Cores used: {cores_used}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "dml_QTE = dml.DoubleMLQTE(data_dml_base,\n",
    "                           ml_g=clone(class_learner),\n",
    "                           ml_m=clone(class_learner),\n",
    "                           quantiles=tau_vec,\n",
    "                           score='PQ',\n",
    "                           n_folds=n_folds,\n",
    "                           normalize_ipw=True,\n",
    "                           trimming_rule=\"truncate\",\n",
    "                           trimming_threshold=1e-2)\n",
    "dml_QTE.fit(n_jobs_models=cores_used)\n",
    "print(dml_QTE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91202581",
   "metadata": {},
   "source": [
    "For uniformly valid confidence intervals, we still need to apply a bootstrap first. \n",
    "Let's take a quick look at the QTEs combinded with a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dml_QTE.bootstrap(n_rep_boot=2000)\n",
    "ci_QTE = dml_QTE.confint(level=0.95, joint=True)\n",
    "\n",
    "data_qte = {\"Quantile\": tau_vec, \"DML QTE\": dml_QTE.coef,\n",
    "            \"DML QTE lower\": ci_QTE[\"2.5 %\"], \"DML QTE upper\": ci_QTE[\"97.5 %\"]}\n",
    "df_qte = pd.DataFrame(data_qte)\n",
    "print(df_qte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10., 7.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(visible=True)\n",
    "\n",
    "\n",
    "ax.plot(df_qte['Quantile'],df_qte['DML QTE'], color='violet', label='Estimated QTE')\n",
    "ax.fill_between(df_qte['Quantile'], df_qte['DML QTE lower'], df_qte['DML QTE upper'], color='violet', alpha=.3, label='Confidence Interval')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Quantile Treatment Effects', fontsize=16)\n",
    "plt.xlabel('Quantile')\n",
    "_ = plt.ylabel('QTE and 95%-CI')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d0c8d17",
   "metadata": {},
   "source": [
    "## Estimating the treatment effect on the Conditional Value a Risk (CVaR)\n",
    "\n",
    "Similar to the evaluation of the estimation of quantile treatment effects (QTEs), we can estimate the conditional value at risk ([CVaR](https://de.wikipedia.org/wiki/Conditional_Value_at_Risk)) for given quantiles. Here, we will only focus on treatment effect estimation, but the DoubleML package also allows for estimation of potential CVaRs.\n",
    "\n",
    "The estimation of treatment effects can be easily done by adjusting the score in the `DoubleMLQTE` object to `score=\"CVaR\"`, as the estimation is based on the same nuisance elements as QTEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "dml_CVAR = dml.DoubleMLQTE(data_dml_base,\n",
    "                           ml_g=clone(reg_learner),\n",
    "                           ml_m=clone(class_learner),\n",
    "                           quantiles=tau_vec,\n",
    "                           score=\"CVaR\",\n",
    "                           n_folds=n_folds,\n",
    "                           normalize_ipw=True,\n",
    "                           trimming_rule=\"truncate\",\n",
    "                           trimming_threshold=1e-2)\n",
    "dml_CVAR.fit(n_jobs_models=cores_used)\n",
    "print(dml_CVAR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f380651e",
   "metadata": {},
   "source": [
    "Estimation of the corresponding (uniformly) valid confidence intervals can be done analogously to the quantile treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_CVAR.bootstrap(n_rep_boot=2000)\n",
    "ci_CVAR = dml_CVAR.confint(level=0.95, joint=True)\n",
    "\n",
    "data_cvar = {\"Quantile\": tau_vec, \"DML CVAR\": dml_CVAR.coef,\n",
    "            \"DML CVAR lower\": ci_CVAR[\"2.5 %\"], \"DML CVAR upper\": ci_CVAR[\"97.5 %\"]}\n",
    "df_cvar = pd.DataFrame(data_cvar)\n",
    "print(df_cvar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "741d477f",
   "metadata": {},
   "source": [
    "Finally, let us take a look at the estimated treatment effects on the CVaR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10., 7.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(visible=True)\n",
    "\n",
    "ax.plot(df_cvar['Quantile'],df_cvar['DML CVAR'], color='violet', label='Estimated CVaR Effect')\n",
    "ax.fill_between(df_cvar['Quantile'], df_cvar['DML CVAR lower'], df_cvar['DML CVAR upper'], color='violet', alpha=.3, label='Confidence Interval')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Conditional Value at Risk', fontsize=16)\n",
    "plt.xlabel('Quantile')\n",
    "_ = plt.ylabel('CVaR Effect and 95%-CI')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b39b3f4",
   "metadata": {},
   "source": [
    "## Estimating local quantile treatment effects (LQTEs)\n",
    "\n",
    "If we have an `IIVM` model with a given instrumental variable, we are still able to identify the local quantile treatment effect (LQTE), the quantile treatment effect on compliers. For the 401(k) pension data we can use `e401` as an instrument for participation `p401`. \n",
    "To fit an `DoubleML` model with an instrument, we have to change the data backend and specify the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DoubleMLData with an instrument\n",
    "\n",
    "# Basic model\n",
    "data_dml_base_iv = dml.DoubleMLData(data,\n",
    "                                    y_col='net_tfa',\n",
    "                                    d_cols='p401',\n",
    "                                    z_cols='e401',\n",
    "                                    x_cols=features_base)\n",
    "\n",
    "print(data_dml_base_iv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "797ab98e",
   "metadata": {},
   "source": [
    "The estimation of local treatment effects can be easily done by adjusting the score in the `DoubleMLQTE` object to `score=\"LPQ\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf543cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dml_LQTE = dml.DoubleMLQTE(data_dml_base_iv,\n",
    "                           ml_g=clone(class_learner),\n",
    "                           ml_m=clone(class_learner),\n",
    "                           quantiles=tau_vec,\n",
    "                           score=\"LPQ\",\n",
    "                           n_folds=n_folds,\n",
    "                           normalize_ipw=True,\n",
    "                           trimming_rule=\"truncate\",\n",
    "                           trimming_threshold=1e-2)\n",
    "dml_LQTE.fit(n_jobs_models=cores_used)\n",
    "print(dml_LQTE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62bd2981",
   "metadata": {},
   "source": [
    "Estimation of the corresponding (uniformly) valid confidence intervals can be done analogously to the quantile treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92451b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_LQTE.bootstrap(n_rep_boot=2000)\n",
    "ci_LQTE = dml_LQTE.confint(level=0.95, joint=True)\n",
    "\n",
    "data_lqte = {\"Quantile\": tau_vec, \"DML LQTE\": dml_LQTE.coef,\n",
    "            \"DML LQTE lower\": ci_LQTE[\"2.5 %\"], \"DML LQTE upper\": ci_LQTE[\"97.5 %\"]}\n",
    "df_lqte = pd.DataFrame(data_lqte)\n",
    "print(df_lqte)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f26ac09",
   "metadata": {},
   "source": [
    "Finally, let us take a look at the estimated local quantile treatment effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10., 7.5\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(visible=True)\n",
    "\n",
    "ax.plot(df_lqte['Quantile'],df_lqte['DML LQTE'], color='violet', label='Estimated LQTE')\n",
    "ax.fill_between(df_lqte['Quantile'], df_lqte['DML LQTE lower'], df_lqte['DML LQTE upper'], color='violet', alpha=.3, label='Confidence Interval')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Local Quantile Treatment Effect', fontsize=16)\n",
    "plt.xlabel('Quantile')\n",
    "_ = plt.ylabel('LQTE and 95%-CI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "04413e1efd13b5e2aaedccc5facec5686292d28983ef24fa021a12c73bd6369e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
