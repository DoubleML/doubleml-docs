{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Static Panel Models with Fixed Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate treatment effects for static panel models with fixed effects in a partially linear panel regression [DoubleMLPLPR](https://docs.doubleml.org/stable/guide/models.html#partially-linear-models-plm) model. The model is based on [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LassoCV\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from doubleml.data import DoubleMLPanelData\n",
    "from doubleml.plm.datasets import make_plpr_CP2025\n",
    "from doubleml import DoubleMLPLPR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the implemented data generating process [make_plpr_CP2025](https://docs.doubleml.org/stable/api/datasets.html#dataset-generators) to generate data similar to the simulation in [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011). For exposition, we use the simple linear `dgp_type=\"dgp1\"`, with 150 units, 10 time periods per unit, and a true treatment effect of `theta=0.5`.\n",
    "\n",
    "We set `time_type=\"int\"` such that the time variable values will be integers. It's also possible to use `\"float\"` or `\"datetime\"` time variables with [DoubleMLPLPR](https://docs.doubleml.org/stable/api/dml_models.html#doubleml-plm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data = make_plpr_CP2025(num_id=150, num_t=10, dim_x=30, theta=0.5, dgp_type=\"dgp1\", time_type=\"int\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a corresponding [DoubleMLPanelData](https://docs.doubleml.org/stable/api/generated/doubleml.data.DoubleMLPanelData.html) object, we need to set `static_panel=True` and specify `id_col` and `time_col` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = DoubleMLPanelData(data, y_col=\"y\", d_cols=\"d\", t_col=\"time\", id_col=\"id\", static_panel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partially linear panel regression (PLPR) model extends the partially linear model to panel data by introducing fixed effects $\\alpha_i^*$.\n",
    "\n",
    "The PLPR model takes the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Y_{it} &= \\theta_0 D_{it} + g_1(X_{it}) + \\alpha_i^* + U_{it}, \\\\\n",
    "    D_{it} &= m_1(X_{it}) + \\gamma_i + V_{it},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $Y_{it}$ outcome, $D_{it}$ treatment, $X_{it}$ covariates, $\\theta_0$ causal treatment effect\n",
    "- $g_1$ and $m_1$ nuisance functions\n",
    "- $\\alpha_i^*$, $\\gamma_i$ unobserved individual heterogeneity, correlated with covariates\n",
    "- $U_{it}$, $V_{it}$ error terms\n",
    "\n",
    "Further note $\\mathbb{E}[U_{it} \\mid D_{it}, X_{it}, \\alpha_i^*] = 0$ and $\\mathbb{E}[V_{it} \\mid X_{it}, \\gamma_i]=0$, but $\\mathbb{E}[\\alpha_i^* \\mid D_{it}, X_{it}] \\neq 0$.\n",
    "\n",
    "Alternatively we can write the partialling-out PLPR as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Y_{it} &= \\theta_0 V_{it} + \\ell_1(X_{it}) + \\alpha_i + U_{it}, \\\\\n",
    "    V_{it} &= D_{it} - m_1(X_{it}) - \\gamma_i,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with nuisance function $\\ell_1$ and fixed effect $\\alpha_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "Define $\\xi_i$ as time-invariant heterogeneity terms influencing outcome and treatment and $L_{t-1}(W_i) = \\{ W_{i1}, \\dots, W_{it-1} \\}$ as lags of a random variable $W_{it}$ at wave $t$.\n",
    "\n",
    "- *No feedback to predictors*\n",
    "$$ X_{it} \\perp L_{t-1} (Y_i, D_i) \\mid L_{t-1} (X_i), \\xi_i $$\n",
    "- *Static panel*\n",
    "$$ Y_{it}, D_{it} \\perp L_{t-1} (Y_i, X_i, D_i) \\mid X_{it}, \\xi_i $$\n",
    "- *Selection on observables and omitted time-invariant variables*\n",
    "$$ Y_{it} (.) \\perp D_{it} \\mid X_{it}, \\xi_i $$\n",
    "- *Homogeneity and linearity of the treatment effect*\n",
    "$$ \\mathbb{E} [Y_{it}(d) - Y_{it}(0) \\mid X_{it}, \\xi_i] = d \\theta_0 $$\n",
    "- *Additive Separability*\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E} [Y_{it}(0) \\mid X_{it}, \\xi_i] &= g_1(X_{it}) + \\alpha^*_i \\quad \\text{where } \\alpha^*_i = \\alpha^*(\\xi_i), \\\\\n",
    "\\mathbb{E} [D_{it} \\mid X_{it}, \\xi_i] &= m_1(X_{it}) + \\gamma_i \\quad \\text{where } \\gamma_i = \\gamma(\\xi_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For more information, see [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the causal effect, we can create a [DoubleMLPLPR](https://docs.doubleml.org/stable/api/dml_models.html#doubleml-plm) object. The model described in [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011) uses block-k-fold cross-fitting, where the entire time series of the sampled unit is allocated to one fold to allow for possible serial correlation within each unit which is common with panel data. Furthermore, cluster robust standard error are employed. [DoubleMLPLPR](https://docs.doubleml.org/stable/guide/models.html#partially-linear-models-plm) implements both aspects by using `id_col` as the cluster variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Approaches\n",
    "\n",
    "[Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011) describes multiple estimation approaches, which can be set with the `approach` parameter. Depending on the type of `approach`, different data transformations are performed along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Random Effects\n",
    "\n",
    "The correlated random effects (cre) approaches includes the a general approach (`cre_general`) and  an approach relying on normality assumptions (`cre_normal`).\n",
    "\n",
    "#### `cre_general`\n",
    "\n",
    "The `cre_general` approach:\n",
    "\n",
    "- Learning $\\ell_1$ from $\\{ Y_{it}, X_{it}, \\bar{X}_i : t=1,\\dots, T \\}_{i=1}^N$.\n",
    "- First learning $\\tilde{m}_1$ from $\\{ D_{it}, X_{it}, \\bar{X}_i : t=1,\\dots, T \\}_{i=1}^N$, with predictions $\\hat{m}_{1,it} = \\tilde{m}_1 (X_{it}, \\bar{X}_i)$\n",
    "    - Calculate $\\hat{\\bar{m}}_i = T^{-1} \\sum_{t=1}^T \\hat{m}_{1,it}$,\n",
    "    - Calculate final nuisance part as $\\hat{m}^*_1 (X_{it}, \\bar{X}_i, \\bar{D}_i) = \\hat{m}_{1,it} + \\bar{D}_i - \\hat{\\bar{m}}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LassoCV()\n",
    "ml_l = clone(learner)\n",
    "ml_m = clone(learner)\n",
    "\n",
    "dml_plpr_cre_general = DoubleMLPLPR(data_obj, ml_l=ml_l, ml_m=ml_m, approach=\"cre_general\", n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the the transformed data using the `data_transform` property after the `DoubleMLPLPR` object was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_cre_general.data_transform.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the covariates inlcude the original $X_{it}$ and additionally the unit mean $\\bar{X}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model, we can print the `DoubleMLPLPR` object. The data summary corresponds to the transformed data. Additional Information at the end also includes a pre-transformation data summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_cre_general.fit()\n",
    "print(dml_plpr_cre_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `cre_normal`\n",
    "\n",
    "The `cre_normal` approach:\n",
    "\n",
    "Under the assumption that the conditional distribution $D_{i1}, \\dots, D_{iT} \\mid X_{i1}, \\dots X_{iT}$ is multivariate normal (see [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011) for further details):\n",
    "- Learn $\\ell_1$ from $\\{ Y_{it}, X_{it}, \\bar{X}_i : t=1,\\dots, T \\}_{i=1}^N$,\n",
    "- Learn $m^*_{1}$ from $\\{ D_{it}, X_{it}, \\bar{X}_i, \\bar{D}_i: t=1,\\dots, T \\}_{i=1}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_cre_normal = DoubleMLPLPR(data_obj, ml_l=ml_l, ml_m=ml_m, approach=\"cre_normal\", n_folds=5)\n",
    "dml_plpr_cre_normal.fit()\n",
    "print(dml_plpr_cre_normal.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cre_normal` approach uses additionally inlcudes $\\bar{D}_i$ in the treatment nuisance estimation. The corresponding data can be assesses by the `d_mean` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_cre_normal.d_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Approaches\n",
    "\n",
    "The transformation approaches include first differences (`fd_exact`) and within-group (`wg_approx`) transformations.\n",
    "\n",
    "#### `fd_exact`\n",
    "\n",
    "Consider first differences (FD) transformation (`fd_exact`) $Q(Y_{it})= Y_{it} - Y_{it-1}$, under the assumptions from above, [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011) show that $\\mathbb{E}[Y_{it}-Y_{it-1} | X_{it-1},X_{it}] =\\Delta \\ell_1 (X_{it-1}, X_{it})$ and $\\mathbb{E}[D_{it}-D_{it-1} | X_{it-1},X_{it}] =\\Delta m_1 (X_{it-1}, X_{it})$. Therefore, the transformed nuisance function can be learnt as\n",
    "\n",
    "- $\\Delta \\ell_1 (X_{it-1}, X_{it})$ from $\\{ Y_{it}-Y_{it-1}, X_{it-1}, X_{it} : t=2, \\dots , T \\}_{i=1}^N$,\n",
    "- $\\Delta m_1 (X_{it-1}, X_{it})$ from $\\{ D_{it}-D_{it-1}, X_{it-1}, X_{it} : t=2, \\dots , T \\}_{i=1}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_fd_exact = DoubleMLPLPR(data_obj, ml_l=ml_l, ml_m=ml_m, approach=\"fd_exact\", n_folds=5)\n",
    "dml_plpr_fd_exact.data_transform.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the outcome and treatment variables are now labeled `y_diff` and `d_diff` to indicate the first-difference transformation. Moreover, lagged covariates $X_{it-1}$ are added and rows for the first time period are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_fd_exact.fit()\n",
    "print(dml_plpr_fd_exact.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `wg_approx`\n",
    "\n",
    "For within-group (WG) transformation (`wg_approx`) $Q(X_{it})= X_{it} - \\bar{X}_{i}$, where $\\bar{X}_{i} = T^{-1} \\sum_{t=1}^T X_{it}$, approximate the model as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Q(Y_{it}) &\\approx \\theta_0 Q(D_{it}) + g_1 (Q(X_{it})) + Q(U_{it}), \\\\\n",
    "    Q(D_{it}) &\\approx m_1 (Q(X_{it})) + Q(V_{it}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similarly for the partialling-out PLPR\n",
    "\n",
    "$$\n",
    "Q(Y_{it}) \\approx \\theta_0 Q(V_{it}) + \\ell_1 (Q(X_{it})) + Q(U_{it}).\n",
    "$$\n",
    "\n",
    "- $\\ell_1$ can be learnt from transformed data $\\{ Q(Y_{it}), Q(X_{it}) : t=1,\\dots,T \\}_{i=1}^N$,\n",
    "- $m_1$ can be learnt from transformed data $\\{ Q(D_{it}), Q(X_{it}) : t=1,\\dots,T \\}_{i=1}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_wg_approx = DoubleMLPLPR(data_obj, ml_l=ml_l, ml_m=ml_m, approach=\"wg_approx\", n_folds=5)\n",
    "dml_plpr_wg_approx.data_transform.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the outcome, treatment and covariate variables are now labeled `y_deman`, `d_deman`, `xi_deman` to indicate the within-group transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_wg_approx.fit()\n",
    "print(dml_plpr_wg_approx.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple linear data generating process `dgp_type=\"dgp1\"`, we can see that all approaches lead to estimated close the true effect of `theta=0.5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data_original` property additionally includes the original data before any transformation was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_plpr_wg_approx.data_original.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing pipelines\n",
    "\n",
    "We can incorporate preprocessing pipelines. For example, when using Lasso, we may want to include polynomial and interaction terms. Here, we create a class that allows us to include, for example, polynomials of order 3 and interactions between all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyPlus(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"PolynomialFeatures(degree=k) and additional terms x_i^(k+1).\"\"\"\n",
    "\n",
    "    def __init__(self, degree=2, interaction_only=False, include_bias=False):\n",
    "        self.degree = degree\n",
    "        self.extra_degree = degree + 1\n",
    "        self.interaction_only = interaction_only\n",
    "        self.include_bias = include_bias\n",
    "        self.poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=include_bias)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.poly.fit(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X)\n",
    "        X_poly = self.poly.transform(X)\n",
    "        X_extra = X ** self.extra_degree\n",
    "        return np.hstack([X_poly, X_extra])\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        input_features = np.array(\n",
    "            input_features\n",
    "            if input_features is not None\n",
    "            else [f\"x{i}\" for i in range(self.n_features_in_)]\n",
    "        )\n",
    "        poly_names = self.poly.get_feature_names_out(input_features)\n",
    "        extra_names = [f\"{name}^{self.extra_degree}\" for name in input_features]\n",
    "        return np.concatenate([poly_names, extra_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we use the non-linear and discontinuous `dgp_type=\"dgp3\"`, with 30 covariates and a true treatment effect `theta=0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 30\n",
    "theta = 0.5\n",
    "\n",
    "np.random.seed(123)\n",
    "data_dgp3 = make_plpr_CP2025(num_id=500, num_t=10, dim_x=dim_x, theta=theta, dgp_type=\"dgp3\")\n",
    "dml_data_dgp3 = DoubleMLPanelData(data_dgp3, y_col=\"y\", d_cols=\"d\", t_col=\"time\", id_col=\"id\", static_panel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the polynomial and intercation transformation for specific sets of covariates. For example, for the `fd_exact` approach, we can apply it to the original $X_{it}$ and lags $X_{it-1}$ seperately using `ColumnTransformer`.\n",
    "\n",
    "To achieve this, we pass need to pass the corresponding indices for these two sets. `DoubleMLPLPR` stacks sets $X_{it}$ and $X_{it-1}$ column-wise. Given our example data has 30 covariates, this means that the first 30 features in the nuisance estimation correspond to the original $X_{it}$, and the last 30 correspond to lags $X_{it-1}$. Therefore we define the indices `indices_x` and `indices_x_tr` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_x = [x for x in range(dim_x)]\n",
    "indices_x_tr = [x + dim_x for x in indices_x]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"poly_x\",\n",
    "            PolyPlus(degree=2, include_bias=False, interaction_only=False),\n",
    "            indices_x,\n",
    "        ),\n",
    "        (\n",
    "            \"poly_x_tr\",\n",
    "            PolyPlus(degree=2, include_bias=False, interaction_only=False),\n",
    "            indices_x_tr,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessor can be applied for approaches `cre_general` and `cre_normal` in the same fashion. In this case the two sets of covariates would be the original $X_{it}$ and the unit mean $\\bar{X}_i$.\n",
    "\n",
    "**Remark**: Note that we set `remainder=\"passthrough\"` such that all remaining features, not part of `indices_x` and `indices_x_tr`, would not be preprocessed but still included in the nuisance estimation. This is particularly important for the `cre_normal` approach, as $\\bar{D}_i$ is further added to $X_{it}$ and $\\bar{X}_i$ in the treatment nuisance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the learner using a pipeline and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_lasso = make_pipeline(\n",
    "    preprocessor, StandardScaler(), LassoCV(alphas=20, cv=2, n_jobs=5)\n",
    ")\n",
    "\n",
    "ml_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_fd = DoubleMLPLPR(dml_data_dgp3, clone(ml_lasso), clone(ml_lasso), approach=\"fd_exact\", n_folds=5)\n",
    "plpr_lasso_fd.fit(store_models=True)\n",
    "print(plpr_lasso_fd.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we apply the polynomial and interactions preprossing to two sets of 30 columns each, the number of features is 1050."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_fd.models[\"ml_m\"][\"d_diff\"][0][0].named_steps[\"lassocv\"].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As describes above, for the `cre_normal` approach adds $\\bar{X}_i$ to the features used in the treatment nuisance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_cre_normal = DoubleMLPLPR(dml_data_dgp3, clone(ml_lasso), clone(ml_lasso), approach=\"cre_normal\", n_folds=5)\n",
    "plpr_lasso_cre_normal.fit(store_models=True)\n",
    "print(plpr_lasso_cre_normal.summary)\n",
    "plpr_lasso_cre_normal.models[\"ml_m\"][\"d\"][0][0].named_steps[\"lassocv\"].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `wg_approx` approach, there is only one set of features. We can create a similar learner for this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_wg = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"poly_x\",\n",
    "            PolyPlus(degree=2, include_bias=False, interaction_only=False),\n",
    "            indices_x,\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "ml_lasso_wg = make_pipeline(\n",
    "    preprocessor_wg, StandardScaler(), LassoCV(alphas=20, cv=2, n_jobs=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_wg = DoubleMLPLPR(dml_data_dgp3, clone(ml_lasso_wg), clone(ml_lasso_wg), approach=\"wg_approx\", n_folds=5)\n",
    "plpr_lasso_wg.fit(store_models=True)\n",
    "print(plpr_lasso_wg.summary)\n",
    "plpr_lasso_wg.models[\"ml_l\"][\"d_demean\"][0][0].named_steps[\"lassocv\"].n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the more complicated data generating process `dgp3`, the approximation approach performs worse compared to the other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, below we should how to select a specific covariate subset for preprocessing. This can be useful in case of the data includes dummy covariates, where adding polynomials might not be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = dml_data_dgp3.x_cols \n",
    "x_cols_to_pre = [\"x3\", \"x6\", \"x22\"]\n",
    "\n",
    "indices_x_pre = [i for i, c in enumerate(x_cols) if c in x_cols_to_pre]\n",
    "\n",
    "preprocessor_alt = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"poly_x\",\n",
    "            PolyPlus(degree=2, include_bias=False, interaction_only=False),\n",
    "            indices_x_pre,\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "ml_lasso_alt = make_pipeline(\n",
    "    preprocessor_alt, StandardScaler(), LassoCV(alphas=20, cv=2, n_jobs=5)\n",
    ")\n",
    "\n",
    "plpr_lasso_wg.learner[\"ml_l\"] = ml_lasso_alt\n",
    "plpr_lasso_wg.learner[\"ml_m\"] = ml_lasso_alt\n",
    "\n",
    "plpr_lasso_wg.fit(store_models=True)\n",
    "plpr_lasso_wg.models[\"ml_l\"][\"d_demean\"][0][0].named_steps[\"lassocv\"].n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_wg.models[\"ml_l\"][\"d_demean\"][0][0].named_steps['columntransformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the resulting features.\n",
    "\n",
    "**Remark**: Note, however, that the feature names here refer only to the corresponding `x_cols` indices, not the column names from the `pd.DataFrame` because [DoubleML](https://docs.doubleml.org/stable/index.html) uses  `np.array`'s for fitting the model. Therefore the difference to the names from `x_cols_to_pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_lasso_wg.models[\"ml_l\"][\"d_demean\"][0][0].named_steps['columntransformer'].get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section we will use the `tune_ml_models()` method to tune hyperparameters using the [Optuna](https://optuna.org/) package. More details can found in the [Python: Hyperparametertuning with Optuna](https://docs.doubleml.org/stable/examples/learners/py_optuna.html) example notebook.\n",
    "\n",
    "As an example, we use [LightGBM](https://lightgbm.readthedocs.io/en/stable/) regressors and compare the estimates for the different static panel model approaches, when applied to the non-linear and discontinuous `dgp3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 30\n",
    "theta = 0.5\n",
    "\n",
    "np.random.seed(11)\n",
    "data_tune = make_plpr_CP2025(num_id=4000, num_t=10, dim_x=dim_x, theta=theta, dgp_type=\"dgp3\")\n",
    "dml_data_tune = DoubleMLPanelData(data_tune, y_col=\"y\", d_cols=\"d\", t_col=\"time\", id_col=\"id\", static_panel=True)\n",
    "ml_boost = LGBMRegressor(random_state=314, verbose=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter space for both ml models\n",
    "def ml_params(trial):\n",
    "    return {\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.4, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 5, log=True),\n",
    "    }\n",
    "\n",
    "param_space = {\n",
    "    \"ml_l\": ml_params,\n",
    "    \"ml_m\": ml_params\n",
    "}\n",
    "\n",
    "optuna_settings = {\n",
    "    \"n_trials\": 100,\n",
    "    \"show_progress_bar\": True,\n",
    "    \"verbosity\": optuna.logging.WARNING,  # Suppress Optuna logs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_tune_cre_general = DoubleMLPLPR(dml_data_tune, clone(ml_boost), clone(ml_boost), approach=\"cre_general\", n_folds=5)\n",
    "\n",
    "plpr_tune_cre_general.tune_ml_models(\n",
    "    ml_param_space=param_space,\n",
    "    optuna_settings=optuna_settings,\n",
    ")\n",
    "\n",
    "plpr_tune_cre_general.fit()\n",
    "plpr_tune_cre_general.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.509102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_tune_cre_normal = DoubleMLPLPR(dml_data_tune, clone(ml_boost), clone(ml_boost), approach=\"cre_normal\", n_folds=5)\n",
    "\n",
    "plpr_tune_cre_normal.tune_ml_models(\n",
    "    ml_param_space=param_space,\n",
    "    optuna_settings=optuna_settings,\n",
    ")\n",
    "\n",
    "plpr_tune_cre_normal.fit()\n",
    "plpr_tune_cre_normal.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_tune_fd = DoubleMLPLPR(dml_data_tune, clone(ml_boost), clone(ml_boost), approach=\"fd_exact\", n_folds=5)\n",
    "\n",
    "plpr_tune_fd.tune_ml_models(\n",
    "    ml_param_space=param_space,\n",
    "    optuna_settings=optuna_settings,\n",
    ")\n",
    "\n",
    "plpr_tune_fd.fit()\n",
    "plpr_tune_fd.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plpr_tune_wg = DoubleMLPLPR(dml_data_tune, clone(ml_boost), clone(ml_boost), approach=\"wg_approx\", n_folds=5)\n",
    "\n",
    "plpr_tune_wg.tune_ml_models(\n",
    "    ml_param_space=param_space,\n",
    "    optuna_settings=optuna_settings,\n",
    ")\n",
    "\n",
    "plpr_tune_wg.fit()\n",
    "plpr_tune_wg.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"colorblind\")\n",
    "\n",
    "ci_cre_general = plpr_tune_cre_general.confint()\n",
    "ci_cre_normal = plpr_tune_cre_normal.confint()\n",
    "ci_fd = plpr_tune_fd.confint()\n",
    "ci_wg = plpr_tune_wg.confint()\n",
    "\n",
    "comparison_data = {\n",
    "    \"Model\": [\"cre_general\", \"cre_normal\", \"fd_exact\", \"wg_approx\"],\n",
    "    \"theta\": [plpr_tune_cre_general.coef[0], plpr_tune_cre_normal.coef[0], plpr_tune_fd.coef[0], plpr_tune_wg.coef[0]],\n",
    "    \"se\": [plpr_tune_cre_general.se[0], plpr_tune_cre_normal.se[0], plpr_tune_fd.se[0], plpr_tune_wg.se[0]],\n",
    "    \"ci_lower\": [ci_cre_general.iloc[0, 0], ci_cre_normal.iloc[0, 0], ci_fd.iloc[0, 0], ci_wg.iloc[0, 0]],\n",
    "    \"ci_upper\": [ci_cre_general.iloc[0, 1], ci_cre_normal.iloc[0, 1], ci_fd.iloc[0, 1], ci_wg.iloc[0, 1]]\n",
    "}\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(f\"True treatment effect: {theta}\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Create comparison plot \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(len(df_comparison)):\n",
    "    plt.errorbar(i, df_comparison.loc[i, \"theta\"],\n",
    "                 yerr=[[df_comparison.loc[i, \"theta\"] - df_comparison.loc[i, \"ci_lower\"]],\n",
    "                       [df_comparison.loc[i, \"ci_upper\"] - df_comparison.loc[i, \"theta\"]]],\n",
    "                 fmt='o', capsize=5, capthick=2, ecolor=palette[i], color=palette[i],\n",
    "                 label=df_comparison.loc[i, \"Model\"], markersize=10, zorder=2)\n",
    "plt.axhline(y=theta, color=palette[4], linestyle='--',\n",
    "            linewidth=2, label=\"True effect\", zorder=1)\n",
    "\n",
    "plt.title(\"Comparison across DoubleMLPLPR approaches\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.xticks(range(4), df_comparison[\"Model\"], rotation=15, ha=\"right\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see that the `wg_approx` leads to a biased estimate in the non-linear and discontinuous `dgp3` setting. The approaches `cre_general`, `cre_normal`, `fd_exact`, in combination with [LightGBM](https://lightgbm.readthedocs.io/en/stable/) regressors, tuned using the [Optuna](https://optuna.org/) package, lead to estimate close to the true treatment effect.\n",
    "\n",
    "This is line with the simulation results in [Clarke and Polselli (2025)](https://doi.org/10.1093/ectj/utaf011), albeit only for only one dataset in this example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
