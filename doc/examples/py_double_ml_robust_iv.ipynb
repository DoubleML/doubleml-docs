{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255d0213",
   "metadata": {},
   "source": [
    "# Python: Confidence Intervals for Instrumental Variables Models That Are Robust to Weak Instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c070d4e",
   "metadata": {},
   "source": [
    "In this example we will show how to use the DoubleML package to obtain confidence sets for the treatment effects that are robust to weak instruments. Weak instruments are those that have a relatively weak correlation with the treatment. It is well known that in this case, standard methods to construct confidence intervals have poor properties and can have coverage much lower than the nominal value. We will assume that the reader of this notebook is already familiar with DoubleML and how it can be used to fit instrumental variable models.\n",
    "\n",
    "Throughout this example\n",
    "\n",
    "- $Z$ is the instrument,\n",
    "- $X$ is a vector of covariates,\n",
    "- $D$ is treatment variable,\n",
    "- $Y$ is the outcome.\n",
    "\n",
    "![robust_iv_example_nb.png](../_static/robust_iv_example_nb.png)\n",
    "\n",
    "\n",
    "Next, we will run a simulation, where we will generate two synthetic data sets, one where the instrument is weak and another where it is not. Then, we will compare the output of the standard way to compute confidence intervals using the ``DoubleMLIIVM`` class, with the confidence sets computed using the ``robust_confset()`` method from the same class. We will see that using the ``robust_confset()`` method is an easy way to ensure the results of an analysis are robust to weak instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import doubleml as dml\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b976ce",
   "metadata": {},
   "source": [
    "# Running a small simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e45c7",
   "metadata": {},
   "source": [
    "The following function generates data from an instrumental variables model. The ``true_effect`` argument is the estimand of interest, the true effect of the treatment on the outcome. The ``instrument_strength`` argument is a measure of the strength of the instrument. The higher it is, the stronger the correlation is between the instrument and the treatment. Notice that the instrument is fully randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82111204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weakiv_data(n_samples, true_effect, instrument_strength):\n",
    "    u = np.random.normal(0, 2, size=n_samples)\n",
    "    X = np.random.normal(0, 1, size=n_samples)\n",
    "    Z = np.random.binomial(1, 0.5, size=n_samples)\n",
    "    D = instrument_strength * Z + u \n",
    "    D = np.array(D > 0, dtype=int)\n",
    "    Y = true_effect * D + np.sign(u)\n",
    "    return pd.DataFrame({\"Y\": Y, \"Z\": Z, \"D\": D, \"X\": X})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c938fd8",
   "metadata": {},
   "source": [
    "To fit the DML model, we need to decide on how we will estimate the nuisance functions. We will use a linear regression model for $g$, and a logistic regression for $r$. We will assume that we know the true $m$ function, as is the case in a controlled experiment, such as an AB test. The following class defines defines this \"fake\" estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a347c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueMFunction(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, prob_dist=(0.5, 0.5)):\n",
    "        self.prob_dist = prob_dist \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.prob_dist_ = np.array(self.prob_dist)\n",
    "        self.classes_ = np.array(sorted(set(y)))\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.tile(self.prob_dist_, (len(X), 1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.full(len(X), self.classes_[np.argmax(self.prob_dist_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf84b0",
   "metadata": {},
   "source": [
    "We will now run a loop, where for each of $100$ replications we will generate data using the previously defined function. We will take a sample size of $5000$, a true effect equal to $1$, and take two possible values for the instrument strength: $0.003$ and $1$. In the latter case the instrument is strong, in the former it is weak. We will then compute both the robust and the standard confidence intervals, check whether they contain the true effect, and compute their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "true_effect = 1\n",
    "output_list = []\n",
    "for _ in range(100):\n",
    "    for instrument_strength in [0.003, 1]:\n",
    "        dataset = generate_weakiv_data(n_samples = n_samples, true_effect = true_effect, instrument_strength = instrument_strength)\n",
    "        dml_data = dml.DoubleMLData(\n",
    "            dataset, y_col='Y', d_cols='D', \n",
    "            z_cols='Z', x_cols='X'\n",
    "        )\n",
    "        ml_g = LinearRegression()\n",
    "        ml_m = TrueMFunction()\n",
    "        ml_r = LogisticRegression(penalty=None)\n",
    "        dml_iivm = dml.DoubleMLIIVM(dml_data, ml_g, ml_m, ml_r)\n",
    "        dml_iivm.fit()\n",
    "        dml_standard_ci = dml_iivm.confint(joint=False)\n",
    "        dml_robust_confset = dml_iivm.robust_confset()\n",
    "        dml_covers = dml_standard_ci[\"2.5 %\"].iloc[0] <= true_effect <= dml_standard_ci[\"97.5 %\"].iloc[0]\n",
    "        robust_covers = any(interval[0] <= true_effect <= interval[1] for interval in dml_robust_confset)\n",
    "        dml_length = dml_standard_ci[\"97.5 %\"].iloc[0] - dml_standard_ci[\"2.5 %\"].iloc[0]\n",
    "        dml_robust_length = max(interval[1] - interval[0] for interval in dml_robust_confset)\n",
    "        output_list.append({\n",
    "            \"instrument_strength\": instrument_strength,\n",
    "            \"dml_covers\": dml_covers,\n",
    "            \"robust_covers\": robust_covers,\n",
    "            \"dml_length\": dml_length,\n",
    "            \"robust_length\": dml_robust_length\n",
    "        })\n",
    "results_df = pd.DataFrame(output_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1366d2",
   "metadata": {},
   "source": [
    "Having stored the results of the simulation in the ``results_df`` dataframe, we will compute some summary statistics. We see in the table below that, when the instrument is strong, the standard DML confidence interval and the robust confidence set behave similarly, with coverage close to the nominal level and similar median lengths. On the other hand, when the instrument is strong, the coverage of the standard DML confidence interval is very low, whereas the coverage of the robust confidence set is again close to the nominal value. Note that in this case the robust confidence set has an infinite median length. When the robust confidence set has infinite length, the analyst should interpret the results as indicating that the data contains little information about the estimand of interest, possibly because the instrument is weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c83edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(output_list)\n",
    "summary_df = results_df.groupby(\"instrument_strength\").agg(\n",
    "    **{\"DML coverage\": (\"dml_covers\", \"mean\"),\n",
    "       \"Robust coverage\": (\"robust_covers\", \"mean\"),\n",
    "       \"DML median length\": (\"dml_length\", \"median\"),\n",
    "       \"Robust median length\": (\"robust_length\", \"median\")}\n",
    ")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd3d05",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946cbbcf",
   "metadata": {},
   "source": [
    "- Chernozhukov, V. & Hansen, C. & Kallus, N. & Spindler, M. & Syrgkanis, V. (2024): Applied Causal Inference Powered by ML and AI; Chapter 13. CausalML-book.org; arXiv:2403.02467.\n",
    "- Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., and Hansen, C. (2018). Double/debiased machine learning for\n",
    "treatment and structural parameters. The Econometrics Journal, 21(1):C1–C68.\n",
    "- Ma, Y. (2023). Identification-robust inference for the late with high-dimensional covariates. arXiv preprint arXiv:2302.09756.\n",
    "- Smucler, E., Lanni, L., Masip, D. (2025). A note on the properties of the confidence set for the local average treatment effect obtained by inverting the score test. arXiv preprint 2506.10449\n",
    "- Stock, J. H. and Wright, J. H. (2000). GMM with weak identification. Econometrica, 68(5):1055–1096.\n",
    "- Takatsu, K., Levis, A. W., Kennedy, E., Kelz, R., and Keele, L. (2023). Doubly robust machine learning for an instrumental\n",
    "variable study of surgical care for cholecystitis. arXiv preprint arXiv:2307.06269."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
