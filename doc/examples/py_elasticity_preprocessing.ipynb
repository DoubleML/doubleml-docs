{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Elasticity Example: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate a subset of a real data set from a UK retail firm, which has been used in a more extensive [blog post on demand elasticity estimation by Lars Roemheld (Roemheld, 2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b). The accompanying code and the data set that has been used in the blog post are available from a [GitHub repository](https://github.com/larsroemheld/causalinf_ex_elasticity). The original data set is a public domain (CC0 1.0 Universal) data set, made available via [kaggle](https://www.kaggle.com/vijayuv/onlineretail).\n",
    "\n",
    "The estimation of price elasticities has also been explored by [Erik KaunismÃ¤ki (2021)](https://helda.helsinki.fi/dhanken/bitstream/handle/10227/441224/Kaunism%C3%A4ki_Erik.pdf?sequence=1) and [Kuan-Pin Lin (2019)](http://web.pdx.edu/~crkl/BDE/MLE-4.pdf). We follow [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b) and acknowledge a technical limitation of the data set in that we only observe sales, not stock days. That means that we have no data at on prices on days where no sales (sales = 0) occurred. It is an issue since typically most of the units are rarely sold; However, we pass over it for the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the data set we proceed as follows: We depart from a cleansed data set used in the analysis of [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b). We will first reproduce the preprocessing steps in his [notebook](https://github.com/larsroemheld/causalinf_ex_elasticity/blob/main/elasticity_dml.ipynb) and later mimick the feature engineering. Finally, we select a subset of the entire data set to have a computationally tractable example.\n",
    "\n",
    "In the first step, [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b) processes the raw data set with information on sales, product codes, quantities and unit prices, country, and description of the product. During preprocessing, outliers were dropped, for example if they have unbelievable high quantities and unit prices. \n",
    "The preprocessed data. The preprocessed data has information on the date (`Date`), the stock keeping unit (`StockCode`, identifying a product), country (`Country`), a short product description as text (`Description`), quantity sold (`Quantity`), revenue (`revenue`) and the price per unit of a product (`UnitPrice`). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing in  [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b)\n",
    "\n",
    "We have already preprocesed data from \n",
    "It seems that start date sales vary between countries. The sales have been recorded since December 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load data install pyarrow in case you haven't installed it before\n",
    "!pip install pyarrow\n",
    "\n",
    "# Load preprocessed data set used in Roemheld (2021) from URL\n",
    "# Note: An internet connection is required\n",
    "df = pd.read_parquet('https://github.com/larsroemheld/causalinf_ex_elasticity/blob/main/ecom_sample_clean.parquet?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in the data set\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Glimpse at head of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df.iloc[0:10]\n",
    "# Save example data set for illustration purpose in analysis notebook\n",
    "df_example.to_csv(\"data/orig_demand_data_example.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of prices per product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3914 products in this data set. We need variation in unit prices within products to estimate elasticity. We choose to make a histogram of the standard variation of the products to identify the distribution of standard deviation between products. We see that there are price products that do not vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique products\n",
    "print(len(pd.unique(df['StockCode'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df.groupby('StockCode').UnitPrice.std().dropna().clip(0, 15), kde=False)\n",
    "plt.title('Distribution of Std(price) over products: reasonable!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop those products which their prices do not vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl = df[\n",
    "    (df.groupby('StockCode').UnitPrice.transform('std') > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mimick Feature Engineering in  [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the price elasticity of demand we have to log-transform the variable on the price and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Logs of main variables\n",
    "df_mdl = df_mdl.assign(\n",
    "    LnP = np.log(df_mdl['UnitPrice']),\n",
    "    LnQ = np.log(df_mdl['Quantity']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the `Date` variable in a way that we have separate variables on the month, the day of the month, the day of the week as well as the stock age, how and the average (median) price of a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from dates and stock code\n",
    "df_mdl = df_mdl.assign(\n",
    "    # Month\n",
    "    month = lambda d: d.Date.dt.month,\n",
    "    # Day of Month\n",
    "    DoM =   lambda d: d.Date.dt.day,\n",
    "    # Day of Week\n",
    "    DoW =   lambda d: d.Date.dt.weekday,\n",
    "    # Stock age: Number of days the product has been sold\n",
    "    stock_age_days = lambda d: \n",
    "        (d.Date - d.groupby('StockCode').Date.transform('min')).dt.days,\n",
    "    # Stock average price: Median of the price\n",
    "    sku_avg_p = lambda d: \n",
    "        d.groupby('StockCode').UnitPrice.transform('median')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the feature generation in the pipeline defined in the original [notebook](https://github.com/larsroemheld/causalinf_ex_elasticity/blob/main/elasticity_dml.ipynb), we apply the same transformations on the entire data set and later pass this as a backend to DoubleML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of Date column\n",
    "one_hot = pd.get_dummies(df_mdl.Date)\n",
    "# Drop column B as it is now encoded\n",
    "df_mdl2 = df_mdl.drop('Date',axis = 1).copy()\n",
    "# Join the encoded df_mdl\n",
    "df_mdl2 = df_mdl2.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mdl2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of Country column\n",
    "one_hot = pd.get_dummies(df_mdl.Country)\n",
    "# Drop column B as it is now encoded\n",
    "df_mdl3 = df_mdl2.drop('Country',axis = 1).copy()\n",
    "# Join the encoded df_mdl\n",
    "df_mdl3 = df_mdl3.join(one_hot).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mdl3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from description data\n",
    "# We get tokens and generate columns using words\n",
    "tokens = CountVectorizer(min_df=0.0025, ngram_range=(1, 3)).fit_transform(df_mdl3['Description'])\n",
    "df_mdl4 = df_mdl3.join(pd.DataFrame(tokens.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mdl4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could one-hot encode the `StockCode` variable too, but it implies to have too much variables and will therefore overfit the ML models. We decide to drop Stock Code fixed effects by de-meaning variables. \n",
    "\n",
    "$$\\text{dLnP}_{i,t} = \\log(p_{i,t}) - \\log(\\bar{p_i})$$\n",
    "\n",
    "$$\\text{dLnQ}_{i,t} = \\log(Q_{i,t}) - \\log(\\bar{Q_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that de-meaning happens on StockCode-level here!\n",
    "df_mdl4['dLnP'] = np.log(df_mdl4.UnitPrice) - np.log(df_mdl4.groupby('StockCode').UnitPrice.transform('mean'))\n",
    "df_mdl4['dLnQ'] = np.log(df_mdl4.Quantity) - np.log(df_mdl4.groupby('StockCode').Quantity.transform('mean')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Variables that are not necessary\n",
    "df_mdl5 = df_mdl4.drop( ['LnP', 'LnQ', 'StockCode', 'Description' ], axis = 1 ).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mdl5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Save a Subset of the Data\n",
    "\n",
    "To have an example that's easier to replicate, we select and save a subset of the data set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl_final = df_mdl5.sample(n = 10000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl_final.to_csv('data/elasticity_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to OLS DML Results in [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b) (with full data set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code replicates the analysis (DML with linear regression learner) in [Roemheld (2021)](https://towardsdatascience.com/causal-inference-example-elasticity-de4a3e2e621b) using the full data set.\n",
    "\n",
    "Note: As in the [notebook](https://github.com/larsroemheld/causalinf_ex_elasticity/blob/main/elasticity_dml.ipynb), no random seed is set, numerical differences might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(df_mdl5.columns[4:(df_mdl5.shape[1]-2),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "data_dml_base = dml.DoubleMLData(df_mdl5,\n",
    "                                 y_col = \"dLnQ\",\n",
    "                                 d_cols = \"dLnP\",\n",
    "                                 x_cols = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols1a = LinearRegression(n_jobs = -1)\n",
    "\n",
    "np.random.seed(123)\n",
    "dml_plr_ols1a = dml.DoubleMLPLR(data_dml_base,\n",
    "                                ml_l = ols1a,\n",
    "                                ml_m = ols1a,\n",
    "                                n_folds = 2,\n",
    "                                score = 'partialling out')\n",
    "\n",
    "dml_plr_ols1a.fit(store_predictions=True)\n",
    "dml_plr_ols1a.summary\n",
    "\n",
    "ols1a_summary = dml_plr_ols1a.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1a_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols1b = LinearRegression(n_jobs = -1)\n",
    "\n",
    "np.random.seed(123)\n",
    "dml_plr_ols1b = dml.DoubleMLPLR(data_dml_base,\n",
    "                                ml_l = ols1b,\n",
    "                                ml_m = ols1b,\n",
    "                                ml_g = ols1b,\n",
    "                                n_folds = 2,\n",
    "                                score = 'IV-type')\n",
    "\n",
    "dml_plr_ols1b.fit(store_predictions=True)\n",
    "dml_plr_ols1b.summary\n",
    "\n",
    "ols1b_summary = dml_plr_ols1b.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1b_summary"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
