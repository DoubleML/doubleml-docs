.. _sensitivity:

Sensitivity analysis
------------------------

The :ref:`DoubleML <doubleml_package>` package implements sensitivity analysis with respect to omitted variable bias
based on `Chernozhukov et al. (2022) <https://www.nber.org/papers/w30302>`_.

General algorithm
+++++++++++++++++

The section :ref:`sensitivity_theory` contains a general summary and the relevant defintions, whereas :ref:`sensitivity_implementation` considers
the general part of the implementation.

.. _sensitivity_theory:

Theory
~~~~~~

Assume that we can write the model in the following representation

.. math::

    \theta_0 = \mathbb{E}[m(W,g_0)],

where usually :math:`g_0(W) = \mathbb{E}[Y|X, D]` (currently, the sensitivity analysis is only available for linear models).
As long as :math:`\mathbb{E}[m(W,f)]` is a continuous linear functional of :math:`f`, there exists a unique square 
integrable random variable :math:`\alpha_0(W)`, called Riesz representer
(see `Riesz-Fr√©chet representation theorem <https://en.wikipedia.org/wiki/Riesz_representation_theorem>`_), such that

.. math::

    \theta_0 = \mathbb{E}[g_0(W)\alpha_0(W)].

The target parameter :math:`\theta_0` has the following representation

.. math::

    \theta_0 = \mathbb{E}[m(W,g_0) + (Y-g_0(W))\alpha_0(W)],

which corresponds to a Neyman orthogonal score function (orthogonal with respect to nuisance elements :math:`(g, \alpha)`).
To bound the omitted variable bias, the following further elements are needed. 
The variance of the outcome regression 

.. math::

    \sigma_0^2 := \mathbb{E}[(Y-g_0(W))^2]

and the second moment of the Riesz representer 

.. math::

    \nu_0^2 := \mathbb{E}[\alpha_0(W)^2] =2\mathbb{E}[m(W,\alpha_0)] -  \mathbb{E}[\alpha_0(W)^2].

Both representations are Neyman orthogonal with respect to :math:`g` and :math:`\alpha`, respectively.
Further, define the corresponding score functions

.. math::

    \psi_{\sigma^2}(W, \sigma^2, g) &:= (Y-g_0(W))^2 - \sigma^2\\
    \psi_{\nu^2}(W, \nu^2, \alpha) &:= 2m(W,\alpha) - \alpha(W)^2 - \nu^2.

Recall that the parameter :math:`\theta_0` is identified via the moment condition

.. math::

    \theta_0 = \mathbb{E}[m(W,g_0)].

If :math:`W=(Y, D, X)` does not include all confounding variables, the "true" target parameter :math:`\tilde{\theta}_0`
would only be identified via the extendend (or "long") form

.. math::

    \tilde{\theta}_0 = \mathbb{E}[m(\tilde{W},\tilde{g}_0)],

where :math:`\tilde{W}=(Y, D, X, A)` includes the unobserved counfounders :math:`A`.
In Theorem 2 of their paper `Chernozhukov et al. (2022) <https://www.nber.org/papers/w30302>`_ are able to bound the omitted variable bias

.. math::

    |\tilde{\theta}_0 -\theta_0|^2 = \rho^2 B^2,

where 

.. math::

    B^2 := \mathbb{E}\Big[\big(g(W) - \tilde{g}(\tilde{W})\big)^2\Big]\mathbb{E}\Big[\big(\alpha(W) - \tilde{\alpha}(\tilde{W})\big)^2\Big],

denotes the product of additional variations in the outcome regression and Riesz representer generated by omitted confounders and

.. math::

    \rho^2 := \textrm{Cor}^2\Big(g(W) - \tilde{g}(\tilde{W}),\alpha(W) - \tilde{\alpha}(\tilde{W})\Big),

denotes the correlations between the deviations generated by omitted confounders. The choice :math:`\rho=1` is conservative and
accounts for adversarial confounding. Further, the bound can be expressed as

.. math::

    B^2 := \sigma_0^2 \nu_0^2 C_Y^2 C_D^2,

where

.. math::

    C_Y^2 &:= \frac{\mathbb{E}[(\tilde{g}(\tilde{W}) - g(W))^2]}{\mathbb{E}[(Y - g(W))^2]}

    C_D^2 &:=\frac{1 - \frac{\mathbb{E}\big[\alpha(W)^2\big]}{\mathbb{E}\big[\tilde{\alpha}(\tilde{W})^2\big]}}{\frac{\mathbb{E}\big[\alpha(W)^2\big]}{\mathbb{E}\big[\tilde{\alpha}(\tilde{W})^2\big]}}.

As :math:`\sigma_0^2` and :math:`\nu_0^2` do not depend on the unobserved confounders :math:`A` they are identified. Further, the other parts have the following interpretations

- ``cf_y``:math:`:=\frac{\mathbb{E}[(\tilde{g}(\tilde{W}) - g(W))^2]}{\mathbb{E}[(Y - g(W))^2]}`  measures the proportion of residual variance in the outcome :math:`Y` explained by the latent confounders :math:`A`

- ``cf_d``:math:`:=1 - \frac{\mathbb{E}\big[\alpha(W)^2\big]}{\mathbb{E}\big[\tilde{\alpha}(\tilde{W})^2\big]}` measures the proportion of residual variance in the Riesz representer :math:`\tilde{\alpha}(\tilde{W})` generated by the latent confounders :math:`A`

.. note::
    - ``cf_y`` has the interpretation as the *nonparametric partial* :math:`R^2` *of* :math:`A` *with* :math:`Y` *given* :math:`(D,X)`
    
    .. math:: 
        
        \frac{\textrm{Var}(\mathbb{E}[Y|D,X,A]) - \textrm{Var}(\mathbb{E}[Y|D,X])}{\textrm{Var}(Y)-\textrm{Var}(\mathbb{E}[Y|D,X])}

    - For model-specific interpretations of ``cf_d`` or :math:`C_D^2`, see the corresponding chapters (e.g. :ref:`sensitivity_plr`).

Consequently, for given values ``cf_y`` and ``cf_d``, we can create lower and upper bounds for target parameter :math:`\tilde{\theta}_0` of the form

.. math::

    \theta_{\pm}:=\theta_0 \pm |\rho| \sigma_0 \nu_0 C_Y C_D

Let :math:`\psi(W,\theta,\eta)` the (correctly scaled) score function for the target parameter :math:`\theta_0`. Then

.. math::

    \psi_{\pm}(W,\theta,\eta_\pm):= \psi(W,\theta,\eta) \pm \frac{|\rho| C_Y C_D}{2 \sigma \nu} \Big(\sigma^2 \psi_{\nu^2}(W, \nu^2, \alpha) + \nu^2 \psi_{\sigma^2}(W, \sigma^2, g)\Big)

determines a orthongonal score function for :math:`\theta_{\pm}`, with nuisance elements :math:`\eta_\pm:=(g, \alpha, \sigma, \nu)`.
The score can be used to calculate the standard deviations of :math:`\theta_{\pm}` via

.. math::

    \sigma^2_{\pm}= \mathbb{E}[\psi_{\pm}(W,\theta,\eta_\pm)^2]
    
For more detail and interpretations see `Chernozhukov et al. (2022) <https://www.nber.org/papers/w30302>`_.

.. _sensitivity_implementation:

Implementation
~~~~~~~~~~~~~~

The :ref:`plr-model` will be used as an example

.. tab-set::

    .. tab-item:: Python
        :sync: py

        .. ipython:: python

            import numpy as np
            import doubleml as dml
            from doubleml.datasets import make_plr_CCDDHNR2018
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.base import clone

            learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)
            ml_l = clone(learner)
            ml_m = clone(learner)
            np.random.seed(1111)
            data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20, return_type='DataFrame')
            obj_dml_data = dml.DoubleMLData(data, 'y', 'd')
            dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

If the sensitivity analysis is implemented (see :ref:`sensitivity_models`), the corresponding sensitivity elements are estimated
automatically by calling the ``fit()`` method. In most cases these elements are based on the following plug-in estimators

.. math::

    \hat{\sigma}^2 &:= \mathbb{E}_n[(Y-\hat{g}(W))^2]

    \hat{\nu}^2 &:= \mathbb{E}_n[2m(W,\hat{\alpha}) -  \hat{\alpha}(W)^2]

where :math:`\hat{g}(W)` and :math:`\hat{\alpha}(W)` denote the cross-fitted predictions of the outcome regression and the Riesz
representer (both are model specific, see :ref:`sensitivity_models`). Further, the corresponding scores are defined as

.. math::

    \psi_{\sigma^2}(W, \hat{\sigma}^2, g) &:= (Y-\hat{g}(W))^2 - \hat{\sigma}^2\\
    \psi_{\nu^2}(W, \hat{\nu}^2, \alpha) &:= 2m(W,\hat{\alpha}) - \hat{\alpha}(W)^2 - \hat{\nu}^2.

After the ``fit()`` call, the sensitivity elements are stored in a dictionary and can be accessed via the ``sensitivity_elements`` property.

.. tab-set::

    .. tab-item:: Python
        :sync: py

        .. ipython:: python
            
            dml_plr_obj.fit()
            dml_plr_obj.sensitivity_elements.keys()

Each value is a :math:`3`-dimensional array, with the variances being of form ``(1, n_rep, n_coefs)`` and the scores of form ``(n_obs, n_rep, n_coefs)``.
The ``sensitivity_analysis()`` method then computes the upper and lower bounds for the estimate, based on the sensitivity parameters
``cf_y``, ``cf_d`` and ``rho`` (default is ``rho=1.0`` to account for adversarial confounding). Additionally, one-sided confidence bounds are computed 
based on a supplied significance level (default ``level=0.95``). 
The results are summarized as a formatted string in the ``sensitivity_summary``

.. tab-set::

    .. tab-item:: Python
        :sync: py

        .. ipython:: python
            
            dml_plr_obj.sensitivity_analysis(cf_y=0.03, cf_d=0.03, rho=1.0, level=0.95)
            print(dml_plr_obj.sensitivity_summary)

or can be directly accessed via the ``sensitivity_params`` property.

.. tab-set::

    .. tab-item:: Python
        :sync: py

        .. ipython:: python
            
            dml_plr_obj.sensitivity_params

The bounds are saved as a nested dictionary, where the keys ``'theta'``
denote the bounds on the parameter :math:`\hat{\theta}_{\pm}`, ``'se'`` denotes the corresponding standard error and ``'ci'`` denotes the lower and upper
confidence bounds for :math:`\hat{\theta}_{\pm}`. Each of the keys refers to a dictionary with keys ``'lower'`` and ``'upper'``
which refer to the lower or upper bound, e.g. ``sensitivity_params['theta']['lower']`` refers to the lower bound :math:`\hat{\theta}_{-}` of the estimated cofficient .

Further, the sensitivity analysis has an input parameter ``theta`` (with default ``theta=0.0``), which refers to the null hypothesis used for each coefficient.
This null hypothesis is used to calculate the robustness values as displayed in the ``sensitivity_params``.

The robustness value $RV$ is defined as the required confounding strength (``cf_y=rv`` and ``cf_d=rv``), such that the lower or upper bound of the causal parameter includes the null hypothesis.
If the estimated parameter :math:`\hat{\theta}` is larger than the null hypothesis the lower bound is used and vice versa.
The robustness value $RVa$ defined analogous, but additionally incorporates statistical uncertainty (as it is based on the confidence intervals of the bounds). 

To obtain a more complete overview over the sensitivity one can call the ``sensitivity_plot()`` method. The methods creates a contour plot, which calculates estimate of the upper or lower bound for :math:`\theta`
(based on the null hypothesis) for each combination of ``cf_y`` and ``cf_d`` in a grid of values.

.. figure:: ../_static/sensitivity_example_nb.png
   :alt: Contour plot
   :figclass: captioned-image

   Contour plot example (see :ref:`examplegallery`)

By adjusting the parameter ``value='ci'`` in the ``sensitivity_plot()`` method the bounds are displayed for the corresponding confidence level.

.. note::

 -  The ``sensitivity_plot()`` requires to call ``sensitivity_analysis`` first, since the choice of the bound (upper or lower) is based on
    the corresponding null hypothesis. Furhter, the parameters ``rho`` and ``level`` are used. Both are constained in the ``sensitivity_params`` property.   
 -  The ``sensitivity_plot()`` is created for the first treatment variable. This can be changed via the ``idx_treatment`` parameter.
 -  The robustness values are given via the intersection countour of the null hypothesis and the identity.

.. _sensitivity_models:

Model-specific implementations
+++++++++++++++++++++++++++++++++++

This section contains the implementation details for each specific model and model specific interpretations.

.. _sensitivity_plr:

Partially linear regression model (PLR)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the :ref:`plr-model` the confounding strength ``cf_d`` can be further be simplified to match the explanation of ``cf_y``. 
Given the that the Riesz representer takes the following form

.. math::

    \alpha(W) = \frac{D-\mathbb{E}[D|X]}{\mathbb{E}[(D-\mathbb{E}[D|X]^2)]}

    \tilde{\alpha}(\tilde{W}) = \frac{D-\mathbb{E}[D|X,A]}{\mathbb{E}[(D-\mathbb{E}[D|X,A]^2)]}

one can show that

.. math::

    C_D^2 :=\frac{\frac{\mathbb{E}\big[\big(\mathbb{E}[D|X,A] - \mathbb{E}[D|X]\big)^2\big]}{\mathbb{E}\big[\big(D - \mathbb{E}[D|X]\big)^2\big]}}{1-\frac{\mathbb{E}\big[\big(\mathbb{E}[D|X,A] - \mathbb{E}[D|X]\big)^2\big]}{\mathbb{E}\big[\big(D - \mathbb{E}[D|X]\big)^2\big]}}.

Therefore,

- ``cf_y``:math:`:=\frac{\mathbb{E}[(\tilde{g}(\tilde{W}) - g(W))^2]}{\mathbb{E}[(Y - g(W))^2]}`  measures the proportion of residual variance in the outcome :math:`Y` explained by the latent confounders :math:`A`

- ``cf_d``:math:`:=\frac{\mathbb{E}\big[\big(\mathbb{E}[D|X,A] - \mathbb{E}[D|X]\big)^2\big]}{\mathbb{E}\big[\big(D - \mathbb{E}[D|X]\big)^2\big]}` measures the proportion of residual variance in the treatment :math:`D` explained by the latent confounders :math:`A`

.. note::
    In the :ref:`plr-model`, both ``cf_y`` and ``cf_d`` can be interpreted as *nonparametric partial* :math:`R^2`

    - ``cf_y`` has the interpretation as the *nonparametric partial* :math:`R^2` *of* :math:`A` *with* :math:`Y` *given* :math:`(D,X)`
    
    .. math:: 
        
        \frac{\textrm{Var}(\mathbb{E}[Y|D,X,A]) - \textrm{Var}(\mathbb{E}[Y|D,X])}{\textrm{Var}(Y)-\textrm{Var}(\mathbb{E}[Y|D,X])}

    - ``cf_d`` has the interpretation as the *nonparametric partial* :math:`R^2` *of* :math:`A` *with* :math:`D` *given* :math:`X`
    
    .. math:: 
        
        \frac{\textrm{Var}(\mathbb{E}[D|X,A]) - \textrm{Var}(\mathbb{E}[D|X])}{\textrm{Var}(D)-\textrm{Var}(\mathbb{E}[D|X])}

Using the partially linear regression model with ``score='partialling out'`` the ``nuisance_elements`` are implemented in the following form

.. math::

    \hat{\sigma}^2 &:= \mathbb{E}_n\Big[\big(Y-\hat{l}(X) - \hat{\theta}(D-\hat{m}(X))\big)^2\Big]

    \hat{\nu}^2 &:= \mathbb{E}_n[\hat{\alpha}(W)^2] = \frac{1}{\mathbb{E}_n\big[(D - \hat{m}(X))^2\big]}

with scores

.. math::

    \psi_{\sigma^2}(W, \hat{\sigma}^2, g) &:=  \big(Y-\hat{l}(X) - \hat{\theta}(D-\hat{m}(X))\big)^2 - \hat{\sigma}^2

    \psi_{\nu^2}(W, \hat{\nu}^2, \alpha) &:= \hat{\nu}^2 - \big(D-\hat{m}(X)\big)^2\big(\hat{\nu}^2)^2.

If ``score='IV-type'`` the senstivity elements are instead set to

.. math::

    \hat{\sigma}^2 &:= \mathbb{E}_n\Big[\big(Y - \hat{\theta}D - \hat{g}(X)\big)^2\Big]

    \psi_{\sigma^2}(W, \hat{\sigma}^2, g) &:=  \big(Y - \hat{\theta}D - \hat{g}(X)\big)^2 - \hat{\sigma}^2.

.. _sensitivity_irm:

Interactive regression model (IRM)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the :ref:`irm-model` the target parameter can be written as

.. math::

    \theta = \mathbb{E}[(g(1,X) - g(0,X))\omega(D,X)]

where :math:`\omega(D,X)` are weights (e.g. set to :math:`1` for the ATE).
This implies the following representations

.. math::

    m(W,g) &= \big(g(1,X) - g(0,X))\omega(D,X)

    \alpha(W) &= \bigg(\frac{D}{m(X)} - \frac{1-D}{1-m(X)}\bigg)  \mathbb{E}[\omega(D,X)|X].

.. note::
    In the :ref:`irm-model` with the ATE, it holds
    
    .. math:: 

        C_D^2= \frac{\mathbb{E}\Big[\big(P(D=1|X,A)(1-P(D=1|X,A))\big)^{-1}\Big]}{\mathbb{E}\Big[\big(P(D=1|X)(1-P(D=1|X))\big)^{-1}\Big]} - 1
    
    which is the *average gain in conditional precision to predict* :math:`D` *by using* :math:`A` *in addition to* :math:`X`.
    This can be used to choose ``cf_d``:math:`:=\frac{C_D^2}{1 + C_D^2}`.

The ``nuisance_elements`` are then computed with plug-in versions according to the general :ref:`sensitivity_implementation`. 
For ``score='ATE'``, the weights are set to one 

.. math::

    \omega(D,X) = 1,

wheras for ``score='ATTE'``

.. math::

    \omega(D,X) = \frac{D}{\mathbb{E}[D]},

such that

.. math::

    \mathbb{E}[\omega(D,X)|X] = \frac{m(X)}{\mathbb{E}[D]}.


Difference-in-Differences for Panel Data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the :ref:`did-pa-model` with ``score='observational'`` and ``in_sample_normalization=True`` the score function implies the following representations

.. math::

    m(W,g) &= \big(g(1,X) - g(0,X))\frac{D}{\mathbb{E}[D]}

    \alpha(W) &= \frac{D}{\mathbb{E}[D]} - \frac{\frac{m(X)(1-D)}{1-m(X)}}{\mathbb{E}\left[\frac{m(X)(1-D)}{1-m(X)}\right]}.

If instead ``in_sample_normalization=False``, the Riesz representer changes to 

.. math::

    \alpha(W) = \frac{D}{\mathbb{E}[D]} - \frac{m(X)(1-D)}{\mathbb{E}[D](1-m(X))}.

For ``score='experimental'`` implies the score function implies the following representations

.. math::

    m(W,g) &= g(1,X) - g(0,X)

    \alpha(W) &= \frac{D}{\mathbb{E}[D]} - \frac{1-D}{1-\mathbb{E}[D]}.

The ``nuisance_elements`` are then computed with plug-in versions according to the general :ref:`sensitivity_implementation`. 

Difference-in-Differences for repeated cross-sections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the :ref:`did-cs-model` with ``score='observational'`` and ``in_sample_normalization=True`` the score function implies the following representations

.. math::

    m(W,g) &= \Big(\big(g(1,1,X) - g(1,0,X)\big) - \big(g(0,1,X) - g(0,0,X)\big)\Big) \frac{D}{\mathbb{E}[D]}

    \alpha(W) &= \frac{DT}{\mathbb{E}[DT]} - \frac{D(1-T)}{\mathbb{E}[D(1-T)]}

    &\quad - \frac{m(X)(1-D)T}{1-m(X)}\mathbb{E}\left[\frac{m(X)(1-D)T}{1-m(X)}\right]^{-1}

    &\quad + \frac{m(X)(1-D)(1-T)}{1-m(X)}\mathbb{E}\left[\frac{m(X)(1-D)(1-T)}{1-m(X)}\right]^{-1}.

If instead ``in_sample_normalization=False``, the Riesz representer (after simplifications) changes to 

.. math::

    \alpha(W) = \left(\frac{T}{\mathbb{E}[D]\mathbb{E}[T]} + \frac{1-T}{\mathbb{E}[D](1-\mathbb{E}[T])}\right)\left(D - (1-D)\frac{m(X)}{1-m(X)}\right).

For ``score='experimental'`` and ``in_sample_normalization=True`` implies the score function implies the following representations

.. math::

    m(W,g) &= \big(g(1,1,X) - g(1,0,X)\big) - \big(g(0,1,X) - g(0,0,X)\big)

    \alpha(W) &= \frac{DT}{\mathbb{E}[DT]} - \frac{D(1-T)}{\mathbb{E}[D(1-T)]} - \frac{(1-D)T}{\mathbb{E}[(1-D)T]} + \frac{(1-D)(1-T)}{\mathbb{E}[(1-D)(1-T)]}.

And again, if instead ``in_sample_normalization=False``, the Riesz representer (after simplifications) changes to 

.. math::

    \alpha(W) = \frac{DT}{\mathbb{E}[D]\mathbb{E}[T]} - \frac{D(1-T)}{\mathbb{E}[D](1-\mathbb{E}[T])} - \frac{(1-D)T}{(1-\mathbb{E}[D])\mathbb{E}[T]} + \frac{(1-D)(1-T)}{(1-\mathbb{E}[D])(1-\mathbb{E}[T])}.


The ``nuisance_elements`` are then computed with plug-in versions according to the general :ref:`sensitivity_implementation`.